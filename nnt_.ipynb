{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKB5dNxklwLdf4N8V/sRKH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "25208a44953a46ad952e058d3e71063a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8122e567c5af416baa45ed1c91c197f8",
              "IPY_MODEL_05737c604efc4315863d324d5b1241d3",
              "IPY_MODEL_5f96f89fab5a452ead975d2a9d559262"
            ],
            "layout": "IPY_MODEL_4a3efa1a4e454f308f116a579ae79cde"
          }
        },
        "8122e567c5af416baa45ed1c91c197f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a07eb8576aac4d8f86e88e0edb608bc5",
            "placeholder": "​",
            "style": "IPY_MODEL_b1bda76ffaf74aae924eec57d6ab5324",
            "value": "100%"
          }
        },
        "05737c604efc4315863d324d5b1241d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb5252b026f54ff696fe243b2acf6a80",
            "max": 20000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4bbd2d6bf61b4ddbb05ed0097e1d6e8a",
            "value": 20000
          }
        },
        "5f96f89fab5a452ead975d2a9d559262": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3d2ad6cd0804eea94d45b874eb43ac6",
            "placeholder": "​",
            "style": "IPY_MODEL_cb2597b53c4a451d892803a6f240d8df",
            "value": " 20000/20000 [02:16&lt;00:00, 129.02it/s]"
          }
        },
        "4a3efa1a4e454f308f116a579ae79cde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a07eb8576aac4d8f86e88e0edb608bc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1bda76ffaf74aae924eec57d6ab5324": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb5252b026f54ff696fe243b2acf6a80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bbd2d6bf61b4ddbb05ed0097e1d6e8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e3d2ad6cd0804eea94d45b874eb43ac6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb2597b53c4a451d892803a6f240d8df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinpius/MachineTranslation/blob/main/Character_level_language_model_with_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "t1 = timer()\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount(\"/content/drive/\", force_remount = True)\n",
        "  import torch, random\n",
        "  from torch.nn import functional as F\n",
        "  from tqdm.auto import tqdm\n",
        "  import matplotlib.pyplot as plt\n",
        "  print(f\">>>> You are in CoLaB notbook weith torch version: {torch.__version__}\")\n",
        "except Exception as e:\n",
        "  print(f\">>>> {type(e)}: {e}\\n>>>> Please correct {type(e)} and reload\")\n",
        "\n",
        "def mytimer(t: float = timer())->float:\n",
        "  h = int(t / (60 * 60))\n",
        "  m = int(t % (60 * 60) / 60)\n",
        "  s = int(t % 60)\n",
        "  return f\"hrs: {h}, mins: {m:>02}, secs: {s:>05.2f}\"\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "BATCH_SIZE = 64 if device == torch.device(\"cuda\") else 32\n",
        "print(f\">>>> Available device: {device}\")\n",
        "!nvidia-smi\n",
        "print(f\">>>> Time elapsed: \\t {mytimer(timer() - t1)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g74vqv2Uelce",
        "outputId": "2eb12ad2-d6c8-468b-b51b-cc38dabad561"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            ">>>> You are in CoLaB notbook weith torch version: 2.0.0+cu118\n",
            ">>>> Available device: cpu\n",
            "/bin/bash: nvidia-smi: command not found\n",
            ">>>> Time elapsed: \t hrs: 0, mins: 00, secs: 37.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We build a deep MLP as a character level language model to predict the probability\n",
        "distribution of the next character given a sequence of characters. This is the language \n",
        "generation model. We employ the dataset by Andrej Karpathy which can be found here: https://github.com/karpathy/makemore"
      ],
      "metadata": {
        "id": "pe38PEfai17a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading and explore the dataset\n",
        "data_path = \"/content/drive/MyDrive/Language models/makemore-master/names.txt\"\n",
        "names = open(data_path, \"r\").read().splitlines()\n",
        "print(f\">>>> Total number of example in names dataset is: {len(names)}\\\n",
        "\\n>>>> The longest name has {max([len(name) for name in names])} characters\\\n",
        "\\n>>>> The shortest name has {min([len(name) for name in names])} characters\\\n",
        "\\n>>>> The first 10 names are: {names[:10]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeIcPB8cijwx",
        "outputId": "029ef11a-53b7-446b-955b-c24e8c0e9b39"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>>> Total number of example in names dataset is: 32033\n",
            ">>>> The longest name has 15 characters\n",
            ">>>> The shortest name has 2 characters\n",
            ">>>> The first 10 names are: ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparations steps: \n",
        "* We build a lookup dictionaries to map characters to numeric and vice-versa\n",
        "* We obtain list of unique character in the entire dataset\n",
        "* We padd every name with a special token to identify the starting and ending of a name\n",
        "* We create a padd index as zero\n",
        "* We create the dataset with sequence length of 3 (can be set as hyperparameter to be optimized by the network\n",
        "* We split the dataset into train, validation and test splits"
      ],
      "metadata": {
        "id": "QFDL7MVvkqCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def myTextDataset(words_list):\n",
        "\n",
        "  all_chars = sorted(list(set(\"\".join(words_list)))) # Fetch all unique characters as a sorted list\n",
        "  assert len(all_chars) == 26 # I know this prior-since they are english names\n",
        "  stoi = {s : i + 1 for i, s in enumerate(all_chars)} # map char to index\n",
        "  stoi[\".\"] = 0 # Add a padding token as 0 index\n",
        "  itos = {i: s for s, i in stoi.items()} # reconvert back to characters\n",
        "  assert len(stoi) == len(itos) == 27 # since we added a special token \".\"\n",
        "\n",
        "  seq_len = 3 # create the block size i.e, we need data in shape [abc -> d, bcd->e, etc]\n",
        "  # building the dataset\n",
        "  inputs, labels = [], []\n",
        "  for name in words_list:\n",
        "    context = [0]*seq_len # start with a sequence of padding characters \n",
        "    name = name + \".\" # adding a padding character\n",
        "    for ch in name:\n",
        "      idx = stoi[ch] # grab the respective index\n",
        "      inputs.append(context), labels.append(idx) # Append the respective inputs and outputs indices\n",
        "      context = context[1:] + [idx] # updating a context vector\n",
        "  inputs, labels = torch.tensor(inputs), torch.tensor(labels) # Cast the results into torch.Tensor\n",
        "  return stoi, itos, all_chars, inputs, labels\n"
      ],
      "metadata": {
        "id": "yD4ERdwomNrT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can now buid our dataset\n",
        "n1, n2 = int(len(names) * 0.8), int(len(names) *0.9) # We split the data into 80% train, 10% val, & 10 test\n",
        "random.shuffle(names) # We shuffle the data before splitting to create randomness\n",
        "train = names[:n1] # Fetch the 80 % training set\n",
        "valid = names[n1:n2] # Fetch the 10 % validation split\n",
        "test = names[n2:] # Fetch the remaing 10 % for testing\n",
        "stoi, itos, all_chars, X_tr, Y_tr = myTextDataset(words_list = train)\n",
        "_, _,_, X_val, Y_val = myTextDataset(words_list = valid)\n",
        "_, _,_, X_test, Y_test = myTextDataset(words_list = test)"
      ],
      "metadata": {
        "id": "nJi3ezE4tuP7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\">>>> List of all unique characters in the dataset:\\n {all_chars}\\\n",
        "\\n>>>> stoi: \\n {stoi}\\n>>>> itos: \\n {itos}\\\n",
        "\\n\\n>>>> X_train shape: {X_tr.shape}, Y_train shape: {Y_tr.shape}\\\n",
        "\\n>>>> X_val shape: {X_val.shape}, Y_val shape: {Y_val.shape}\\\n",
        "\\n>>>> X_test shape: {X_test.shape}, Y_test shape: {Y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afEg-oqxvbqq",
        "outputId": "dd96a03f-0fbe-4a26-ac4c-b163dd603329"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>>> List of all unique characters in the dataset:\n",
            " ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            ">>>> stoi: \n",
            " {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n",
            ">>>> itos: \n",
            " {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "\n",
            ">>>> X_train shape: torch.Size([182536, 3]), Y_train shape: torch.Size([182536])\n",
            ">>>> X_val shape: torch.Size([22788, 3]), Y_val shape: torch.Size([22788])\n",
            ">>>> X_test shape: torch.Size([22822, 3]), Y_test shape: torch.Size([22822])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can now Create our deep MLP to train a language model to generate new names "
      ],
      "metadata": {
        "id": "ACqsjFhKwTOw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearLayer:\n",
        "  \"\"\"\"\"\n",
        "  This class implement the Linear layer in MLP\n",
        "  \"\"\"\"\"\n",
        "  def __init__(self, in_dim, out_dim, bias = True):\n",
        "\n",
        "    # We use kaiming initialization for the weights\n",
        "    self.weight = torch.randn(size = (in_dim, out_dim),\n",
        "                              generator = gen) * (1/in_dim **2)\n",
        "    self.bias = torch.zeros(out_dim) # initialize the bias to zero\n",
        "  \n",
        "  def __call__(self, input_tensor: torch.Tensor)->torch.Tensor:\n",
        "    self.out = torch.mm(input_tensor, self.weight)\n",
        "    if self.bias is not None: # We do not need bias for batchnorm layer\n",
        "      self.out += self.bias\n",
        "    return self.out\n",
        "  \n",
        "  def parameters(self):\n",
        "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
        "\n",
        "class Tanh:\n",
        "  \"\"\"\"\"\n",
        "  This class implement a tanh activation function\n",
        "  \"\"\"\"\"\n",
        "  def __call__(self, input_tensor: torch.Tensor)->torch.Tensor:\n",
        "    self.out = torch.tanh(input_tensor)\n",
        "    return self.out\n",
        "  \n",
        "  def parameters(self):\n",
        "    return []\n",
        "\n",
        "class Batchnorm1d:\n",
        "  \"\"\"\"\"\n",
        "  This class implement batch normalization layer for MLP\n",
        "  Parameters: mean, var ==> beta, gamma\n",
        "  momentum: fixed to 0.1 to adjust the batchnorm parameters for evaluation\n",
        "  eps: small constant to avoid division by a zero variance\n",
        "  dim: is the output dimension of the linear layer just before a batchnorm layer\n",
        "  \"\"\"\"\"\n",
        "  def __init__(self, dim, momentum = 0.1, eps = 1e-6):\n",
        "\n",
        "    self.dim = dim\n",
        "    self.training = True\n",
        "    self.momentum = momentum\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(self.dim)\n",
        "    self.beta = torch.zeros(self.dim) \n",
        "    self.rnmean = torch.zeros(self.dim)\n",
        "    self.rnvar = torch.ones(self.dim)\n",
        "\n",
        "  def __call__(self, input_tensor: torch.Tensor)->torch.Tensor:\n",
        "    if self.training: # We learn the beta & gamma params\n",
        "      xmean = input_tensor.mean(dim = 0, keepdims = True) # compute the batch mean\n",
        "      xvar = input_tensor.var(dim = 0, keepdims = True) # compute the batch variance\n",
        "    else: # We set a fixed mean and var which is updated wrt momentum \n",
        "      xmean = self.rnmean\n",
        "      xvar = self.rnvar\n",
        "    x_hat = (input_tensor - xmean) / torch.sqrt(xvar + self.eps) # normalize the batch\n",
        "    self.out = self.gamma * x_hat + self.beta # We scale and centering the otput of batchnorm [pars to be learned]\n",
        "\n",
        "    # We update the rnmean and rnvar depending on momentum and the learned barchnorm pars \n",
        "    if self.training: # We do the update of the buffer during training but no grad is required\n",
        "      with torch.no_grad():\n",
        "        self.rnmean = (1-self.momentum) * self.rnmean + self.momentum * xmean #updating the buffer's mean\n",
        "        self.rnvar = self.rnvar * (1-self.momentum) + self.momentum * xvar # updating the buffer's variance\n",
        "    \n",
        "    return self.out\n",
        "  \n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n"
      ],
      "metadata": {
        "id": "zN6hIeFK5f9j"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can now instantiating our MLP language model using the above defined classes as follows\n",
        "seq_len = 3\n",
        "embd_dim = 10\n",
        "hidden_dim = 200\n",
        "vocab_size = 27\n",
        "gen = torch.Generator().manual_seed(239101)\n",
        "Wo = torch.randn(size = (vocab_size, embd_dim), generator = gen) # Embedding weights tensor\n",
        "\n",
        "MLPs = [\n",
        "    LinearLayer(in_dim = embd_dim * seq_len, out_dim = hidden_dim), Batchnorm1d(hidden_dim), Tanh(),\n",
        "    LinearLayer(in_dim = hidden_dim, out_dim = hidden_dim),Batchnorm1d(hidden_dim), Tanh(),\n",
        "    LinearLayer(in_dim = hidden_dim, out_dim = hidden_dim),Batchnorm1d(hidden_dim), Tanh(),\n",
        "    LinearLayer(in_dim = hidden_dim, out_dim = hidden_dim),Batchnorm1d(hidden_dim), Tanh(),\n",
        "    LinearLayer(in_dim = hidden_dim, out_dim = vocab_size)\n",
        "]\n",
        "\n",
        "# use the gain of 0.1 for the output layer and 5/3 for the hidden layer\n",
        "with torch.no_grad():\n",
        "  MLPs[-1].weight *= 0.1\n",
        "  for layer in MLPs:\n",
        "    if isinstance(layer, LinearLayer):\n",
        "      layer.weight *= 5/3\n",
        "\n",
        "params = [Wo] + [p for layer in MLPs for p in layer.parameters()]\n",
        "print(f\">>>> The deep MLP has {sum([p.numel() for p in params]):,} parameters\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L37CUz6wJ4WW",
        "outputId": "2a147c96-615e-47e6-ca72-44a509c02aa2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>>> The deep MLP has 134,097 parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for par in params: par.requires_grad = True "
      ],
      "metadata": {
        "id": "H1bE90hJUi_S"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assess if everthing works fine\n",
        "EMB = Wo[X_tr]\n",
        "EMB = EMB.view(EMB.shape[0], -1)\n",
        "for layer in MLPs:\n",
        "  EMB = layer(EMB)\n",
        "assert EMB.shape == (EMB.shape[0], vocab_size)"
      ],
      "metadata": {
        "id": "rowPTRu2NC1i"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The training loop:\n",
        "EPOCHS = 20000\n",
        "all_loss = []\n",
        "steps = []\n",
        "nll = 0\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "  ix = torch.randint(0, X_tr.shape[0], (BATCH_SIZE,)) # sample the minibatch of size BATCH_SIZE\n",
        "  x_em = Wo[X_tr[ix]] # Get the embedding for the batch\n",
        "  x_em = x_em.view(x_em.shape[0], -1)\n",
        "\n",
        "  # Running the forward prop\n",
        "  for layer in MLPs:\n",
        "    x_em = layer(x_em)\n",
        "  tr_loss = F.cross_entropy(x_em, Y_tr[ix]) # Fetch loss\n",
        "  print(f\">>>> Epoch {epoch + 1 if epoch == 0 else epoch}:\\t Train Loss: {tr_loss.item():.4f}\")\n",
        "\n",
        "  # Running the backward pass\n",
        "  for layer in MLPs:\n",
        "    layer.out.retain_grad()\n",
        "    for par in params:\n",
        "      par.grad = None\n",
        "  tr_loss.backward()\n",
        "\n",
        "  LR = 0.1 if epoch < 10000 else 0.01 # Decay the learning rate after half epochs training\n",
        "  for par in params:\n",
        "    par.data +=-LR * par.grad\n",
        "  all_loss.append(tr_loss.item())\n",
        "  steps.append(epoch)\n",
        "  nll += tr_loss.mean()\n",
        "\n",
        "print(f\">>>> Average training loss: {nll / EPOCHS:.4f}\")\n",
        "\n",
        "plt.plot(steps, all_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "25208a44953a46ad952e058d3e71063a",
            "8122e567c5af416baa45ed1c91c197f8",
            "05737c604efc4315863d324d5b1241d3",
            "5f96f89fab5a452ead975d2a9d559262",
            "4a3efa1a4e454f308f116a579ae79cde",
            "a07eb8576aac4d8f86e88e0edb608bc5",
            "b1bda76ffaf74aae924eec57d6ab5324",
            "bb5252b026f54ff696fe243b2acf6a80",
            "4bbd2d6bf61b4ddbb05ed0097e1d6e8a",
            "e3d2ad6cd0804eea94d45b874eb43ac6",
            "cb2597b53c4a451d892803a6f240d8df"
          ]
        },
        "id": "EjjrBmjFNT3W",
        "outputId": "77910eb4-5e8b-4794-cc27-4e8180c860f6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25208a44953a46ad952e058d3e71063a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            ">>>> Epoch 15001:\t Train Loss: 2.7513\n",
            ">>>> Epoch 15002:\t Train Loss: 1.8971\n",
            ">>>> Epoch 15003:\t Train Loss: 2.2166\n",
            ">>>> Epoch 15004:\t Train Loss: 2.4201\n",
            ">>>> Epoch 15005:\t Train Loss: 1.8726\n",
            ">>>> Epoch 15006:\t Train Loss: 2.1727\n",
            ">>>> Epoch 15007:\t Train Loss: 2.3356\n",
            ">>>> Epoch 15008:\t Train Loss: 2.3973\n",
            ">>>> Epoch 15009:\t Train Loss: 2.4800\n",
            ">>>> Epoch 15010:\t Train Loss: 2.3437\n",
            ">>>> Epoch 15011:\t Train Loss: 2.4550\n",
            ">>>> Epoch 15012:\t Train Loss: 2.1180\n",
            ">>>> Epoch 15013:\t Train Loss: 2.1361\n",
            ">>>> Epoch 15014:\t Train Loss: 2.4818\n",
            ">>>> Epoch 15015:\t Train Loss: 1.9478\n",
            ">>>> Epoch 15016:\t Train Loss: 2.2805\n",
            ">>>> Epoch 15017:\t Train Loss: 2.2781\n",
            ">>>> Epoch 15018:\t Train Loss: 2.4077\n",
            ">>>> Epoch 15019:\t Train Loss: 2.1760\n",
            ">>>> Epoch 15020:\t Train Loss: 2.0190\n",
            ">>>> Epoch 15021:\t Train Loss: 2.4935\n",
            ">>>> Epoch 15022:\t Train Loss: 2.2741\n",
            ">>>> Epoch 15023:\t Train Loss: 2.1380\n",
            ">>>> Epoch 15024:\t Train Loss: 2.3963\n",
            ">>>> Epoch 15025:\t Train Loss: 2.7471\n",
            ">>>> Epoch 15026:\t Train Loss: 1.9283\n",
            ">>>> Epoch 15027:\t Train Loss: 1.8231\n",
            ">>>> Epoch 15028:\t Train Loss: 2.1124\n",
            ">>>> Epoch 15029:\t Train Loss: 2.2103\n",
            ">>>> Epoch 15030:\t Train Loss: 2.3055\n",
            ">>>> Epoch 15031:\t Train Loss: 2.5366\n",
            ">>>> Epoch 15032:\t Train Loss: 1.9838\n",
            ">>>> Epoch 15033:\t Train Loss: 2.2383\n",
            ">>>> Epoch 15034:\t Train Loss: 2.6272\n",
            ">>>> Epoch 15035:\t Train Loss: 2.2096\n",
            ">>>> Epoch 15036:\t Train Loss: 1.8583\n",
            ">>>> Epoch 15037:\t Train Loss: 2.3767\n",
            ">>>> Epoch 15038:\t Train Loss: 2.4343\n",
            ">>>> Epoch 15039:\t Train Loss: 2.6209\n",
            ">>>> Epoch 15040:\t Train Loss: 2.5460\n",
            ">>>> Epoch 15041:\t Train Loss: 2.0413\n",
            ">>>> Epoch 15042:\t Train Loss: 2.1859\n",
            ">>>> Epoch 15043:\t Train Loss: 2.1484\n",
            ">>>> Epoch 15044:\t Train Loss: 2.3405\n",
            ">>>> Epoch 15045:\t Train Loss: 2.3665\n",
            ">>>> Epoch 15046:\t Train Loss: 2.2144\n",
            ">>>> Epoch 15047:\t Train Loss: 2.6994\n",
            ">>>> Epoch 15048:\t Train Loss: 2.2955\n",
            ">>>> Epoch 15049:\t Train Loss: 2.0445\n",
            ">>>> Epoch 15050:\t Train Loss: 2.6579\n",
            ">>>> Epoch 15051:\t Train Loss: 2.1560\n",
            ">>>> Epoch 15052:\t Train Loss: 2.0986\n",
            ">>>> Epoch 15053:\t Train Loss: 2.0931\n",
            ">>>> Epoch 15054:\t Train Loss: 2.1010\n",
            ">>>> Epoch 15055:\t Train Loss: 1.7467\n",
            ">>>> Epoch 15056:\t Train Loss: 2.1488\n",
            ">>>> Epoch 15057:\t Train Loss: 2.4873\n",
            ">>>> Epoch 15058:\t Train Loss: 2.1864\n",
            ">>>> Epoch 15059:\t Train Loss: 2.0705\n",
            ">>>> Epoch 15060:\t Train Loss: 2.3109\n",
            ">>>> Epoch 15061:\t Train Loss: 2.0133\n",
            ">>>> Epoch 15062:\t Train Loss: 2.1130\n",
            ">>>> Epoch 15063:\t Train Loss: 2.2728\n",
            ">>>> Epoch 15064:\t Train Loss: 2.0429\n",
            ">>>> Epoch 15065:\t Train Loss: 1.8262\n",
            ">>>> Epoch 15066:\t Train Loss: 2.1342\n",
            ">>>> Epoch 15067:\t Train Loss: 2.0303\n",
            ">>>> Epoch 15068:\t Train Loss: 2.4562\n",
            ">>>> Epoch 15069:\t Train Loss: 1.8709\n",
            ">>>> Epoch 15070:\t Train Loss: 2.1086\n",
            ">>>> Epoch 15071:\t Train Loss: 2.5999\n",
            ">>>> Epoch 15072:\t Train Loss: 1.8171\n",
            ">>>> Epoch 15073:\t Train Loss: 2.3524\n",
            ">>>> Epoch 15074:\t Train Loss: 2.0667\n",
            ">>>> Epoch 15075:\t Train Loss: 2.7795\n",
            ">>>> Epoch 15076:\t Train Loss: 2.4257\n",
            ">>>> Epoch 15077:\t Train Loss: 2.1214\n",
            ">>>> Epoch 15078:\t Train Loss: 2.3297\n",
            ">>>> Epoch 15079:\t Train Loss: 2.4878\n",
            ">>>> Epoch 15080:\t Train Loss: 2.4184\n",
            ">>>> Epoch 15081:\t Train Loss: 2.0310\n",
            ">>>> Epoch 15082:\t Train Loss: 2.3105\n",
            ">>>> Epoch 15083:\t Train Loss: 2.0766\n",
            ">>>> Epoch 15084:\t Train Loss: 2.1079\n",
            ">>>> Epoch 15085:\t Train Loss: 2.3019\n",
            ">>>> Epoch 15086:\t Train Loss: 2.1262\n",
            ">>>> Epoch 15087:\t Train Loss: 2.2102\n",
            ">>>> Epoch 15088:\t Train Loss: 2.1764\n",
            ">>>> Epoch 15089:\t Train Loss: 2.0189\n",
            ">>>> Epoch 15090:\t Train Loss: 1.9859\n",
            ">>>> Epoch 15091:\t Train Loss: 2.2044\n",
            ">>>> Epoch 15092:\t Train Loss: 2.3501\n",
            ">>>> Epoch 15093:\t Train Loss: 1.8970\n",
            ">>>> Epoch 15094:\t Train Loss: 2.3987\n",
            ">>>> Epoch 15095:\t Train Loss: 2.2167\n",
            ">>>> Epoch 15096:\t Train Loss: 2.0736\n",
            ">>>> Epoch 15097:\t Train Loss: 2.3291\n",
            ">>>> Epoch 15098:\t Train Loss: 2.3793\n",
            ">>>> Epoch 15099:\t Train Loss: 1.9756\n",
            ">>>> Epoch 15100:\t Train Loss: 2.1274\n",
            ">>>> Epoch 15101:\t Train Loss: 2.3652\n",
            ">>>> Epoch 15102:\t Train Loss: 2.1413\n",
            ">>>> Epoch 15103:\t Train Loss: 2.1700\n",
            ">>>> Epoch 15104:\t Train Loss: 2.1284\n",
            ">>>> Epoch 15105:\t Train Loss: 2.2177\n",
            ">>>> Epoch 15106:\t Train Loss: 2.1552\n",
            ">>>> Epoch 15107:\t Train Loss: 2.1932\n",
            ">>>> Epoch 15108:\t Train Loss: 1.9706\n",
            ">>>> Epoch 15109:\t Train Loss: 2.2224\n",
            ">>>> Epoch 15110:\t Train Loss: 2.1994\n",
            ">>>> Epoch 15111:\t Train Loss: 2.3578\n",
            ">>>> Epoch 15112:\t Train Loss: 2.0392\n",
            ">>>> Epoch 15113:\t Train Loss: 2.3242\n",
            ">>>> Epoch 15114:\t Train Loss: 2.5270\n",
            ">>>> Epoch 15115:\t Train Loss: 2.0703\n",
            ">>>> Epoch 15116:\t Train Loss: 2.1881\n",
            ">>>> Epoch 15117:\t Train Loss: 2.3064\n",
            ">>>> Epoch 15118:\t Train Loss: 2.6578\n",
            ">>>> Epoch 15119:\t Train Loss: 1.9355\n",
            ">>>> Epoch 15120:\t Train Loss: 2.1838\n",
            ">>>> Epoch 15121:\t Train Loss: 2.3855\n",
            ">>>> Epoch 15122:\t Train Loss: 2.4049\n",
            ">>>> Epoch 15123:\t Train Loss: 1.9227\n",
            ">>>> Epoch 15124:\t Train Loss: 2.1768\n",
            ">>>> Epoch 15125:\t Train Loss: 2.3547\n",
            ">>>> Epoch 15126:\t Train Loss: 1.8985\n",
            ">>>> Epoch 15127:\t Train Loss: 2.1769\n",
            ">>>> Epoch 15128:\t Train Loss: 2.5766\n",
            ">>>> Epoch 15129:\t Train Loss: 2.2908\n",
            ">>>> Epoch 15130:\t Train Loss: 2.2171\n",
            ">>>> Epoch 15131:\t Train Loss: 2.3217\n",
            ">>>> Epoch 15132:\t Train Loss: 2.0253\n",
            ">>>> Epoch 15133:\t Train Loss: 2.4261\n",
            ">>>> Epoch 15134:\t Train Loss: 2.6514\n",
            ">>>> Epoch 15135:\t Train Loss: 2.1134\n",
            ">>>> Epoch 15136:\t Train Loss: 1.9054\n",
            ">>>> Epoch 15137:\t Train Loss: 2.3111\n",
            ">>>> Epoch 15138:\t Train Loss: 2.3925\n",
            ">>>> Epoch 15139:\t Train Loss: 2.2933\n",
            ">>>> Epoch 15140:\t Train Loss: 2.5150\n",
            ">>>> Epoch 15141:\t Train Loss: 1.9181\n",
            ">>>> Epoch 15142:\t Train Loss: 2.3297\n",
            ">>>> Epoch 15143:\t Train Loss: 2.2598\n",
            ">>>> Epoch 15144:\t Train Loss: 2.2503\n",
            ">>>> Epoch 15145:\t Train Loss: 2.2130\n",
            ">>>> Epoch 15146:\t Train Loss: 2.4706\n",
            ">>>> Epoch 15147:\t Train Loss: 2.1985\n",
            ">>>> Epoch 15148:\t Train Loss: 2.0885\n",
            ">>>> Epoch 15149:\t Train Loss: 2.5331\n",
            ">>>> Epoch 15150:\t Train Loss: 2.3469\n",
            ">>>> Epoch 15151:\t Train Loss: 2.4138\n",
            ">>>> Epoch 15152:\t Train Loss: 2.1288\n",
            ">>>> Epoch 15153:\t Train Loss: 2.1640\n",
            ">>>> Epoch 15154:\t Train Loss: 2.3583\n",
            ">>>> Epoch 15155:\t Train Loss: 2.1916\n",
            ">>>> Epoch 15156:\t Train Loss: 2.5460\n",
            ">>>> Epoch 15157:\t Train Loss: 2.3573\n",
            ">>>> Epoch 15158:\t Train Loss: 2.6282\n",
            ">>>> Epoch 15159:\t Train Loss: 1.9733\n",
            ">>>> Epoch 15160:\t Train Loss: 2.2265\n",
            ">>>> Epoch 15161:\t Train Loss: 2.1400\n",
            ">>>> Epoch 15162:\t Train Loss: 2.4919\n",
            ">>>> Epoch 15163:\t Train Loss: 1.9745\n",
            ">>>> Epoch 15164:\t Train Loss: 2.2767\n",
            ">>>> Epoch 15165:\t Train Loss: 1.8803\n",
            ">>>> Epoch 15166:\t Train Loss: 2.2577\n",
            ">>>> Epoch 15167:\t Train Loss: 2.4327\n",
            ">>>> Epoch 15168:\t Train Loss: 2.2510\n",
            ">>>> Epoch 15169:\t Train Loss: 2.1426\n",
            ">>>> Epoch 15170:\t Train Loss: 2.2537\n",
            ">>>> Epoch 15171:\t Train Loss: 2.2183\n",
            ">>>> Epoch 15172:\t Train Loss: 2.2513\n",
            ">>>> Epoch 15173:\t Train Loss: 2.4690\n",
            ">>>> Epoch 15174:\t Train Loss: 2.2452\n",
            ">>>> Epoch 15175:\t Train Loss: 2.3839\n",
            ">>>> Epoch 15176:\t Train Loss: 1.8365\n",
            ">>>> Epoch 15177:\t Train Loss: 1.9825\n",
            ">>>> Epoch 15178:\t Train Loss: 2.5812\n",
            ">>>> Epoch 15179:\t Train Loss: 2.2095\n",
            ">>>> Epoch 15180:\t Train Loss: 2.2895\n",
            ">>>> Epoch 15181:\t Train Loss: 2.1296\n",
            ">>>> Epoch 15182:\t Train Loss: 2.4885\n",
            ">>>> Epoch 15183:\t Train Loss: 2.1492\n",
            ">>>> Epoch 15184:\t Train Loss: 1.8608\n",
            ">>>> Epoch 15185:\t Train Loss: 2.3187\n",
            ">>>> Epoch 15186:\t Train Loss: 2.3490\n",
            ">>>> Epoch 15187:\t Train Loss: 2.5368\n",
            ">>>> Epoch 15188:\t Train Loss: 2.4053\n",
            ">>>> Epoch 15189:\t Train Loss: 2.2686\n",
            ">>>> Epoch 15190:\t Train Loss: 1.9783\n",
            ">>>> Epoch 15191:\t Train Loss: 2.2283\n",
            ">>>> Epoch 15192:\t Train Loss: 2.5634\n",
            ">>>> Epoch 15193:\t Train Loss: 2.1220\n",
            ">>>> Epoch 15194:\t Train Loss: 2.2871\n",
            ">>>> Epoch 15195:\t Train Loss: 2.2454\n",
            ">>>> Epoch 15196:\t Train Loss: 2.3492\n",
            ">>>> Epoch 15197:\t Train Loss: 1.9125\n",
            ">>>> Epoch 15198:\t Train Loss: 1.9942\n",
            ">>>> Epoch 15199:\t Train Loss: 2.1942\n",
            ">>>> Epoch 15200:\t Train Loss: 2.5912\n",
            ">>>> Epoch 15201:\t Train Loss: 2.4432\n",
            ">>>> Epoch 15202:\t Train Loss: 2.2776\n",
            ">>>> Epoch 15203:\t Train Loss: 1.9647\n",
            ">>>> Epoch 15204:\t Train Loss: 2.3071\n",
            ">>>> Epoch 15205:\t Train Loss: 2.1028\n",
            ">>>> Epoch 15206:\t Train Loss: 2.5079\n",
            ">>>> Epoch 15207:\t Train Loss: 2.5672\n",
            ">>>> Epoch 15208:\t Train Loss: 1.9572\n",
            ">>>> Epoch 15209:\t Train Loss: 2.2380\n",
            ">>>> Epoch 15210:\t Train Loss: 2.5456\n",
            ">>>> Epoch 15211:\t Train Loss: 2.1255\n",
            ">>>> Epoch 15212:\t Train Loss: 1.9802\n",
            ">>>> Epoch 15213:\t Train Loss: 2.4060\n",
            ">>>> Epoch 15214:\t Train Loss: 2.6704\n",
            ">>>> Epoch 15215:\t Train Loss: 2.3180\n",
            ">>>> Epoch 15216:\t Train Loss: 2.3925\n",
            ">>>> Epoch 15217:\t Train Loss: 2.0964\n",
            ">>>> Epoch 15218:\t Train Loss: 2.3145\n",
            ">>>> Epoch 15219:\t Train Loss: 2.3078\n",
            ">>>> Epoch 15220:\t Train Loss: 2.6536\n",
            ">>>> Epoch 15221:\t Train Loss: 2.1666\n",
            ">>>> Epoch 15222:\t Train Loss: 2.4772\n",
            ">>>> Epoch 15223:\t Train Loss: 1.8236\n",
            ">>>> Epoch 15224:\t Train Loss: 1.9842\n",
            ">>>> Epoch 15225:\t Train Loss: 2.4248\n",
            ">>>> Epoch 15226:\t Train Loss: 2.5845\n",
            ">>>> Epoch 15227:\t Train Loss: 2.2658\n",
            ">>>> Epoch 15228:\t Train Loss: 2.3524\n",
            ">>>> Epoch 15229:\t Train Loss: 2.4755\n",
            ">>>> Epoch 15230:\t Train Loss: 2.2531\n",
            ">>>> Epoch 15231:\t Train Loss: 2.3554\n",
            ">>>> Epoch 15232:\t Train Loss: 2.2615\n",
            ">>>> Epoch 15233:\t Train Loss: 1.8771\n",
            ">>>> Epoch 15234:\t Train Loss: 2.1830\n",
            ">>>> Epoch 15235:\t Train Loss: 2.7465\n",
            ">>>> Epoch 15236:\t Train Loss: 2.2813\n",
            ">>>> Epoch 15237:\t Train Loss: 2.2870\n",
            ">>>> Epoch 15238:\t Train Loss: 2.2801\n",
            ">>>> Epoch 15239:\t Train Loss: 1.9800\n",
            ">>>> Epoch 15240:\t Train Loss: 1.9768\n",
            ">>>> Epoch 15241:\t Train Loss: 2.3862\n",
            ">>>> Epoch 15242:\t Train Loss: 2.6571\n",
            ">>>> Epoch 15243:\t Train Loss: 1.9630\n",
            ">>>> Epoch 15244:\t Train Loss: 2.5321\n",
            ">>>> Epoch 15245:\t Train Loss: 2.0919\n",
            ">>>> Epoch 15246:\t Train Loss: 2.4148\n",
            ">>>> Epoch 15247:\t Train Loss: 2.0730\n",
            ">>>> Epoch 15248:\t Train Loss: 2.4437\n",
            ">>>> Epoch 15249:\t Train Loss: 2.3450\n",
            ">>>> Epoch 15250:\t Train Loss: 1.7875\n",
            ">>>> Epoch 15251:\t Train Loss: 2.8672\n",
            ">>>> Epoch 15252:\t Train Loss: 1.8358\n",
            ">>>> Epoch 15253:\t Train Loss: 2.2296\n",
            ">>>> Epoch 15254:\t Train Loss: 2.1692\n",
            ">>>> Epoch 15255:\t Train Loss: 2.0152\n",
            ">>>> Epoch 15256:\t Train Loss: 2.0964\n",
            ">>>> Epoch 15257:\t Train Loss: 1.8649\n",
            ">>>> Epoch 15258:\t Train Loss: 2.5579\n",
            ">>>> Epoch 15259:\t Train Loss: 2.1551\n",
            ">>>> Epoch 15260:\t Train Loss: 2.2356\n",
            ">>>> Epoch 15261:\t Train Loss: 2.2834\n",
            ">>>> Epoch 15262:\t Train Loss: 2.0329\n",
            ">>>> Epoch 15263:\t Train Loss: 2.4056\n",
            ">>>> Epoch 15264:\t Train Loss: 2.2840\n",
            ">>>> Epoch 15265:\t Train Loss: 2.2248\n",
            ">>>> Epoch 15266:\t Train Loss: 2.3257\n",
            ">>>> Epoch 15267:\t Train Loss: 2.4060\n",
            ">>>> Epoch 15268:\t Train Loss: 2.1047\n",
            ">>>> Epoch 15269:\t Train Loss: 2.4989\n",
            ">>>> Epoch 15270:\t Train Loss: 2.3346\n",
            ">>>> Epoch 15271:\t Train Loss: 2.2487\n",
            ">>>> Epoch 15272:\t Train Loss: 2.2344\n",
            ">>>> Epoch 15273:\t Train Loss: 2.2977\n",
            ">>>> Epoch 15274:\t Train Loss: 2.2825\n",
            ">>>> Epoch 15275:\t Train Loss: 2.3364\n",
            ">>>> Epoch 15276:\t Train Loss: 2.2042\n",
            ">>>> Epoch 15277:\t Train Loss: 2.0362\n",
            ">>>> Epoch 15278:\t Train Loss: 2.2746\n",
            ">>>> Epoch 15279:\t Train Loss: 1.7751\n",
            ">>>> Epoch 15280:\t Train Loss: 2.2807\n",
            ">>>> Epoch 15281:\t Train Loss: 2.6545\n",
            ">>>> Epoch 15282:\t Train Loss: 2.2485\n",
            ">>>> Epoch 15283:\t Train Loss: 2.0918\n",
            ">>>> Epoch 15284:\t Train Loss: 2.3539\n",
            ">>>> Epoch 15285:\t Train Loss: 2.3760\n",
            ">>>> Epoch 15286:\t Train Loss: 2.2263\n",
            ">>>> Epoch 15287:\t Train Loss: 2.3438\n",
            ">>>> Epoch 15288:\t Train Loss: 2.2209\n",
            ">>>> Epoch 15289:\t Train Loss: 2.4199\n",
            ">>>> Epoch 15290:\t Train Loss: 2.4518\n",
            ">>>> Epoch 15291:\t Train Loss: 2.0467\n",
            ">>>> Epoch 15292:\t Train Loss: 2.5506\n",
            ">>>> Epoch 15293:\t Train Loss: 2.1871\n",
            ">>>> Epoch 15294:\t Train Loss: 2.5090\n",
            ">>>> Epoch 15295:\t Train Loss: 2.2983\n",
            ">>>> Epoch 15296:\t Train Loss: 2.3610\n",
            ">>>> Epoch 15297:\t Train Loss: 2.1126\n",
            ">>>> Epoch 15298:\t Train Loss: 2.5350\n",
            ">>>> Epoch 15299:\t Train Loss: 2.1538\n",
            ">>>> Epoch 15300:\t Train Loss: 2.3604\n",
            ">>>> Epoch 15301:\t Train Loss: 2.0956\n",
            ">>>> Epoch 15302:\t Train Loss: 2.1258\n",
            ">>>> Epoch 15303:\t Train Loss: 2.1396\n",
            ">>>> Epoch 15304:\t Train Loss: 2.2115\n",
            ">>>> Epoch 15305:\t Train Loss: 2.5446\n",
            ">>>> Epoch 15306:\t Train Loss: 1.8209\n",
            ">>>> Epoch 15307:\t Train Loss: 2.1355\n",
            ">>>> Epoch 15308:\t Train Loss: 2.4645\n",
            ">>>> Epoch 15309:\t Train Loss: 2.5175\n",
            ">>>> Epoch 15310:\t Train Loss: 2.5365\n",
            ">>>> Epoch 15311:\t Train Loss: 2.4271\n",
            ">>>> Epoch 15312:\t Train Loss: 2.3508\n",
            ">>>> Epoch 15313:\t Train Loss: 1.9356\n",
            ">>>> Epoch 15314:\t Train Loss: 2.0180\n",
            ">>>> Epoch 15315:\t Train Loss: 2.2789\n",
            ">>>> Epoch 15316:\t Train Loss: 2.6308\n",
            ">>>> Epoch 15317:\t Train Loss: 2.0674\n",
            ">>>> Epoch 15318:\t Train Loss: 2.1847\n",
            ">>>> Epoch 15319:\t Train Loss: 1.9844\n",
            ">>>> Epoch 15320:\t Train Loss: 2.4553\n",
            ">>>> Epoch 15321:\t Train Loss: 2.2363\n",
            ">>>> Epoch 15322:\t Train Loss: 2.3918\n",
            ">>>> Epoch 15323:\t Train Loss: 2.4448\n",
            ">>>> Epoch 15324:\t Train Loss: 2.4988\n",
            ">>>> Epoch 15325:\t Train Loss: 2.2343\n",
            ">>>> Epoch 15326:\t Train Loss: 2.2965\n",
            ">>>> Epoch 15327:\t Train Loss: 2.5595\n",
            ">>>> Epoch 15328:\t Train Loss: 2.5012\n",
            ">>>> Epoch 15329:\t Train Loss: 2.5576\n",
            ">>>> Epoch 15330:\t Train Loss: 2.0619\n",
            ">>>> Epoch 15331:\t Train Loss: 1.9248\n",
            ">>>> Epoch 15332:\t Train Loss: 2.6003\n",
            ">>>> Epoch 15333:\t Train Loss: 2.3055\n",
            ">>>> Epoch 15334:\t Train Loss: 2.2691\n",
            ">>>> Epoch 15335:\t Train Loss: 2.5965\n",
            ">>>> Epoch 15336:\t Train Loss: 2.4628\n",
            ">>>> Epoch 15337:\t Train Loss: 2.4234\n",
            ">>>> Epoch 15338:\t Train Loss: 1.9152\n",
            ">>>> Epoch 15339:\t Train Loss: 2.3496\n",
            ">>>> Epoch 15340:\t Train Loss: 2.4815\n",
            ">>>> Epoch 15341:\t Train Loss: 2.3964\n",
            ">>>> Epoch 15342:\t Train Loss: 2.4288\n",
            ">>>> Epoch 15343:\t Train Loss: 2.2661\n",
            ">>>> Epoch 15344:\t Train Loss: 2.4554\n",
            ">>>> Epoch 15345:\t Train Loss: 2.5071\n",
            ">>>> Epoch 15346:\t Train Loss: 2.1446\n",
            ">>>> Epoch 15347:\t Train Loss: 1.8898\n",
            ">>>> Epoch 15348:\t Train Loss: 2.1924\n",
            ">>>> Epoch 15349:\t Train Loss: 2.4983\n",
            ">>>> Epoch 15350:\t Train Loss: 2.0619\n",
            ">>>> Epoch 15351:\t Train Loss: 2.3276\n",
            ">>>> Epoch 15352:\t Train Loss: 2.1625\n",
            ">>>> Epoch 15353:\t Train Loss: 2.2621\n",
            ">>>> Epoch 15354:\t Train Loss: 1.9542\n",
            ">>>> Epoch 15355:\t Train Loss: 2.0355\n",
            ">>>> Epoch 15356:\t Train Loss: 2.4814\n",
            ">>>> Epoch 15357:\t Train Loss: 2.2747\n",
            ">>>> Epoch 15358:\t Train Loss: 2.2437\n",
            ">>>> Epoch 15359:\t Train Loss: 2.3440\n",
            ">>>> Epoch 15360:\t Train Loss: 2.5057\n",
            ">>>> Epoch 15361:\t Train Loss: 2.0498\n",
            ">>>> Epoch 15362:\t Train Loss: 2.4206\n",
            ">>>> Epoch 15363:\t Train Loss: 2.1755\n",
            ">>>> Epoch 15364:\t Train Loss: 2.0673\n",
            ">>>> Epoch 15365:\t Train Loss: 1.7928\n",
            ">>>> Epoch 15366:\t Train Loss: 2.7130\n",
            ">>>> Epoch 15367:\t Train Loss: 2.2737\n",
            ">>>> Epoch 15368:\t Train Loss: 2.1473\n",
            ">>>> Epoch 15369:\t Train Loss: 2.0417\n",
            ">>>> Epoch 15370:\t Train Loss: 2.0104\n",
            ">>>> Epoch 15371:\t Train Loss: 2.4174\n",
            ">>>> Epoch 15372:\t Train Loss: 2.3912\n",
            ">>>> Epoch 15373:\t Train Loss: 2.1533\n",
            ">>>> Epoch 15374:\t Train Loss: 2.7766\n",
            ">>>> Epoch 15375:\t Train Loss: 2.5442\n",
            ">>>> Epoch 15376:\t Train Loss: 2.4063\n",
            ">>>> Epoch 15377:\t Train Loss: 2.2261\n",
            ">>>> Epoch 15378:\t Train Loss: 2.1570\n",
            ">>>> Epoch 15379:\t Train Loss: 2.7372\n",
            ">>>> Epoch 15380:\t Train Loss: 2.1389\n",
            ">>>> Epoch 15381:\t Train Loss: 2.2107\n",
            ">>>> Epoch 15382:\t Train Loss: 1.8939\n",
            ">>>> Epoch 15383:\t Train Loss: 2.4191\n",
            ">>>> Epoch 15384:\t Train Loss: 1.9028\n",
            ">>>> Epoch 15385:\t Train Loss: 2.0784\n",
            ">>>> Epoch 15386:\t Train Loss: 2.6872\n",
            ">>>> Epoch 15387:\t Train Loss: 2.6828\n",
            ">>>> Epoch 15388:\t Train Loss: 1.9801\n",
            ">>>> Epoch 15389:\t Train Loss: 1.9098\n",
            ">>>> Epoch 15390:\t Train Loss: 2.1956\n",
            ">>>> Epoch 15391:\t Train Loss: 2.2791\n",
            ">>>> Epoch 15392:\t Train Loss: 2.2309\n",
            ">>>> Epoch 15393:\t Train Loss: 2.0942\n",
            ">>>> Epoch 15394:\t Train Loss: 2.3435\n",
            ">>>> Epoch 15395:\t Train Loss: 2.1636\n",
            ">>>> Epoch 15396:\t Train Loss: 2.5348\n",
            ">>>> Epoch 15397:\t Train Loss: 1.8011\n",
            ">>>> Epoch 15398:\t Train Loss: 1.9702\n",
            ">>>> Epoch 15399:\t Train Loss: 2.3640\n",
            ">>>> Epoch 15400:\t Train Loss: 2.2961\n",
            ">>>> Epoch 15401:\t Train Loss: 2.1748\n",
            ">>>> Epoch 15402:\t Train Loss: 2.1277\n",
            ">>>> Epoch 15403:\t Train Loss: 2.3650\n",
            ">>>> Epoch 15404:\t Train Loss: 2.2436\n",
            ">>>> Epoch 15405:\t Train Loss: 2.0129\n",
            ">>>> Epoch 15406:\t Train Loss: 2.1507\n",
            ">>>> Epoch 15407:\t Train Loss: 2.1397\n",
            ">>>> Epoch 15408:\t Train Loss: 2.3231\n",
            ">>>> Epoch 15409:\t Train Loss: 2.0456\n",
            ">>>> Epoch 15410:\t Train Loss: 2.5388\n",
            ">>>> Epoch 15411:\t Train Loss: 2.1025\n",
            ">>>> Epoch 15412:\t Train Loss: 2.5055\n",
            ">>>> Epoch 15413:\t Train Loss: 2.2632\n",
            ">>>> Epoch 15414:\t Train Loss: 2.1758\n",
            ">>>> Epoch 15415:\t Train Loss: 2.6126\n",
            ">>>> Epoch 15416:\t Train Loss: 1.9948\n",
            ">>>> Epoch 15417:\t Train Loss: 2.1655\n",
            ">>>> Epoch 15418:\t Train Loss: 2.0530\n",
            ">>>> Epoch 15419:\t Train Loss: 2.3043\n",
            ">>>> Epoch 15420:\t Train Loss: 2.5417\n",
            ">>>> Epoch 15421:\t Train Loss: 2.0673\n",
            ">>>> Epoch 15422:\t Train Loss: 2.3578\n",
            ">>>> Epoch 15423:\t Train Loss: 2.6697\n",
            ">>>> Epoch 15424:\t Train Loss: 2.2543\n",
            ">>>> Epoch 15425:\t Train Loss: 2.0222\n",
            ">>>> Epoch 15426:\t Train Loss: 2.2293\n",
            ">>>> Epoch 15427:\t Train Loss: 2.4833\n",
            ">>>> Epoch 15428:\t Train Loss: 2.4852\n",
            ">>>> Epoch 15429:\t Train Loss: 2.0716\n",
            ">>>> Epoch 15430:\t Train Loss: 2.2116\n",
            ">>>> Epoch 15431:\t Train Loss: 2.4335\n",
            ">>>> Epoch 15432:\t Train Loss: 2.2129\n",
            ">>>> Epoch 15433:\t Train Loss: 2.6844\n",
            ">>>> Epoch 15434:\t Train Loss: 2.1045\n",
            ">>>> Epoch 15435:\t Train Loss: 2.1657\n",
            ">>>> Epoch 15436:\t Train Loss: 2.2793\n",
            ">>>> Epoch 15437:\t Train Loss: 2.5727\n",
            ">>>> Epoch 15438:\t Train Loss: 2.3390\n",
            ">>>> Epoch 15439:\t Train Loss: 2.1662\n",
            ">>>> Epoch 15440:\t Train Loss: 2.1828\n",
            ">>>> Epoch 15441:\t Train Loss: 2.2951\n",
            ">>>> Epoch 15442:\t Train Loss: 2.6795\n",
            ">>>> Epoch 15443:\t Train Loss: 2.2932\n",
            ">>>> Epoch 15444:\t Train Loss: 2.1782\n",
            ">>>> Epoch 15445:\t Train Loss: 2.3346\n",
            ">>>> Epoch 15446:\t Train Loss: 2.2561\n",
            ">>>> Epoch 15447:\t Train Loss: 2.2540\n",
            ">>>> Epoch 15448:\t Train Loss: 2.2554\n",
            ">>>> Epoch 15449:\t Train Loss: 2.1718\n",
            ">>>> Epoch 15450:\t Train Loss: 2.3169\n",
            ">>>> Epoch 15451:\t Train Loss: 1.9082\n",
            ">>>> Epoch 15452:\t Train Loss: 2.3247\n",
            ">>>> Epoch 15453:\t Train Loss: 2.2941\n",
            ">>>> Epoch 15454:\t Train Loss: 2.1756\n",
            ">>>> Epoch 15455:\t Train Loss: 1.9502\n",
            ">>>> Epoch 15456:\t Train Loss: 1.9762\n",
            ">>>> Epoch 15457:\t Train Loss: 2.2029\n",
            ">>>> Epoch 15458:\t Train Loss: 2.3207\n",
            ">>>> Epoch 15459:\t Train Loss: 1.8623\n",
            ">>>> Epoch 15460:\t Train Loss: 2.3275\n",
            ">>>> Epoch 15461:\t Train Loss: 2.5857\n",
            ">>>> Epoch 15462:\t Train Loss: 2.3285\n",
            ">>>> Epoch 15463:\t Train Loss: 2.2072\n",
            ">>>> Epoch 15464:\t Train Loss: 2.0118\n",
            ">>>> Epoch 15465:\t Train Loss: 2.1415\n",
            ">>>> Epoch 15466:\t Train Loss: 2.2325\n",
            ">>>> Epoch 15467:\t Train Loss: 1.9326\n",
            ">>>> Epoch 15468:\t Train Loss: 2.1982\n",
            ">>>> Epoch 15469:\t Train Loss: 2.2286\n",
            ">>>> Epoch 15470:\t Train Loss: 2.2207\n",
            ">>>> Epoch 15471:\t Train Loss: 2.6122\n",
            ">>>> Epoch 15472:\t Train Loss: 2.6544\n",
            ">>>> Epoch 15473:\t Train Loss: 2.1366\n",
            ">>>> Epoch 15474:\t Train Loss: 2.5100\n",
            ">>>> Epoch 15475:\t Train Loss: 2.5108\n",
            ">>>> Epoch 15476:\t Train Loss: 2.4866\n",
            ">>>> Epoch 15477:\t Train Loss: 2.1497\n",
            ">>>> Epoch 15478:\t Train Loss: 2.0937\n",
            ">>>> Epoch 15479:\t Train Loss: 2.3902\n",
            ">>>> Epoch 15480:\t Train Loss: 2.2205\n",
            ">>>> Epoch 15481:\t Train Loss: 2.5184\n",
            ">>>> Epoch 15482:\t Train Loss: 2.3272\n",
            ">>>> Epoch 15483:\t Train Loss: 1.9245\n",
            ">>>> Epoch 15484:\t Train Loss: 1.8457\n",
            ">>>> Epoch 15485:\t Train Loss: 2.1047\n",
            ">>>> Epoch 15486:\t Train Loss: 1.9310\n",
            ">>>> Epoch 15487:\t Train Loss: 2.5471\n",
            ">>>> Epoch 15488:\t Train Loss: 2.4960\n",
            ">>>> Epoch 15489:\t Train Loss: 2.2761\n",
            ">>>> Epoch 15490:\t Train Loss: 2.5189\n",
            ">>>> Epoch 15491:\t Train Loss: 2.7022\n",
            ">>>> Epoch 15492:\t Train Loss: 2.4590\n",
            ">>>> Epoch 15493:\t Train Loss: 1.8828\n",
            ">>>> Epoch 15494:\t Train Loss: 2.5355\n",
            ">>>> Epoch 15495:\t Train Loss: 2.2776\n",
            ">>>> Epoch 15496:\t Train Loss: 2.1638\n",
            ">>>> Epoch 15497:\t Train Loss: 2.3668\n",
            ">>>> Epoch 15498:\t Train Loss: 2.0790\n",
            ">>>> Epoch 15499:\t Train Loss: 2.7657\n",
            ">>>> Epoch 15500:\t Train Loss: 2.0448\n",
            ">>>> Epoch 15501:\t Train Loss: 2.3439\n",
            ">>>> Epoch 15502:\t Train Loss: 2.1635\n",
            ">>>> Epoch 15503:\t Train Loss: 2.2330\n",
            ">>>> Epoch 15504:\t Train Loss: 2.4235\n",
            ">>>> Epoch 15505:\t Train Loss: 2.1734\n",
            ">>>> Epoch 15506:\t Train Loss: 2.4884\n",
            ">>>> Epoch 15507:\t Train Loss: 2.0556\n",
            ">>>> Epoch 15508:\t Train Loss: 2.5334\n",
            ">>>> Epoch 15509:\t Train Loss: 2.3403\n",
            ">>>> Epoch 15510:\t Train Loss: 2.8315\n",
            ">>>> Epoch 15511:\t Train Loss: 1.8772\n",
            ">>>> Epoch 15512:\t Train Loss: 2.5978\n",
            ">>>> Epoch 15513:\t Train Loss: 2.2862\n",
            ">>>> Epoch 15514:\t Train Loss: 2.0770\n",
            ">>>> Epoch 15515:\t Train Loss: 2.0535\n",
            ">>>> Epoch 15516:\t Train Loss: 2.1035\n",
            ">>>> Epoch 15517:\t Train Loss: 2.3918\n",
            ">>>> Epoch 15518:\t Train Loss: 2.1774\n",
            ">>>> Epoch 15519:\t Train Loss: 2.2517\n",
            ">>>> Epoch 15520:\t Train Loss: 2.1421\n",
            ">>>> Epoch 15521:\t Train Loss: 2.7403\n",
            ">>>> Epoch 15522:\t Train Loss: 2.2419\n",
            ">>>> Epoch 15523:\t Train Loss: 2.3173\n",
            ">>>> Epoch 15524:\t Train Loss: 2.4696\n",
            ">>>> Epoch 15525:\t Train Loss: 1.8793\n",
            ">>>> Epoch 15526:\t Train Loss: 2.1303\n",
            ">>>> Epoch 15527:\t Train Loss: 2.6643\n",
            ">>>> Epoch 15528:\t Train Loss: 2.1110\n",
            ">>>> Epoch 15529:\t Train Loss: 2.2149\n",
            ">>>> Epoch 15530:\t Train Loss: 2.5087\n",
            ">>>> Epoch 15531:\t Train Loss: 2.3483\n",
            ">>>> Epoch 15532:\t Train Loss: 2.3793\n",
            ">>>> Epoch 15533:\t Train Loss: 2.2282\n",
            ">>>> Epoch 15534:\t Train Loss: 2.0907\n",
            ">>>> Epoch 15535:\t Train Loss: 2.3899\n",
            ">>>> Epoch 15536:\t Train Loss: 2.3742\n",
            ">>>> Epoch 15537:\t Train Loss: 2.0238\n",
            ">>>> Epoch 15538:\t Train Loss: 2.1725\n",
            ">>>> Epoch 15539:\t Train Loss: 2.0304\n",
            ">>>> Epoch 15540:\t Train Loss: 2.1682\n",
            ">>>> Epoch 15541:\t Train Loss: 2.1136\n",
            ">>>> Epoch 15542:\t Train Loss: 2.2798\n",
            ">>>> Epoch 15543:\t Train Loss: 2.7218\n",
            ">>>> Epoch 15544:\t Train Loss: 2.2485\n",
            ">>>> Epoch 15545:\t Train Loss: 2.1967\n",
            ">>>> Epoch 15546:\t Train Loss: 2.1021\n",
            ">>>> Epoch 15547:\t Train Loss: 2.3500\n",
            ">>>> Epoch 15548:\t Train Loss: 2.0963\n",
            ">>>> Epoch 15549:\t Train Loss: 2.5219\n",
            ">>>> Epoch 15550:\t Train Loss: 2.3334\n",
            ">>>> Epoch 15551:\t Train Loss: 2.4554\n",
            ">>>> Epoch 15552:\t Train Loss: 2.0967\n",
            ">>>> Epoch 15553:\t Train Loss: 2.0880\n",
            ">>>> Epoch 15554:\t Train Loss: 2.2966\n",
            ">>>> Epoch 15555:\t Train Loss: 2.7026\n",
            ">>>> Epoch 15556:\t Train Loss: 2.2959\n",
            ">>>> Epoch 15557:\t Train Loss: 2.3528\n",
            ">>>> Epoch 15558:\t Train Loss: 2.4185\n",
            ">>>> Epoch 15559:\t Train Loss: 2.6084\n",
            ">>>> Epoch 15560:\t Train Loss: 2.1589\n",
            ">>>> Epoch 15561:\t Train Loss: 2.1495\n",
            ">>>> Epoch 15562:\t Train Loss: 2.4151\n",
            ">>>> Epoch 15563:\t Train Loss: 2.0891\n",
            ">>>> Epoch 15564:\t Train Loss: 2.2055\n",
            ">>>> Epoch 15565:\t Train Loss: 2.3970\n",
            ">>>> Epoch 15566:\t Train Loss: 2.5167\n",
            ">>>> Epoch 15567:\t Train Loss: 2.2604\n",
            ">>>> Epoch 15568:\t Train Loss: 2.3525\n",
            ">>>> Epoch 15569:\t Train Loss: 2.3116\n",
            ">>>> Epoch 15570:\t Train Loss: 2.5542\n",
            ">>>> Epoch 15571:\t Train Loss: 2.0979\n",
            ">>>> Epoch 15572:\t Train Loss: 2.1318\n",
            ">>>> Epoch 15573:\t Train Loss: 2.2625\n",
            ">>>> Epoch 15574:\t Train Loss: 2.4892\n",
            ">>>> Epoch 15575:\t Train Loss: 2.2934\n",
            ">>>> Epoch 15576:\t Train Loss: 2.2044\n",
            ">>>> Epoch 15577:\t Train Loss: 2.3527\n",
            ">>>> Epoch 15578:\t Train Loss: 2.0591\n",
            ">>>> Epoch 15579:\t Train Loss: 2.2149\n",
            ">>>> Epoch 15580:\t Train Loss: 2.4507\n",
            ">>>> Epoch 15581:\t Train Loss: 2.1361\n",
            ">>>> Epoch 15582:\t Train Loss: 2.0103\n",
            ">>>> Epoch 15583:\t Train Loss: 2.2500\n",
            ">>>> Epoch 15584:\t Train Loss: 2.0454\n",
            ">>>> Epoch 15585:\t Train Loss: 2.4281\n",
            ">>>> Epoch 15586:\t Train Loss: 1.9704\n",
            ">>>> Epoch 15587:\t Train Loss: 2.2932\n",
            ">>>> Epoch 15588:\t Train Loss: 2.4108\n",
            ">>>> Epoch 15589:\t Train Loss: 2.3026\n",
            ">>>> Epoch 15590:\t Train Loss: 2.4009\n",
            ">>>> Epoch 15591:\t Train Loss: 2.4882\n",
            ">>>> Epoch 15592:\t Train Loss: 2.5165\n",
            ">>>> Epoch 15593:\t Train Loss: 2.1872\n",
            ">>>> Epoch 15594:\t Train Loss: 2.0777\n",
            ">>>> Epoch 15595:\t Train Loss: 2.1795\n",
            ">>>> Epoch 15596:\t Train Loss: 2.3425\n",
            ">>>> Epoch 15597:\t Train Loss: 1.9415\n",
            ">>>> Epoch 15598:\t Train Loss: 2.1142\n",
            ">>>> Epoch 15599:\t Train Loss: 2.2916\n",
            ">>>> Epoch 15600:\t Train Loss: 2.2996\n",
            ">>>> Epoch 15601:\t Train Loss: 2.1616\n",
            ">>>> Epoch 15602:\t Train Loss: 2.5662\n",
            ">>>> Epoch 15603:\t Train Loss: 2.0420\n",
            ">>>> Epoch 15604:\t Train Loss: 2.2556\n",
            ">>>> Epoch 15605:\t Train Loss: 2.2052\n",
            ">>>> Epoch 15606:\t Train Loss: 2.2627\n",
            ">>>> Epoch 15607:\t Train Loss: 2.4308\n",
            ">>>> Epoch 15608:\t Train Loss: 2.1703\n",
            ">>>> Epoch 15609:\t Train Loss: 2.3268\n",
            ">>>> Epoch 15610:\t Train Loss: 2.2208\n",
            ">>>> Epoch 15611:\t Train Loss: 2.4260\n",
            ">>>> Epoch 15612:\t Train Loss: 2.2604\n",
            ">>>> Epoch 15613:\t Train Loss: 2.1339\n",
            ">>>> Epoch 15614:\t Train Loss: 2.1781\n",
            ">>>> Epoch 15615:\t Train Loss: 2.0283\n",
            ">>>> Epoch 15616:\t Train Loss: 2.0857\n",
            ">>>> Epoch 15617:\t Train Loss: 2.1667\n",
            ">>>> Epoch 15618:\t Train Loss: 2.2497\n",
            ">>>> Epoch 15619:\t Train Loss: 2.5139\n",
            ">>>> Epoch 15620:\t Train Loss: 2.3127\n",
            ">>>> Epoch 15621:\t Train Loss: 1.9099\n",
            ">>>> Epoch 15622:\t Train Loss: 2.4492\n",
            ">>>> Epoch 15623:\t Train Loss: 2.3387\n",
            ">>>> Epoch 15624:\t Train Loss: 2.3799\n",
            ">>>> Epoch 15625:\t Train Loss: 2.3806\n",
            ">>>> Epoch 15626:\t Train Loss: 1.8910\n",
            ">>>> Epoch 15627:\t Train Loss: 2.7477\n",
            ">>>> Epoch 15628:\t Train Loss: 2.3830\n",
            ">>>> Epoch 15629:\t Train Loss: 2.3558\n",
            ">>>> Epoch 15630:\t Train Loss: 2.2675\n",
            ">>>> Epoch 15631:\t Train Loss: 2.2962\n",
            ">>>> Epoch 15632:\t Train Loss: 2.3710\n",
            ">>>> Epoch 15633:\t Train Loss: 2.3068\n",
            ">>>> Epoch 15634:\t Train Loss: 2.4267\n",
            ">>>> Epoch 15635:\t Train Loss: 2.2942\n",
            ">>>> Epoch 15636:\t Train Loss: 2.1995\n",
            ">>>> Epoch 15637:\t Train Loss: 2.3058\n",
            ">>>> Epoch 15638:\t Train Loss: 2.1551\n",
            ">>>> Epoch 15639:\t Train Loss: 2.6043\n",
            ">>>> Epoch 15640:\t Train Loss: 2.1199\n",
            ">>>> Epoch 15641:\t Train Loss: 2.7127\n",
            ">>>> Epoch 15642:\t Train Loss: 2.1903\n",
            ">>>> Epoch 15643:\t Train Loss: 2.3527\n",
            ">>>> Epoch 15644:\t Train Loss: 2.1914\n",
            ">>>> Epoch 15645:\t Train Loss: 2.3280\n",
            ">>>> Epoch 15646:\t Train Loss: 2.0770\n",
            ">>>> Epoch 15647:\t Train Loss: 2.5349\n",
            ">>>> Epoch 15648:\t Train Loss: 2.3459\n",
            ">>>> Epoch 15649:\t Train Loss: 2.3080\n",
            ">>>> Epoch 15650:\t Train Loss: 2.2849\n",
            ">>>> Epoch 15651:\t Train Loss: 2.3830\n",
            ">>>> Epoch 15652:\t Train Loss: 2.0491\n",
            ">>>> Epoch 15653:\t Train Loss: 2.1010\n",
            ">>>> Epoch 15654:\t Train Loss: 2.3195\n",
            ">>>> Epoch 15655:\t Train Loss: 2.3579\n",
            ">>>> Epoch 15656:\t Train Loss: 2.2404\n",
            ">>>> Epoch 15657:\t Train Loss: 1.9916\n",
            ">>>> Epoch 15658:\t Train Loss: 2.3472\n",
            ">>>> Epoch 15659:\t Train Loss: 2.3723\n",
            ">>>> Epoch 15660:\t Train Loss: 1.9744\n",
            ">>>> Epoch 15661:\t Train Loss: 2.1095\n",
            ">>>> Epoch 15662:\t Train Loss: 2.1068\n",
            ">>>> Epoch 15663:\t Train Loss: 2.2792\n",
            ">>>> Epoch 15664:\t Train Loss: 1.9681\n",
            ">>>> Epoch 15665:\t Train Loss: 1.9533\n",
            ">>>> Epoch 15666:\t Train Loss: 2.2392\n",
            ">>>> Epoch 15667:\t Train Loss: 2.2963\n",
            ">>>> Epoch 15668:\t Train Loss: 2.5417\n",
            ">>>> Epoch 15669:\t Train Loss: 1.9943\n",
            ">>>> Epoch 15670:\t Train Loss: 2.7080\n",
            ">>>> Epoch 15671:\t Train Loss: 2.2095\n",
            ">>>> Epoch 15672:\t Train Loss: 2.3966\n",
            ">>>> Epoch 15673:\t Train Loss: 2.0313\n",
            ">>>> Epoch 15674:\t Train Loss: 2.4092\n",
            ">>>> Epoch 15675:\t Train Loss: 2.4044\n",
            ">>>> Epoch 15676:\t Train Loss: 2.1819\n",
            ">>>> Epoch 15677:\t Train Loss: 2.4474\n",
            ">>>> Epoch 15678:\t Train Loss: 1.9211\n",
            ">>>> Epoch 15679:\t Train Loss: 2.2252\n",
            ">>>> Epoch 15680:\t Train Loss: 2.4416\n",
            ">>>> Epoch 15681:\t Train Loss: 2.0232\n",
            ">>>> Epoch 15682:\t Train Loss: 2.2224\n",
            ">>>> Epoch 15683:\t Train Loss: 2.1641\n",
            ">>>> Epoch 15684:\t Train Loss: 2.3370\n",
            ">>>> Epoch 15685:\t Train Loss: 2.1254\n",
            ">>>> Epoch 15686:\t Train Loss: 2.5259\n",
            ">>>> Epoch 15687:\t Train Loss: 2.4450\n",
            ">>>> Epoch 15688:\t Train Loss: 2.1598\n",
            ">>>> Epoch 15689:\t Train Loss: 2.4352\n",
            ">>>> Epoch 15690:\t Train Loss: 1.9706\n",
            ">>>> Epoch 15691:\t Train Loss: 2.2098\n",
            ">>>> Epoch 15692:\t Train Loss: 2.3319\n",
            ">>>> Epoch 15693:\t Train Loss: 2.4958\n",
            ">>>> Epoch 15694:\t Train Loss: 2.1372\n",
            ">>>> Epoch 15695:\t Train Loss: 2.5064\n",
            ">>>> Epoch 15696:\t Train Loss: 2.0974\n",
            ">>>> Epoch 15697:\t Train Loss: 2.1231\n",
            ">>>> Epoch 15698:\t Train Loss: 2.2774\n",
            ">>>> Epoch 15699:\t Train Loss: 2.0825\n",
            ">>>> Epoch 15700:\t Train Loss: 2.6446\n",
            ">>>> Epoch 15701:\t Train Loss: 2.2028\n",
            ">>>> Epoch 15702:\t Train Loss: 2.0618\n",
            ">>>> Epoch 15703:\t Train Loss: 2.7406\n",
            ">>>> Epoch 15704:\t Train Loss: 2.2844\n",
            ">>>> Epoch 15705:\t Train Loss: 2.4177\n",
            ">>>> Epoch 15706:\t Train Loss: 2.1430\n",
            ">>>> Epoch 15707:\t Train Loss: 1.9699\n",
            ">>>> Epoch 15708:\t Train Loss: 2.1382\n",
            ">>>> Epoch 15709:\t Train Loss: 1.8571\n",
            ">>>> Epoch 15710:\t Train Loss: 2.2343\n",
            ">>>> Epoch 15711:\t Train Loss: 1.9079\n",
            ">>>> Epoch 15712:\t Train Loss: 2.3492\n",
            ">>>> Epoch 15713:\t Train Loss: 2.3195\n",
            ">>>> Epoch 15714:\t Train Loss: 2.1153\n",
            ">>>> Epoch 15715:\t Train Loss: 1.8733\n",
            ">>>> Epoch 15716:\t Train Loss: 2.0698\n",
            ">>>> Epoch 15717:\t Train Loss: 2.3617\n",
            ">>>> Epoch 15718:\t Train Loss: 2.1594\n",
            ">>>> Epoch 15719:\t Train Loss: 2.2656\n",
            ">>>> Epoch 15720:\t Train Loss: 2.4352\n",
            ">>>> Epoch 15721:\t Train Loss: 2.2218\n",
            ">>>> Epoch 15722:\t Train Loss: 1.7691\n",
            ">>>> Epoch 15723:\t Train Loss: 2.1770\n",
            ">>>> Epoch 15724:\t Train Loss: 2.3068\n",
            ">>>> Epoch 15725:\t Train Loss: 2.0993\n",
            ">>>> Epoch 15726:\t Train Loss: 2.4664\n",
            ">>>> Epoch 15727:\t Train Loss: 2.1665\n",
            ">>>> Epoch 15728:\t Train Loss: 2.2032\n",
            ">>>> Epoch 15729:\t Train Loss: 2.2495\n",
            ">>>> Epoch 15730:\t Train Loss: 2.4115\n",
            ">>>> Epoch 15731:\t Train Loss: 2.3426\n",
            ">>>> Epoch 15732:\t Train Loss: 2.0601\n",
            ">>>> Epoch 15733:\t Train Loss: 2.7523\n",
            ">>>> Epoch 15734:\t Train Loss: 2.1564\n",
            ">>>> Epoch 15735:\t Train Loss: 2.5483\n",
            ">>>> Epoch 15736:\t Train Loss: 2.1648\n",
            ">>>> Epoch 15737:\t Train Loss: 2.6302\n",
            ">>>> Epoch 15738:\t Train Loss: 2.1035\n",
            ">>>> Epoch 15739:\t Train Loss: 2.2828\n",
            ">>>> Epoch 15740:\t Train Loss: 2.3310\n",
            ">>>> Epoch 15741:\t Train Loss: 1.9905\n",
            ">>>> Epoch 15742:\t Train Loss: 1.9307\n",
            ">>>> Epoch 15743:\t Train Loss: 2.5809\n",
            ">>>> Epoch 15744:\t Train Loss: 1.9627\n",
            ">>>> Epoch 15745:\t Train Loss: 2.2666\n",
            ">>>> Epoch 15746:\t Train Loss: 2.5983\n",
            ">>>> Epoch 15747:\t Train Loss: 2.0597\n",
            ">>>> Epoch 15748:\t Train Loss: 2.0570\n",
            ">>>> Epoch 15749:\t Train Loss: 2.0762\n",
            ">>>> Epoch 15750:\t Train Loss: 2.3999\n",
            ">>>> Epoch 15751:\t Train Loss: 2.4216\n",
            ">>>> Epoch 15752:\t Train Loss: 2.1463\n",
            ">>>> Epoch 15753:\t Train Loss: 2.0873\n",
            ">>>> Epoch 15754:\t Train Loss: 2.4298\n",
            ">>>> Epoch 15755:\t Train Loss: 2.4238\n",
            ">>>> Epoch 15756:\t Train Loss: 2.5869\n",
            ">>>> Epoch 15757:\t Train Loss: 2.0896\n",
            ">>>> Epoch 15758:\t Train Loss: 2.2708\n",
            ">>>> Epoch 15759:\t Train Loss: 2.0179\n",
            ">>>> Epoch 15760:\t Train Loss: 2.1612\n",
            ">>>> Epoch 15761:\t Train Loss: 2.3618\n",
            ">>>> Epoch 15762:\t Train Loss: 2.3841\n",
            ">>>> Epoch 15763:\t Train Loss: 2.3364\n",
            ">>>> Epoch 15764:\t Train Loss: 2.2416\n",
            ">>>> Epoch 15765:\t Train Loss: 2.5139\n",
            ">>>> Epoch 15766:\t Train Loss: 2.5149\n",
            ">>>> Epoch 15767:\t Train Loss: 2.2572\n",
            ">>>> Epoch 15768:\t Train Loss: 2.2646\n",
            ">>>> Epoch 15769:\t Train Loss: 2.2133\n",
            ">>>> Epoch 15770:\t Train Loss: 2.2529\n",
            ">>>> Epoch 15771:\t Train Loss: 2.1943\n",
            ">>>> Epoch 15772:\t Train Loss: 2.4358\n",
            ">>>> Epoch 15773:\t Train Loss: 2.4344\n",
            ">>>> Epoch 15774:\t Train Loss: 2.1349\n",
            ">>>> Epoch 15775:\t Train Loss: 2.3250\n",
            ">>>> Epoch 15776:\t Train Loss: 2.3071\n",
            ">>>> Epoch 15777:\t Train Loss: 1.9007\n",
            ">>>> Epoch 15778:\t Train Loss: 2.3839\n",
            ">>>> Epoch 15779:\t Train Loss: 2.1024\n",
            ">>>> Epoch 15780:\t Train Loss: 2.0652\n",
            ">>>> Epoch 15781:\t Train Loss: 2.4655\n",
            ">>>> Epoch 15782:\t Train Loss: 1.8856\n",
            ">>>> Epoch 15783:\t Train Loss: 2.2240\n",
            ">>>> Epoch 15784:\t Train Loss: 2.3182\n",
            ">>>> Epoch 15785:\t Train Loss: 2.0641\n",
            ">>>> Epoch 15786:\t Train Loss: 2.4001\n",
            ">>>> Epoch 15787:\t Train Loss: 1.8934\n",
            ">>>> Epoch 15788:\t Train Loss: 2.1165\n",
            ">>>> Epoch 15789:\t Train Loss: 2.6859\n",
            ">>>> Epoch 15790:\t Train Loss: 2.3783\n",
            ">>>> Epoch 15791:\t Train Loss: 2.1849\n",
            ">>>> Epoch 15792:\t Train Loss: 1.9309\n",
            ">>>> Epoch 15793:\t Train Loss: 2.4530\n",
            ">>>> Epoch 15794:\t Train Loss: 2.5384\n",
            ">>>> Epoch 15795:\t Train Loss: 2.3096\n",
            ">>>> Epoch 15796:\t Train Loss: 2.4945\n",
            ">>>> Epoch 15797:\t Train Loss: 2.2258\n",
            ">>>> Epoch 15798:\t Train Loss: 2.2396\n",
            ">>>> Epoch 15799:\t Train Loss: 2.2948\n",
            ">>>> Epoch 15800:\t Train Loss: 2.4526\n",
            ">>>> Epoch 15801:\t Train Loss: 2.4560\n",
            ">>>> Epoch 15802:\t Train Loss: 2.1218\n",
            ">>>> Epoch 15803:\t Train Loss: 2.2051\n",
            ">>>> Epoch 15804:\t Train Loss: 2.5539\n",
            ">>>> Epoch 15805:\t Train Loss: 2.0465\n",
            ">>>> Epoch 15806:\t Train Loss: 2.1337\n",
            ">>>> Epoch 15807:\t Train Loss: 2.4587\n",
            ">>>> Epoch 15808:\t Train Loss: 2.4695\n",
            ">>>> Epoch 15809:\t Train Loss: 2.7732\n",
            ">>>> Epoch 15810:\t Train Loss: 2.4915\n",
            ">>>> Epoch 15811:\t Train Loss: 2.0995\n",
            ">>>> Epoch 15812:\t Train Loss: 2.3480\n",
            ">>>> Epoch 15813:\t Train Loss: 2.1958\n",
            ">>>> Epoch 15814:\t Train Loss: 2.0567\n",
            ">>>> Epoch 15815:\t Train Loss: 2.2525\n",
            ">>>> Epoch 15816:\t Train Loss: 2.0913\n",
            ">>>> Epoch 15817:\t Train Loss: 2.2008\n",
            ">>>> Epoch 15818:\t Train Loss: 2.2379\n",
            ">>>> Epoch 15819:\t Train Loss: 2.3141\n",
            ">>>> Epoch 15820:\t Train Loss: 2.1810\n",
            ">>>> Epoch 15821:\t Train Loss: 2.0925\n",
            ">>>> Epoch 15822:\t Train Loss: 2.5862\n",
            ">>>> Epoch 15823:\t Train Loss: 2.1850\n",
            ">>>> Epoch 15824:\t Train Loss: 1.8088\n",
            ">>>> Epoch 15825:\t Train Loss: 2.3108\n",
            ">>>> Epoch 15826:\t Train Loss: 1.8469\n",
            ">>>> Epoch 15827:\t Train Loss: 2.4114\n",
            ">>>> Epoch 15828:\t Train Loss: 2.3958\n",
            ">>>> Epoch 15829:\t Train Loss: 2.4570\n",
            ">>>> Epoch 15830:\t Train Loss: 2.7264\n",
            ">>>> Epoch 15831:\t Train Loss: 1.9212\n",
            ">>>> Epoch 15832:\t Train Loss: 2.6644\n",
            ">>>> Epoch 15833:\t Train Loss: 2.1019\n",
            ">>>> Epoch 15834:\t Train Loss: 1.9522\n",
            ">>>> Epoch 15835:\t Train Loss: 2.1287\n",
            ">>>> Epoch 15836:\t Train Loss: 2.5970\n",
            ">>>> Epoch 15837:\t Train Loss: 2.5388\n",
            ">>>> Epoch 15838:\t Train Loss: 1.9592\n",
            ">>>> Epoch 15839:\t Train Loss: 1.8600\n",
            ">>>> Epoch 15840:\t Train Loss: 2.7009\n",
            ">>>> Epoch 15841:\t Train Loss: 2.2943\n",
            ">>>> Epoch 15842:\t Train Loss: 2.2982\n",
            ">>>> Epoch 15843:\t Train Loss: 1.8473\n",
            ">>>> Epoch 15844:\t Train Loss: 1.9300\n",
            ">>>> Epoch 15845:\t Train Loss: 2.6219\n",
            ">>>> Epoch 15846:\t Train Loss: 2.4883\n",
            ">>>> Epoch 15847:\t Train Loss: 2.4015\n",
            ">>>> Epoch 15848:\t Train Loss: 2.1297\n",
            ">>>> Epoch 15849:\t Train Loss: 2.4163\n",
            ">>>> Epoch 15850:\t Train Loss: 2.0533\n",
            ">>>> Epoch 15851:\t Train Loss: 2.5946\n",
            ">>>> Epoch 15852:\t Train Loss: 2.1369\n",
            ">>>> Epoch 15853:\t Train Loss: 2.0185\n",
            ">>>> Epoch 15854:\t Train Loss: 2.0204\n",
            ">>>> Epoch 15855:\t Train Loss: 2.1638\n",
            ">>>> Epoch 15856:\t Train Loss: 2.1379\n",
            ">>>> Epoch 15857:\t Train Loss: 2.2279\n",
            ">>>> Epoch 15858:\t Train Loss: 2.1670\n",
            ">>>> Epoch 15859:\t Train Loss: 2.3367\n",
            ">>>> Epoch 15860:\t Train Loss: 2.1128\n",
            ">>>> Epoch 15861:\t Train Loss: 2.0191\n",
            ">>>> Epoch 15862:\t Train Loss: 2.1154\n",
            ">>>> Epoch 15863:\t Train Loss: 2.2683\n",
            ">>>> Epoch 15864:\t Train Loss: 2.2561\n",
            ">>>> Epoch 15865:\t Train Loss: 2.3516\n",
            ">>>> Epoch 15866:\t Train Loss: 2.4071\n",
            ">>>> Epoch 15867:\t Train Loss: 2.2654\n",
            ">>>> Epoch 15868:\t Train Loss: 1.9526\n",
            ">>>> Epoch 15869:\t Train Loss: 2.0822\n",
            ">>>> Epoch 15870:\t Train Loss: 2.3205\n",
            ">>>> Epoch 15871:\t Train Loss: 2.1763\n",
            ">>>> Epoch 15872:\t Train Loss: 2.1327\n",
            ">>>> Epoch 15873:\t Train Loss: 2.3995\n",
            ">>>> Epoch 15874:\t Train Loss: 2.0230\n",
            ">>>> Epoch 15875:\t Train Loss: 2.4702\n",
            ">>>> Epoch 15876:\t Train Loss: 1.9294\n",
            ">>>> Epoch 15877:\t Train Loss: 2.4252\n",
            ">>>> Epoch 15878:\t Train Loss: 2.5141\n",
            ">>>> Epoch 15879:\t Train Loss: 2.1893\n",
            ">>>> Epoch 15880:\t Train Loss: 2.1815\n",
            ">>>> Epoch 15881:\t Train Loss: 2.5770\n",
            ">>>> Epoch 15882:\t Train Loss: 2.2961\n",
            ">>>> Epoch 15883:\t Train Loss: 2.2652\n",
            ">>>> Epoch 15884:\t Train Loss: 2.0760\n",
            ">>>> Epoch 15885:\t Train Loss: 2.5440\n",
            ">>>> Epoch 15886:\t Train Loss: 2.1766\n",
            ">>>> Epoch 15887:\t Train Loss: 2.1703\n",
            ">>>> Epoch 15888:\t Train Loss: 2.5859\n",
            ">>>> Epoch 15889:\t Train Loss: 2.4959\n",
            ">>>> Epoch 15890:\t Train Loss: 2.0392\n",
            ">>>> Epoch 15891:\t Train Loss: 2.5949\n",
            ">>>> Epoch 15892:\t Train Loss: 2.3531\n",
            ">>>> Epoch 15893:\t Train Loss: 2.1290\n",
            ">>>> Epoch 15894:\t Train Loss: 2.3620\n",
            ">>>> Epoch 15895:\t Train Loss: 2.2456\n",
            ">>>> Epoch 15896:\t Train Loss: 2.1621\n",
            ">>>> Epoch 15897:\t Train Loss: 2.3425\n",
            ">>>> Epoch 15898:\t Train Loss: 1.8544\n",
            ">>>> Epoch 15899:\t Train Loss: 1.9475\n",
            ">>>> Epoch 15900:\t Train Loss: 2.7993\n",
            ">>>> Epoch 15901:\t Train Loss: 1.9430\n",
            ">>>> Epoch 15902:\t Train Loss: 2.8861\n",
            ">>>> Epoch 15903:\t Train Loss: 2.0713\n",
            ">>>> Epoch 15904:\t Train Loss: 2.3218\n",
            ">>>> Epoch 15905:\t Train Loss: 2.0991\n",
            ">>>> Epoch 15906:\t Train Loss: 2.1813\n",
            ">>>> Epoch 15907:\t Train Loss: 2.3347\n",
            ">>>> Epoch 15908:\t Train Loss: 2.1453\n",
            ">>>> Epoch 15909:\t Train Loss: 2.0810\n",
            ">>>> Epoch 15910:\t Train Loss: 2.4586\n",
            ">>>> Epoch 15911:\t Train Loss: 2.0240\n",
            ">>>> Epoch 15912:\t Train Loss: 2.5743\n",
            ">>>> Epoch 15913:\t Train Loss: 2.2328\n",
            ">>>> Epoch 15914:\t Train Loss: 2.2898\n",
            ">>>> Epoch 15915:\t Train Loss: 2.1783\n",
            ">>>> Epoch 15916:\t Train Loss: 2.1780\n",
            ">>>> Epoch 15917:\t Train Loss: 1.8069\n",
            ">>>> Epoch 15918:\t Train Loss: 2.3143\n",
            ">>>> Epoch 15919:\t Train Loss: 2.1966\n",
            ">>>> Epoch 15920:\t Train Loss: 2.1603\n",
            ">>>> Epoch 15921:\t Train Loss: 2.0449\n",
            ">>>> Epoch 15922:\t Train Loss: 2.4445\n",
            ">>>> Epoch 15923:\t Train Loss: 2.2119\n",
            ">>>> Epoch 15924:\t Train Loss: 2.1684\n",
            ">>>> Epoch 15925:\t Train Loss: 2.1782\n",
            ">>>> Epoch 15926:\t Train Loss: 2.0740\n",
            ">>>> Epoch 15927:\t Train Loss: 1.9152\n",
            ">>>> Epoch 15928:\t Train Loss: 2.3583\n",
            ">>>> Epoch 15929:\t Train Loss: 2.6060\n",
            ">>>> Epoch 15930:\t Train Loss: 2.1206\n",
            ">>>> Epoch 15931:\t Train Loss: 2.1166\n",
            ">>>> Epoch 15932:\t Train Loss: 1.9078\n",
            ">>>> Epoch 15933:\t Train Loss: 1.8598\n",
            ">>>> Epoch 15934:\t Train Loss: 2.3934\n",
            ">>>> Epoch 15935:\t Train Loss: 2.2056\n",
            ">>>> Epoch 15936:\t Train Loss: 1.9164\n",
            ">>>> Epoch 15937:\t Train Loss: 1.8990\n",
            ">>>> Epoch 15938:\t Train Loss: 2.6963\n",
            ">>>> Epoch 15939:\t Train Loss: 1.8380\n",
            ">>>> Epoch 15940:\t Train Loss: 2.0637\n",
            ">>>> Epoch 15941:\t Train Loss: 2.4602\n",
            ">>>> Epoch 15942:\t Train Loss: 1.9832\n",
            ">>>> Epoch 15943:\t Train Loss: 2.1283\n",
            ">>>> Epoch 15944:\t Train Loss: 2.6064\n",
            ">>>> Epoch 15945:\t Train Loss: 2.4107\n",
            ">>>> Epoch 15946:\t Train Loss: 2.0703\n",
            ">>>> Epoch 15947:\t Train Loss: 2.2696\n",
            ">>>> Epoch 15948:\t Train Loss: 2.2449\n",
            ">>>> Epoch 15949:\t Train Loss: 2.1336\n",
            ">>>> Epoch 15950:\t Train Loss: 2.2010\n",
            ">>>> Epoch 15951:\t Train Loss: 2.1853\n",
            ">>>> Epoch 15952:\t Train Loss: 2.3829\n",
            ">>>> Epoch 15953:\t Train Loss: 2.7264\n",
            ">>>> Epoch 15954:\t Train Loss: 2.3271\n",
            ">>>> Epoch 15955:\t Train Loss: 2.1101\n",
            ">>>> Epoch 15956:\t Train Loss: 2.2769\n",
            ">>>> Epoch 15957:\t Train Loss: 2.1450\n",
            ">>>> Epoch 15958:\t Train Loss: 2.1952\n",
            ">>>> Epoch 15959:\t Train Loss: 2.4748\n",
            ">>>> Epoch 15960:\t Train Loss: 2.1298\n",
            ">>>> Epoch 15961:\t Train Loss: 2.3499\n",
            ">>>> Epoch 15962:\t Train Loss: 2.1272\n",
            ">>>> Epoch 15963:\t Train Loss: 2.1238\n",
            ">>>> Epoch 15964:\t Train Loss: 2.4875\n",
            ">>>> Epoch 15965:\t Train Loss: 2.3066\n",
            ">>>> Epoch 15966:\t Train Loss: 2.0251\n",
            ">>>> Epoch 15967:\t Train Loss: 2.2051\n",
            ">>>> Epoch 15968:\t Train Loss: 2.2254\n",
            ">>>> Epoch 15969:\t Train Loss: 1.9462\n",
            ">>>> Epoch 15970:\t Train Loss: 2.0968\n",
            ">>>> Epoch 15971:\t Train Loss: 2.0331\n",
            ">>>> Epoch 15972:\t Train Loss: 2.5865\n",
            ">>>> Epoch 15973:\t Train Loss: 2.1257\n",
            ">>>> Epoch 15974:\t Train Loss: 2.3676\n",
            ">>>> Epoch 15975:\t Train Loss: 2.1672\n",
            ">>>> Epoch 15976:\t Train Loss: 2.3708\n",
            ">>>> Epoch 15977:\t Train Loss: 2.1333\n",
            ">>>> Epoch 15978:\t Train Loss: 2.0334\n",
            ">>>> Epoch 15979:\t Train Loss: 2.4612\n",
            ">>>> Epoch 15980:\t Train Loss: 2.2037\n",
            ">>>> Epoch 15981:\t Train Loss: 2.2416\n",
            ">>>> Epoch 15982:\t Train Loss: 2.2310\n",
            ">>>> Epoch 15983:\t Train Loss: 2.1876\n",
            ">>>> Epoch 15984:\t Train Loss: 2.2626\n",
            ">>>> Epoch 15985:\t Train Loss: 1.9508\n",
            ">>>> Epoch 15986:\t Train Loss: 2.2502\n",
            ">>>> Epoch 15987:\t Train Loss: 2.5881\n",
            ">>>> Epoch 15988:\t Train Loss: 2.1219\n",
            ">>>> Epoch 15989:\t Train Loss: 2.0266\n",
            ">>>> Epoch 15990:\t Train Loss: 2.0879\n",
            ">>>> Epoch 15991:\t Train Loss: 2.1582\n",
            ">>>> Epoch 15992:\t Train Loss: 2.3537\n",
            ">>>> Epoch 15993:\t Train Loss: 1.9490\n",
            ">>>> Epoch 15994:\t Train Loss: 2.1823\n",
            ">>>> Epoch 15995:\t Train Loss: 2.6247\n",
            ">>>> Epoch 15996:\t Train Loss: 2.8972\n",
            ">>>> Epoch 15997:\t Train Loss: 2.1750\n",
            ">>>> Epoch 15998:\t Train Loss: 2.2187\n",
            ">>>> Epoch 15999:\t Train Loss: 2.0536\n",
            ">>>> Epoch 16000:\t Train Loss: 2.2949\n",
            ">>>> Epoch 16001:\t Train Loss: 2.2456\n",
            ">>>> Epoch 16002:\t Train Loss: 1.9125\n",
            ">>>> Epoch 16003:\t Train Loss: 2.4175\n",
            ">>>> Epoch 16004:\t Train Loss: 2.4543\n",
            ">>>> Epoch 16005:\t Train Loss: 1.9269\n",
            ">>>> Epoch 16006:\t Train Loss: 1.9293\n",
            ">>>> Epoch 16007:\t Train Loss: 2.2327\n",
            ">>>> Epoch 16008:\t Train Loss: 2.3755\n",
            ">>>> Epoch 16009:\t Train Loss: 1.9137\n",
            ">>>> Epoch 16010:\t Train Loss: 1.8558\n",
            ">>>> Epoch 16011:\t Train Loss: 2.5240\n",
            ">>>> Epoch 16012:\t Train Loss: 2.3806\n",
            ">>>> Epoch 16013:\t Train Loss: 2.3099\n",
            ">>>> Epoch 16014:\t Train Loss: 2.4678\n",
            ">>>> Epoch 16015:\t Train Loss: 2.2461\n",
            ">>>> Epoch 16016:\t Train Loss: 2.2476\n",
            ">>>> Epoch 16017:\t Train Loss: 2.2731\n",
            ">>>> Epoch 16018:\t Train Loss: 2.4508\n",
            ">>>> Epoch 16019:\t Train Loss: 2.1375\n",
            ">>>> Epoch 16020:\t Train Loss: 1.9264\n",
            ">>>> Epoch 16021:\t Train Loss: 2.2282\n",
            ">>>> Epoch 16022:\t Train Loss: 2.0823\n",
            ">>>> Epoch 16023:\t Train Loss: 2.2709\n",
            ">>>> Epoch 16024:\t Train Loss: 2.3256\n",
            ">>>> Epoch 16025:\t Train Loss: 2.5555\n",
            ">>>> Epoch 16026:\t Train Loss: 1.8706\n",
            ">>>> Epoch 16027:\t Train Loss: 2.1493\n",
            ">>>> Epoch 16028:\t Train Loss: 2.1041\n",
            ">>>> Epoch 16029:\t Train Loss: 2.0207\n",
            ">>>> Epoch 16030:\t Train Loss: 2.2395\n",
            ">>>> Epoch 16031:\t Train Loss: 2.6344\n",
            ">>>> Epoch 16032:\t Train Loss: 2.4694\n",
            ">>>> Epoch 16033:\t Train Loss: 2.5722\n",
            ">>>> Epoch 16034:\t Train Loss: 2.0283\n",
            ">>>> Epoch 16035:\t Train Loss: 2.5147\n",
            ">>>> Epoch 16036:\t Train Loss: 2.1606\n",
            ">>>> Epoch 16037:\t Train Loss: 2.1814\n",
            ">>>> Epoch 16038:\t Train Loss: 2.1214\n",
            ">>>> Epoch 16039:\t Train Loss: 2.5272\n",
            ">>>> Epoch 16040:\t Train Loss: 1.9689\n",
            ">>>> Epoch 16041:\t Train Loss: 2.1852\n",
            ">>>> Epoch 16042:\t Train Loss: 2.4092\n",
            ">>>> Epoch 16043:\t Train Loss: 2.0114\n",
            ">>>> Epoch 16044:\t Train Loss: 2.1179\n",
            ">>>> Epoch 16045:\t Train Loss: 1.9813\n",
            ">>>> Epoch 16046:\t Train Loss: 2.3160\n",
            ">>>> Epoch 16047:\t Train Loss: 2.1180\n",
            ">>>> Epoch 16048:\t Train Loss: 2.3619\n",
            ">>>> Epoch 16049:\t Train Loss: 2.5495\n",
            ">>>> Epoch 16050:\t Train Loss: 2.1941\n",
            ">>>> Epoch 16051:\t Train Loss: 2.2010\n",
            ">>>> Epoch 16052:\t Train Loss: 1.9787\n",
            ">>>> Epoch 16053:\t Train Loss: 2.1300\n",
            ">>>> Epoch 16054:\t Train Loss: 1.7742\n",
            ">>>> Epoch 16055:\t Train Loss: 2.1546\n",
            ">>>> Epoch 16056:\t Train Loss: 2.0723\n",
            ">>>> Epoch 16057:\t Train Loss: 2.1639\n",
            ">>>> Epoch 16058:\t Train Loss: 2.1498\n",
            ">>>> Epoch 16059:\t Train Loss: 2.3862\n",
            ">>>> Epoch 16060:\t Train Loss: 2.2043\n",
            ">>>> Epoch 16061:\t Train Loss: 2.1444\n",
            ">>>> Epoch 16062:\t Train Loss: 1.9416\n",
            ">>>> Epoch 16063:\t Train Loss: 2.2875\n",
            ">>>> Epoch 16064:\t Train Loss: 2.4645\n",
            ">>>> Epoch 16065:\t Train Loss: 2.1722\n",
            ">>>> Epoch 16066:\t Train Loss: 2.1960\n",
            ">>>> Epoch 16067:\t Train Loss: 2.5028\n",
            ">>>> Epoch 16068:\t Train Loss: 2.0634\n",
            ">>>> Epoch 16069:\t Train Loss: 2.2085\n",
            ">>>> Epoch 16070:\t Train Loss: 2.3074\n",
            ">>>> Epoch 16071:\t Train Loss: 2.3329\n",
            ">>>> Epoch 16072:\t Train Loss: 1.9741\n",
            ">>>> Epoch 16073:\t Train Loss: 2.1055\n",
            ">>>> Epoch 16074:\t Train Loss: 2.2220\n",
            ">>>> Epoch 16075:\t Train Loss: 2.3987\n",
            ">>>> Epoch 16076:\t Train Loss: 2.3736\n",
            ">>>> Epoch 16077:\t Train Loss: 2.6056\n",
            ">>>> Epoch 16078:\t Train Loss: 2.1172\n",
            ">>>> Epoch 16079:\t Train Loss: 2.5782\n",
            ">>>> Epoch 16080:\t Train Loss: 2.1830\n",
            ">>>> Epoch 16081:\t Train Loss: 2.0444\n",
            ">>>> Epoch 16082:\t Train Loss: 2.0408\n",
            ">>>> Epoch 16083:\t Train Loss: 2.2425\n",
            ">>>> Epoch 16084:\t Train Loss: 2.2816\n",
            ">>>> Epoch 16085:\t Train Loss: 2.3187\n",
            ">>>> Epoch 16086:\t Train Loss: 2.4817\n",
            ">>>> Epoch 16087:\t Train Loss: 2.3607\n",
            ">>>> Epoch 16088:\t Train Loss: 2.2441\n",
            ">>>> Epoch 16089:\t Train Loss: 2.4481\n",
            ">>>> Epoch 16090:\t Train Loss: 2.1913\n",
            ">>>> Epoch 16091:\t Train Loss: 2.4984\n",
            ">>>> Epoch 16092:\t Train Loss: 2.3130\n",
            ">>>> Epoch 16093:\t Train Loss: 2.4154\n",
            ">>>> Epoch 16094:\t Train Loss: 2.2103\n",
            ">>>> Epoch 16095:\t Train Loss: 2.0503\n",
            ">>>> Epoch 16096:\t Train Loss: 2.3245\n",
            ">>>> Epoch 16097:\t Train Loss: 2.1042\n",
            ">>>> Epoch 16098:\t Train Loss: 2.1927\n",
            ">>>> Epoch 16099:\t Train Loss: 2.2044\n",
            ">>>> Epoch 16100:\t Train Loss: 1.7459\n",
            ">>>> Epoch 16101:\t Train Loss: 1.9736\n",
            ">>>> Epoch 16102:\t Train Loss: 1.8781\n",
            ">>>> Epoch 16103:\t Train Loss: 1.9765\n",
            ">>>> Epoch 16104:\t Train Loss: 2.6530\n",
            ">>>> Epoch 16105:\t Train Loss: 2.0869\n",
            ">>>> Epoch 16106:\t Train Loss: 2.4021\n",
            ">>>> Epoch 16107:\t Train Loss: 2.2320\n",
            ">>>> Epoch 16108:\t Train Loss: 2.1902\n",
            ">>>> Epoch 16109:\t Train Loss: 2.2657\n",
            ">>>> Epoch 16110:\t Train Loss: 2.4510\n",
            ">>>> Epoch 16111:\t Train Loss: 2.3623\n",
            ">>>> Epoch 16112:\t Train Loss: 2.4471\n",
            ">>>> Epoch 16113:\t Train Loss: 2.1420\n",
            ">>>> Epoch 16114:\t Train Loss: 2.4746\n",
            ">>>> Epoch 16115:\t Train Loss: 1.8901\n",
            ">>>> Epoch 16116:\t Train Loss: 1.9897\n",
            ">>>> Epoch 16117:\t Train Loss: 2.4696\n",
            ">>>> Epoch 16118:\t Train Loss: 2.3237\n",
            ">>>> Epoch 16119:\t Train Loss: 2.6274\n",
            ">>>> Epoch 16120:\t Train Loss: 2.3939\n",
            ">>>> Epoch 16121:\t Train Loss: 2.4400\n",
            ">>>> Epoch 16122:\t Train Loss: 2.1517\n",
            ">>>> Epoch 16123:\t Train Loss: 2.1017\n",
            ">>>> Epoch 16124:\t Train Loss: 2.1195\n",
            ">>>> Epoch 16125:\t Train Loss: 1.9132\n",
            ">>>> Epoch 16126:\t Train Loss: 2.3259\n",
            ">>>> Epoch 16127:\t Train Loss: 2.2036\n",
            ">>>> Epoch 16128:\t Train Loss: 2.2654\n",
            ">>>> Epoch 16129:\t Train Loss: 2.5891\n",
            ">>>> Epoch 16130:\t Train Loss: 2.5295\n",
            ">>>> Epoch 16131:\t Train Loss: 2.1392\n",
            ">>>> Epoch 16132:\t Train Loss: 2.4596\n",
            ">>>> Epoch 16133:\t Train Loss: 2.1184\n",
            ">>>> Epoch 16134:\t Train Loss: 2.0675\n",
            ">>>> Epoch 16135:\t Train Loss: 2.2276\n",
            ">>>> Epoch 16136:\t Train Loss: 2.1383\n",
            ">>>> Epoch 16137:\t Train Loss: 2.0971\n",
            ">>>> Epoch 16138:\t Train Loss: 2.2765\n",
            ">>>> Epoch 16139:\t Train Loss: 2.4279\n",
            ">>>> Epoch 16140:\t Train Loss: 2.3484\n",
            ">>>> Epoch 16141:\t Train Loss: 2.3180\n",
            ">>>> Epoch 16142:\t Train Loss: 2.2448\n",
            ">>>> Epoch 16143:\t Train Loss: 2.4448\n",
            ">>>> Epoch 16144:\t Train Loss: 2.1051\n",
            ">>>> Epoch 16145:\t Train Loss: 2.1904\n",
            ">>>> Epoch 16146:\t Train Loss: 2.5127\n",
            ">>>> Epoch 16147:\t Train Loss: 2.3319\n",
            ">>>> Epoch 16148:\t Train Loss: 2.4572\n",
            ">>>> Epoch 16149:\t Train Loss: 2.0843\n",
            ">>>> Epoch 16150:\t Train Loss: 2.0340\n",
            ">>>> Epoch 16151:\t Train Loss: 2.4551\n",
            ">>>> Epoch 16152:\t Train Loss: 2.1382\n",
            ">>>> Epoch 16153:\t Train Loss: 2.0015\n",
            ">>>> Epoch 16154:\t Train Loss: 2.3211\n",
            ">>>> Epoch 16155:\t Train Loss: 2.1188\n",
            ">>>> Epoch 16156:\t Train Loss: 2.3580\n",
            ">>>> Epoch 16157:\t Train Loss: 2.2107\n",
            ">>>> Epoch 16158:\t Train Loss: 2.5215\n",
            ">>>> Epoch 16159:\t Train Loss: 2.7779\n",
            ">>>> Epoch 16160:\t Train Loss: 2.5413\n",
            ">>>> Epoch 16161:\t Train Loss: 2.0098\n",
            ">>>> Epoch 16162:\t Train Loss: 2.2115\n",
            ">>>> Epoch 16163:\t Train Loss: 2.2179\n",
            ">>>> Epoch 16164:\t Train Loss: 2.3805\n",
            ">>>> Epoch 16165:\t Train Loss: 1.9911\n",
            ">>>> Epoch 16166:\t Train Loss: 1.9818\n",
            ">>>> Epoch 16167:\t Train Loss: 2.3213\n",
            ">>>> Epoch 16168:\t Train Loss: 2.5972\n",
            ">>>> Epoch 16169:\t Train Loss: 1.9583\n",
            ">>>> Epoch 16170:\t Train Loss: 2.3598\n",
            ">>>> Epoch 16171:\t Train Loss: 2.5386\n",
            ">>>> Epoch 16172:\t Train Loss: 2.3016\n",
            ">>>> Epoch 16173:\t Train Loss: 2.0610\n",
            ">>>> Epoch 16174:\t Train Loss: 2.4444\n",
            ">>>> Epoch 16175:\t Train Loss: 2.0630\n",
            ">>>> Epoch 16176:\t Train Loss: 2.2518\n",
            ">>>> Epoch 16177:\t Train Loss: 2.5719\n",
            ">>>> Epoch 16178:\t Train Loss: 2.0816\n",
            ">>>> Epoch 16179:\t Train Loss: 2.6645\n",
            ">>>> Epoch 16180:\t Train Loss: 2.5684\n",
            ">>>> Epoch 16181:\t Train Loss: 2.1528\n",
            ">>>> Epoch 16182:\t Train Loss: 2.1479\n",
            ">>>> Epoch 16183:\t Train Loss: 2.2891\n",
            ">>>> Epoch 16184:\t Train Loss: 2.1077\n",
            ">>>> Epoch 16185:\t Train Loss: 2.3784\n",
            ">>>> Epoch 16186:\t Train Loss: 2.0387\n",
            ">>>> Epoch 16187:\t Train Loss: 2.1134\n",
            ">>>> Epoch 16188:\t Train Loss: 2.3216\n",
            ">>>> Epoch 16189:\t Train Loss: 2.0050\n",
            ">>>> Epoch 16190:\t Train Loss: 2.3672\n",
            ">>>> Epoch 16191:\t Train Loss: 2.1032\n",
            ">>>> Epoch 16192:\t Train Loss: 2.1507\n",
            ">>>> Epoch 16193:\t Train Loss: 2.4258\n",
            ">>>> Epoch 16194:\t Train Loss: 2.2350\n",
            ">>>> Epoch 16195:\t Train Loss: 2.0211\n",
            ">>>> Epoch 16196:\t Train Loss: 2.4933\n",
            ">>>> Epoch 16197:\t Train Loss: 2.4017\n",
            ">>>> Epoch 16198:\t Train Loss: 1.8736\n",
            ">>>> Epoch 16199:\t Train Loss: 2.2245\n",
            ">>>> Epoch 16200:\t Train Loss: 1.9386\n",
            ">>>> Epoch 16201:\t Train Loss: 2.5423\n",
            ">>>> Epoch 16202:\t Train Loss: 2.0005\n",
            ">>>> Epoch 16203:\t Train Loss: 2.0418\n",
            ">>>> Epoch 16204:\t Train Loss: 1.9082\n",
            ">>>> Epoch 16205:\t Train Loss: 1.9869\n",
            ">>>> Epoch 16206:\t Train Loss: 2.2832\n",
            ">>>> Epoch 16207:\t Train Loss: 2.4863\n",
            ">>>> Epoch 16208:\t Train Loss: 2.0866\n",
            ">>>> Epoch 16209:\t Train Loss: 2.4734\n",
            ">>>> Epoch 16210:\t Train Loss: 2.3308\n",
            ">>>> Epoch 16211:\t Train Loss: 2.2870\n",
            ">>>> Epoch 16212:\t Train Loss: 1.9977\n",
            ">>>> Epoch 16213:\t Train Loss: 2.3544\n",
            ">>>> Epoch 16214:\t Train Loss: 2.1939\n",
            ">>>> Epoch 16215:\t Train Loss: 2.5282\n",
            ">>>> Epoch 16216:\t Train Loss: 2.1560\n",
            ">>>> Epoch 16217:\t Train Loss: 2.1184\n",
            ">>>> Epoch 16218:\t Train Loss: 2.3699\n",
            ">>>> Epoch 16219:\t Train Loss: 1.9755\n",
            ">>>> Epoch 16220:\t Train Loss: 2.0996\n",
            ">>>> Epoch 16221:\t Train Loss: 2.5544\n",
            ">>>> Epoch 16222:\t Train Loss: 2.1002\n",
            ">>>> Epoch 16223:\t Train Loss: 2.5324\n",
            ">>>> Epoch 16224:\t Train Loss: 2.3044\n",
            ">>>> Epoch 16225:\t Train Loss: 2.1730\n",
            ">>>> Epoch 16226:\t Train Loss: 2.2825\n",
            ">>>> Epoch 16227:\t Train Loss: 2.3151\n",
            ">>>> Epoch 16228:\t Train Loss: 2.2643\n",
            ">>>> Epoch 16229:\t Train Loss: 2.6111\n",
            ">>>> Epoch 16230:\t Train Loss: 2.0982\n",
            ">>>> Epoch 16231:\t Train Loss: 2.3230\n",
            ">>>> Epoch 16232:\t Train Loss: 2.1552\n",
            ">>>> Epoch 16233:\t Train Loss: 1.9773\n",
            ">>>> Epoch 16234:\t Train Loss: 2.5600\n",
            ">>>> Epoch 16235:\t Train Loss: 1.9035\n",
            ">>>> Epoch 16236:\t Train Loss: 2.6981\n",
            ">>>> Epoch 16237:\t Train Loss: 2.2300\n",
            ">>>> Epoch 16238:\t Train Loss: 1.9933\n",
            ">>>> Epoch 16239:\t Train Loss: 2.1927\n",
            ">>>> Epoch 16240:\t Train Loss: 2.3200\n",
            ">>>> Epoch 16241:\t Train Loss: 2.2603\n",
            ">>>> Epoch 16242:\t Train Loss: 1.9667\n",
            ">>>> Epoch 16243:\t Train Loss: 2.2236\n",
            ">>>> Epoch 16244:\t Train Loss: 1.9854\n",
            ">>>> Epoch 16245:\t Train Loss: 2.1332\n",
            ">>>> Epoch 16246:\t Train Loss: 2.2083\n",
            ">>>> Epoch 16247:\t Train Loss: 2.3267\n",
            ">>>> Epoch 16248:\t Train Loss: 2.2241\n",
            ">>>> Epoch 16249:\t Train Loss: 1.9317\n",
            ">>>> Epoch 16250:\t Train Loss: 2.0873\n",
            ">>>> Epoch 16251:\t Train Loss: 2.3096\n",
            ">>>> Epoch 16252:\t Train Loss: 2.2537\n",
            ">>>> Epoch 16253:\t Train Loss: 2.4685\n",
            ">>>> Epoch 16254:\t Train Loss: 2.2337\n",
            ">>>> Epoch 16255:\t Train Loss: 1.9282\n",
            ">>>> Epoch 16256:\t Train Loss: 2.4497\n",
            ">>>> Epoch 16257:\t Train Loss: 2.7659\n",
            ">>>> Epoch 16258:\t Train Loss: 2.2467\n",
            ">>>> Epoch 16259:\t Train Loss: 2.1910\n",
            ">>>> Epoch 16260:\t Train Loss: 2.2458\n",
            ">>>> Epoch 16261:\t Train Loss: 2.3380\n",
            ">>>> Epoch 16262:\t Train Loss: 2.2531\n",
            ">>>> Epoch 16263:\t Train Loss: 2.3796\n",
            ">>>> Epoch 16264:\t Train Loss: 2.3640\n",
            ">>>> Epoch 16265:\t Train Loss: 2.3108\n",
            ">>>> Epoch 16266:\t Train Loss: 2.2447\n",
            ">>>> Epoch 16267:\t Train Loss: 2.4763\n",
            ">>>> Epoch 16268:\t Train Loss: 1.9427\n",
            ">>>> Epoch 16269:\t Train Loss: 2.0773\n",
            ">>>> Epoch 16270:\t Train Loss: 2.0024\n",
            ">>>> Epoch 16271:\t Train Loss: 2.3510\n",
            ">>>> Epoch 16272:\t Train Loss: 1.8101\n",
            ">>>> Epoch 16273:\t Train Loss: 2.0636\n",
            ">>>> Epoch 16274:\t Train Loss: 2.1323\n",
            ">>>> Epoch 16275:\t Train Loss: 2.4920\n",
            ">>>> Epoch 16276:\t Train Loss: 2.7058\n",
            ">>>> Epoch 16277:\t Train Loss: 2.5734\n",
            ">>>> Epoch 16278:\t Train Loss: 2.4407\n",
            ">>>> Epoch 16279:\t Train Loss: 2.1313\n",
            ">>>> Epoch 16280:\t Train Loss: 2.1104\n",
            ">>>> Epoch 16281:\t Train Loss: 2.2309\n",
            ">>>> Epoch 16282:\t Train Loss: 2.1703\n",
            ">>>> Epoch 16283:\t Train Loss: 2.2057\n",
            ">>>> Epoch 16284:\t Train Loss: 2.2096\n",
            ">>>> Epoch 16285:\t Train Loss: 2.0676\n",
            ">>>> Epoch 16286:\t Train Loss: 2.2250\n",
            ">>>> Epoch 16287:\t Train Loss: 2.3773\n",
            ">>>> Epoch 16288:\t Train Loss: 2.3914\n",
            ">>>> Epoch 16289:\t Train Loss: 2.3963\n",
            ">>>> Epoch 16290:\t Train Loss: 2.2662\n",
            ">>>> Epoch 16291:\t Train Loss: 2.2262\n",
            ">>>> Epoch 16292:\t Train Loss: 2.5162\n",
            ">>>> Epoch 16293:\t Train Loss: 2.4139\n",
            ">>>> Epoch 16294:\t Train Loss: 2.4096\n",
            ">>>> Epoch 16295:\t Train Loss: 2.3487\n",
            ">>>> Epoch 16296:\t Train Loss: 2.0081\n",
            ">>>> Epoch 16297:\t Train Loss: 2.5849\n",
            ">>>> Epoch 16298:\t Train Loss: 2.4338\n",
            ">>>> Epoch 16299:\t Train Loss: 2.2872\n",
            ">>>> Epoch 16300:\t Train Loss: 2.6547\n",
            ">>>> Epoch 16301:\t Train Loss: 2.2747\n",
            ">>>> Epoch 16302:\t Train Loss: 2.1417\n",
            ">>>> Epoch 16303:\t Train Loss: 2.4351\n",
            ">>>> Epoch 16304:\t Train Loss: 2.0702\n",
            ">>>> Epoch 16305:\t Train Loss: 2.1595\n",
            ">>>> Epoch 16306:\t Train Loss: 2.3911\n",
            ">>>> Epoch 16307:\t Train Loss: 2.4743\n",
            ">>>> Epoch 16308:\t Train Loss: 2.0059\n",
            ">>>> Epoch 16309:\t Train Loss: 2.2464\n",
            ">>>> Epoch 16310:\t Train Loss: 2.3462\n",
            ">>>> Epoch 16311:\t Train Loss: 2.0230\n",
            ">>>> Epoch 16312:\t Train Loss: 2.2606\n",
            ">>>> Epoch 16313:\t Train Loss: 2.0833\n",
            ">>>> Epoch 16314:\t Train Loss: 2.5387\n",
            ">>>> Epoch 16315:\t Train Loss: 2.2522\n",
            ">>>> Epoch 16316:\t Train Loss: 2.0802\n",
            ">>>> Epoch 16317:\t Train Loss: 2.1393\n",
            ">>>> Epoch 16318:\t Train Loss: 2.1478\n",
            ">>>> Epoch 16319:\t Train Loss: 2.3148\n",
            ">>>> Epoch 16320:\t Train Loss: 2.0322\n",
            ">>>> Epoch 16321:\t Train Loss: 1.9190\n",
            ">>>> Epoch 16322:\t Train Loss: 2.0581\n",
            ">>>> Epoch 16323:\t Train Loss: 2.5482\n",
            ">>>> Epoch 16324:\t Train Loss: 2.0909\n",
            ">>>> Epoch 16325:\t Train Loss: 2.4023\n",
            ">>>> Epoch 16326:\t Train Loss: 2.4264\n",
            ">>>> Epoch 16327:\t Train Loss: 2.1451\n",
            ">>>> Epoch 16328:\t Train Loss: 2.4594\n",
            ">>>> Epoch 16329:\t Train Loss: 2.0872\n",
            ">>>> Epoch 16330:\t Train Loss: 2.1530\n",
            ">>>> Epoch 16331:\t Train Loss: 1.8532\n",
            ">>>> Epoch 16332:\t Train Loss: 2.2493\n",
            ">>>> Epoch 16333:\t Train Loss: 2.3702\n",
            ">>>> Epoch 16334:\t Train Loss: 2.3597\n",
            ">>>> Epoch 16335:\t Train Loss: 2.3506\n",
            ">>>> Epoch 16336:\t Train Loss: 1.9141\n",
            ">>>> Epoch 16337:\t Train Loss: 2.2806\n",
            ">>>> Epoch 16338:\t Train Loss: 2.4326\n",
            ">>>> Epoch 16339:\t Train Loss: 2.2079\n",
            ">>>> Epoch 16340:\t Train Loss: 2.1018\n",
            ">>>> Epoch 16341:\t Train Loss: 2.1133\n",
            ">>>> Epoch 16342:\t Train Loss: 2.2132\n",
            ">>>> Epoch 16343:\t Train Loss: 2.0778\n",
            ">>>> Epoch 16344:\t Train Loss: 2.1439\n",
            ">>>> Epoch 16345:\t Train Loss: 2.0199\n",
            ">>>> Epoch 16346:\t Train Loss: 2.1400\n",
            ">>>> Epoch 16347:\t Train Loss: 2.2938\n",
            ">>>> Epoch 16348:\t Train Loss: 2.0821\n",
            ">>>> Epoch 16349:\t Train Loss: 2.4527\n",
            ">>>> Epoch 16350:\t Train Loss: 1.9615\n",
            ">>>> Epoch 16351:\t Train Loss: 2.3620\n",
            ">>>> Epoch 16352:\t Train Loss: 2.1424\n",
            ">>>> Epoch 16353:\t Train Loss: 2.4675\n",
            ">>>> Epoch 16354:\t Train Loss: 2.1741\n",
            ">>>> Epoch 16355:\t Train Loss: 2.2729\n",
            ">>>> Epoch 16356:\t Train Loss: 2.1685\n",
            ">>>> Epoch 16357:\t Train Loss: 2.2851\n",
            ">>>> Epoch 16358:\t Train Loss: 2.4214\n",
            ">>>> Epoch 16359:\t Train Loss: 2.2422\n",
            ">>>> Epoch 16360:\t Train Loss: 2.4559\n",
            ">>>> Epoch 16361:\t Train Loss: 2.0912\n",
            ">>>> Epoch 16362:\t Train Loss: 2.2278\n",
            ">>>> Epoch 16363:\t Train Loss: 2.1750\n",
            ">>>> Epoch 16364:\t Train Loss: 2.3894\n",
            ">>>> Epoch 16365:\t Train Loss: 2.0448\n",
            ">>>> Epoch 16366:\t Train Loss: 2.4017\n",
            ">>>> Epoch 16367:\t Train Loss: 2.3118\n",
            ">>>> Epoch 16368:\t Train Loss: 2.2694\n",
            ">>>> Epoch 16369:\t Train Loss: 2.0850\n",
            ">>>> Epoch 16370:\t Train Loss: 1.9954\n",
            ">>>> Epoch 16371:\t Train Loss: 2.1121\n",
            ">>>> Epoch 16372:\t Train Loss: 2.2408\n",
            ">>>> Epoch 16373:\t Train Loss: 2.3556\n",
            ">>>> Epoch 16374:\t Train Loss: 2.3379\n",
            ">>>> Epoch 16375:\t Train Loss: 2.2815\n",
            ">>>> Epoch 16376:\t Train Loss: 2.2092\n",
            ">>>> Epoch 16377:\t Train Loss: 2.4571\n",
            ">>>> Epoch 16378:\t Train Loss: 2.1040\n",
            ">>>> Epoch 16379:\t Train Loss: 2.1460\n",
            ">>>> Epoch 16380:\t Train Loss: 2.2502\n",
            ">>>> Epoch 16381:\t Train Loss: 2.0512\n",
            ">>>> Epoch 16382:\t Train Loss: 2.4663\n",
            ">>>> Epoch 16383:\t Train Loss: 1.9653\n",
            ">>>> Epoch 16384:\t Train Loss: 2.1228\n",
            ">>>> Epoch 16385:\t Train Loss: 2.1778\n",
            ">>>> Epoch 16386:\t Train Loss: 2.3501\n",
            ">>>> Epoch 16387:\t Train Loss: 2.2358\n",
            ">>>> Epoch 16388:\t Train Loss: 2.2288\n",
            ">>>> Epoch 16389:\t Train Loss: 2.0204\n",
            ">>>> Epoch 16390:\t Train Loss: 2.3100\n",
            ">>>> Epoch 16391:\t Train Loss: 2.2459\n",
            ">>>> Epoch 16392:\t Train Loss: 2.1569\n",
            ">>>> Epoch 16393:\t Train Loss: 1.9615\n",
            ">>>> Epoch 16394:\t Train Loss: 2.0165\n",
            ">>>> Epoch 16395:\t Train Loss: 1.9843\n",
            ">>>> Epoch 16396:\t Train Loss: 2.2241\n",
            ">>>> Epoch 16397:\t Train Loss: 2.4269\n",
            ">>>> Epoch 16398:\t Train Loss: 2.4827\n",
            ">>>> Epoch 16399:\t Train Loss: 2.0652\n",
            ">>>> Epoch 16400:\t Train Loss: 2.3806\n",
            ">>>> Epoch 16401:\t Train Loss: 2.1349\n",
            ">>>> Epoch 16402:\t Train Loss: 2.4391\n",
            ">>>> Epoch 16403:\t Train Loss: 2.0044\n",
            ">>>> Epoch 16404:\t Train Loss: 2.3483\n",
            ">>>> Epoch 16405:\t Train Loss: 2.4358\n",
            ">>>> Epoch 16406:\t Train Loss: 2.2328\n",
            ">>>> Epoch 16407:\t Train Loss: 2.5457\n",
            ">>>> Epoch 16408:\t Train Loss: 2.5963\n",
            ">>>> Epoch 16409:\t Train Loss: 2.3579\n",
            ">>>> Epoch 16410:\t Train Loss: 1.8824\n",
            ">>>> Epoch 16411:\t Train Loss: 1.9185\n",
            ">>>> Epoch 16412:\t Train Loss: 2.3306\n",
            ">>>> Epoch 16413:\t Train Loss: 2.3156\n",
            ">>>> Epoch 16414:\t Train Loss: 2.1485\n",
            ">>>> Epoch 16415:\t Train Loss: 1.9776\n",
            ">>>> Epoch 16416:\t Train Loss: 2.2013\n",
            ">>>> Epoch 16417:\t Train Loss: 2.3975\n",
            ">>>> Epoch 16418:\t Train Loss: 2.2718\n",
            ">>>> Epoch 16419:\t Train Loss: 2.3452\n",
            ">>>> Epoch 16420:\t Train Loss: 2.4739\n",
            ">>>> Epoch 16421:\t Train Loss: 2.0323\n",
            ">>>> Epoch 16422:\t Train Loss: 1.7530\n",
            ">>>> Epoch 16423:\t Train Loss: 2.3412\n",
            ">>>> Epoch 16424:\t Train Loss: 2.2559\n",
            ">>>> Epoch 16425:\t Train Loss: 2.4169\n",
            ">>>> Epoch 16426:\t Train Loss: 2.0388\n",
            ">>>> Epoch 16427:\t Train Loss: 2.2143\n",
            ">>>> Epoch 16428:\t Train Loss: 2.1266\n",
            ">>>> Epoch 16429:\t Train Loss: 2.0924\n",
            ">>>> Epoch 16430:\t Train Loss: 2.3344\n",
            ">>>> Epoch 16431:\t Train Loss: 2.1180\n",
            ">>>> Epoch 16432:\t Train Loss: 2.4813\n",
            ">>>> Epoch 16433:\t Train Loss: 2.1819\n",
            ">>>> Epoch 16434:\t Train Loss: 2.3371\n",
            ">>>> Epoch 16435:\t Train Loss: 2.3479\n",
            ">>>> Epoch 16436:\t Train Loss: 2.2889\n",
            ">>>> Epoch 16437:\t Train Loss: 1.8272\n",
            ">>>> Epoch 16438:\t Train Loss: 2.0852\n",
            ">>>> Epoch 16439:\t Train Loss: 2.2457\n",
            ">>>> Epoch 16440:\t Train Loss: 2.0864\n",
            ">>>> Epoch 16441:\t Train Loss: 2.1612\n",
            ">>>> Epoch 16442:\t Train Loss: 2.4331\n",
            ">>>> Epoch 16443:\t Train Loss: 2.9323\n",
            ">>>> Epoch 16444:\t Train Loss: 1.9736\n",
            ">>>> Epoch 16445:\t Train Loss: 2.5710\n",
            ">>>> Epoch 16446:\t Train Loss: 2.5606\n",
            ">>>> Epoch 16447:\t Train Loss: 2.2285\n",
            ">>>> Epoch 16448:\t Train Loss: 1.8082\n",
            ">>>> Epoch 16449:\t Train Loss: 2.3724\n",
            ">>>> Epoch 16450:\t Train Loss: 2.2247\n",
            ">>>> Epoch 16451:\t Train Loss: 2.4318\n",
            ">>>> Epoch 16452:\t Train Loss: 2.3395\n",
            ">>>> Epoch 16453:\t Train Loss: 2.3388\n",
            ">>>> Epoch 16454:\t Train Loss: 2.4010\n",
            ">>>> Epoch 16455:\t Train Loss: 2.3982\n",
            ">>>> Epoch 16456:\t Train Loss: 1.9695\n",
            ">>>> Epoch 16457:\t Train Loss: 2.5855\n",
            ">>>> Epoch 16458:\t Train Loss: 2.3942\n",
            ">>>> Epoch 16459:\t Train Loss: 1.8414\n",
            ">>>> Epoch 16460:\t Train Loss: 1.8128\n",
            ">>>> Epoch 16461:\t Train Loss: 2.0688\n",
            ">>>> Epoch 16462:\t Train Loss: 2.1594\n",
            ">>>> Epoch 16463:\t Train Loss: 2.4446\n",
            ">>>> Epoch 16464:\t Train Loss: 2.3245\n",
            ">>>> Epoch 16465:\t Train Loss: 2.2358\n",
            ">>>> Epoch 16466:\t Train Loss: 2.2194\n",
            ">>>> Epoch 16467:\t Train Loss: 2.0750\n",
            ">>>> Epoch 16468:\t Train Loss: 2.3194\n",
            ">>>> Epoch 16469:\t Train Loss: 2.2769\n",
            ">>>> Epoch 16470:\t Train Loss: 2.1819\n",
            ">>>> Epoch 16471:\t Train Loss: 2.0267\n",
            ">>>> Epoch 16472:\t Train Loss: 2.0575\n",
            ">>>> Epoch 16473:\t Train Loss: 2.2559\n",
            ">>>> Epoch 16474:\t Train Loss: 2.4462\n",
            ">>>> Epoch 16475:\t Train Loss: 2.2458\n",
            ">>>> Epoch 16476:\t Train Loss: 2.2624\n",
            ">>>> Epoch 16477:\t Train Loss: 2.2013\n",
            ">>>> Epoch 16478:\t Train Loss: 2.3171\n",
            ">>>> Epoch 16479:\t Train Loss: 2.0045\n",
            ">>>> Epoch 16480:\t Train Loss: 2.3964\n",
            ">>>> Epoch 16481:\t Train Loss: 2.1195\n",
            ">>>> Epoch 16482:\t Train Loss: 1.9321\n",
            ">>>> Epoch 16483:\t Train Loss: 2.0991\n",
            ">>>> Epoch 16484:\t Train Loss: 2.2273\n",
            ">>>> Epoch 16485:\t Train Loss: 2.1770\n",
            ">>>> Epoch 16486:\t Train Loss: 2.1393\n",
            ">>>> Epoch 16487:\t Train Loss: 2.0955\n",
            ">>>> Epoch 16488:\t Train Loss: 2.2257\n",
            ">>>> Epoch 16489:\t Train Loss: 1.9398\n",
            ">>>> Epoch 16490:\t Train Loss: 1.9585\n",
            ">>>> Epoch 16491:\t Train Loss: 2.2362\n",
            ">>>> Epoch 16492:\t Train Loss: 2.2437\n",
            ">>>> Epoch 16493:\t Train Loss: 2.3820\n",
            ">>>> Epoch 16494:\t Train Loss: 2.3883\n",
            ">>>> Epoch 16495:\t Train Loss: 2.3851\n",
            ">>>> Epoch 16496:\t Train Loss: 2.3373\n",
            ">>>> Epoch 16497:\t Train Loss: 2.1493\n",
            ">>>> Epoch 16498:\t Train Loss: 2.1165\n",
            ">>>> Epoch 16499:\t Train Loss: 1.9776\n",
            ">>>> Epoch 16500:\t Train Loss: 2.0698\n",
            ">>>> Epoch 16501:\t Train Loss: 2.1117\n",
            ">>>> Epoch 16502:\t Train Loss: 2.4403\n",
            ">>>> Epoch 16503:\t Train Loss: 2.0407\n",
            ">>>> Epoch 16504:\t Train Loss: 2.3477\n",
            ">>>> Epoch 16505:\t Train Loss: 2.0082\n",
            ">>>> Epoch 16506:\t Train Loss: 2.1894\n",
            ">>>> Epoch 16507:\t Train Loss: 2.4735\n",
            ">>>> Epoch 16508:\t Train Loss: 2.4567\n",
            ">>>> Epoch 16509:\t Train Loss: 2.2690\n",
            ">>>> Epoch 16510:\t Train Loss: 2.4504\n",
            ">>>> Epoch 16511:\t Train Loss: 2.0974\n",
            ">>>> Epoch 16512:\t Train Loss: 2.2652\n",
            ">>>> Epoch 16513:\t Train Loss: 2.4775\n",
            ">>>> Epoch 16514:\t Train Loss: 2.5646\n",
            ">>>> Epoch 16515:\t Train Loss: 2.4163\n",
            ">>>> Epoch 16516:\t Train Loss: 2.3838\n",
            ">>>> Epoch 16517:\t Train Loss: 2.2968\n",
            ">>>> Epoch 16518:\t Train Loss: 2.2775\n",
            ">>>> Epoch 16519:\t Train Loss: 2.5356\n",
            ">>>> Epoch 16520:\t Train Loss: 2.2218\n",
            ">>>> Epoch 16521:\t Train Loss: 2.2971\n",
            ">>>> Epoch 16522:\t Train Loss: 2.1982\n",
            ">>>> Epoch 16523:\t Train Loss: 2.0179\n",
            ">>>> Epoch 16524:\t Train Loss: 2.3049\n",
            ">>>> Epoch 16525:\t Train Loss: 2.2794\n",
            ">>>> Epoch 16526:\t Train Loss: 1.8666\n",
            ">>>> Epoch 16527:\t Train Loss: 2.4820\n",
            ">>>> Epoch 16528:\t Train Loss: 2.0094\n",
            ">>>> Epoch 16529:\t Train Loss: 2.0844\n",
            ">>>> Epoch 16530:\t Train Loss: 2.3132\n",
            ">>>> Epoch 16531:\t Train Loss: 2.4073\n",
            ">>>> Epoch 16532:\t Train Loss: 2.4521\n",
            ">>>> Epoch 16533:\t Train Loss: 2.1888\n",
            ">>>> Epoch 16534:\t Train Loss: 2.0090\n",
            ">>>> Epoch 16535:\t Train Loss: 2.3181\n",
            ">>>> Epoch 16536:\t Train Loss: 2.2533\n",
            ">>>> Epoch 16537:\t Train Loss: 2.0061\n",
            ">>>> Epoch 16538:\t Train Loss: 2.4706\n",
            ">>>> Epoch 16539:\t Train Loss: 2.3116\n",
            ">>>> Epoch 16540:\t Train Loss: 2.0871\n",
            ">>>> Epoch 16541:\t Train Loss: 2.2678\n",
            ">>>> Epoch 16542:\t Train Loss: 2.1091\n",
            ">>>> Epoch 16543:\t Train Loss: 1.9061\n",
            ">>>> Epoch 16544:\t Train Loss: 2.2753\n",
            ">>>> Epoch 16545:\t Train Loss: 2.1375\n",
            ">>>> Epoch 16546:\t Train Loss: 2.1765\n",
            ">>>> Epoch 16547:\t Train Loss: 2.1630\n",
            ">>>> Epoch 16548:\t Train Loss: 2.0399\n",
            ">>>> Epoch 16549:\t Train Loss: 2.3310\n",
            ">>>> Epoch 16550:\t Train Loss: 2.4284\n",
            ">>>> Epoch 16551:\t Train Loss: 2.3833\n",
            ">>>> Epoch 16552:\t Train Loss: 2.4558\n",
            ">>>> Epoch 16553:\t Train Loss: 1.8175\n",
            ">>>> Epoch 16554:\t Train Loss: 2.0802\n",
            ">>>> Epoch 16555:\t Train Loss: 1.8883\n",
            ">>>> Epoch 16556:\t Train Loss: 2.1451\n",
            ">>>> Epoch 16557:\t Train Loss: 2.3235\n",
            ">>>> Epoch 16558:\t Train Loss: 2.4055\n",
            ">>>> Epoch 16559:\t Train Loss: 2.3034\n",
            ">>>> Epoch 16560:\t Train Loss: 2.1344\n",
            ">>>> Epoch 16561:\t Train Loss: 2.1820\n",
            ">>>> Epoch 16562:\t Train Loss: 2.3591\n",
            ">>>> Epoch 16563:\t Train Loss: 2.4669\n",
            ">>>> Epoch 16564:\t Train Loss: 2.2189\n",
            ">>>> Epoch 16565:\t Train Loss: 2.1448\n",
            ">>>> Epoch 16566:\t Train Loss: 2.4425\n",
            ">>>> Epoch 16567:\t Train Loss: 2.1194\n",
            ">>>> Epoch 16568:\t Train Loss: 2.4257\n",
            ">>>> Epoch 16569:\t Train Loss: 1.9792\n",
            ">>>> Epoch 16570:\t Train Loss: 2.5384\n",
            ">>>> Epoch 16571:\t Train Loss: 2.9230\n",
            ">>>> Epoch 16572:\t Train Loss: 2.2259\n",
            ">>>> Epoch 16573:\t Train Loss: 2.0852\n",
            ">>>> Epoch 16574:\t Train Loss: 1.9861\n",
            ">>>> Epoch 16575:\t Train Loss: 2.3357\n",
            ">>>> Epoch 16576:\t Train Loss: 2.3134\n",
            ">>>> Epoch 16577:\t Train Loss: 2.3438\n",
            ">>>> Epoch 16578:\t Train Loss: 2.4153\n",
            ">>>> Epoch 16579:\t Train Loss: 2.5944\n",
            ">>>> Epoch 16580:\t Train Loss: 2.1199\n",
            ">>>> Epoch 16581:\t Train Loss: 2.2568\n",
            ">>>> Epoch 16582:\t Train Loss: 2.3212\n",
            ">>>> Epoch 16583:\t Train Loss: 2.4691\n",
            ">>>> Epoch 16584:\t Train Loss: 2.3307\n",
            ">>>> Epoch 16585:\t Train Loss: 2.1858\n",
            ">>>> Epoch 16586:\t Train Loss: 2.2008\n",
            ">>>> Epoch 16587:\t Train Loss: 2.2087\n",
            ">>>> Epoch 16588:\t Train Loss: 2.2953\n",
            ">>>> Epoch 16589:\t Train Loss: 2.6924\n",
            ">>>> Epoch 16590:\t Train Loss: 2.5083\n",
            ">>>> Epoch 16591:\t Train Loss: 2.4494\n",
            ">>>> Epoch 16592:\t Train Loss: 1.8653\n",
            ">>>> Epoch 16593:\t Train Loss: 2.2560\n",
            ">>>> Epoch 16594:\t Train Loss: 2.2164\n",
            ">>>> Epoch 16595:\t Train Loss: 2.4096\n",
            ">>>> Epoch 16596:\t Train Loss: 2.0011\n",
            ">>>> Epoch 16597:\t Train Loss: 2.2965\n",
            ">>>> Epoch 16598:\t Train Loss: 2.3440\n",
            ">>>> Epoch 16599:\t Train Loss: 2.3861\n",
            ">>>> Epoch 16600:\t Train Loss: 2.0087\n",
            ">>>> Epoch 16601:\t Train Loss: 1.9173\n",
            ">>>> Epoch 16602:\t Train Loss: 2.0854\n",
            ">>>> Epoch 16603:\t Train Loss: 2.4781\n",
            ">>>> Epoch 16604:\t Train Loss: 2.2442\n",
            ">>>> Epoch 16605:\t Train Loss: 2.4971\n",
            ">>>> Epoch 16606:\t Train Loss: 2.0472\n",
            ">>>> Epoch 16607:\t Train Loss: 2.3717\n",
            ">>>> Epoch 16608:\t Train Loss: 2.2881\n",
            ">>>> Epoch 16609:\t Train Loss: 2.2634\n",
            ">>>> Epoch 16610:\t Train Loss: 2.2060\n",
            ">>>> Epoch 16611:\t Train Loss: 1.9163\n",
            ">>>> Epoch 16612:\t Train Loss: 2.1443\n",
            ">>>> Epoch 16613:\t Train Loss: 1.9585\n",
            ">>>> Epoch 16614:\t Train Loss: 2.3641\n",
            ">>>> Epoch 16615:\t Train Loss: 2.1249\n",
            ">>>> Epoch 16616:\t Train Loss: 2.4021\n",
            ">>>> Epoch 16617:\t Train Loss: 2.1767\n",
            ">>>> Epoch 16618:\t Train Loss: 2.2381\n",
            ">>>> Epoch 16619:\t Train Loss: 2.0165\n",
            ">>>> Epoch 16620:\t Train Loss: 2.0307\n",
            ">>>> Epoch 16621:\t Train Loss: 2.5258\n",
            ">>>> Epoch 16622:\t Train Loss: 2.3786\n",
            ">>>> Epoch 16623:\t Train Loss: 2.0152\n",
            ">>>> Epoch 16624:\t Train Loss: 2.2997\n",
            ">>>> Epoch 16625:\t Train Loss: 2.4653\n",
            ">>>> Epoch 16626:\t Train Loss: 2.3834\n",
            ">>>> Epoch 16627:\t Train Loss: 2.6117\n",
            ">>>> Epoch 16628:\t Train Loss: 2.8048\n",
            ">>>> Epoch 16629:\t Train Loss: 2.3129\n",
            ">>>> Epoch 16630:\t Train Loss: 2.4378\n",
            ">>>> Epoch 16631:\t Train Loss: 2.5080\n",
            ">>>> Epoch 16632:\t Train Loss: 2.0914\n",
            ">>>> Epoch 16633:\t Train Loss: 2.2566\n",
            ">>>> Epoch 16634:\t Train Loss: 2.0124\n",
            ">>>> Epoch 16635:\t Train Loss: 2.2181\n",
            ">>>> Epoch 16636:\t Train Loss: 2.7653\n",
            ">>>> Epoch 16637:\t Train Loss: 2.1736\n",
            ">>>> Epoch 16638:\t Train Loss: 2.2052\n",
            ">>>> Epoch 16639:\t Train Loss: 2.1661\n",
            ">>>> Epoch 16640:\t Train Loss: 1.9738\n",
            ">>>> Epoch 16641:\t Train Loss: 2.0971\n",
            ">>>> Epoch 16642:\t Train Loss: 2.6607\n",
            ">>>> Epoch 16643:\t Train Loss: 2.2300\n",
            ">>>> Epoch 16644:\t Train Loss: 2.4118\n",
            ">>>> Epoch 16645:\t Train Loss: 2.3729\n",
            ">>>> Epoch 16646:\t Train Loss: 2.2393\n",
            ">>>> Epoch 16647:\t Train Loss: 2.5493\n",
            ">>>> Epoch 16648:\t Train Loss: 2.2659\n",
            ">>>> Epoch 16649:\t Train Loss: 2.4711\n",
            ">>>> Epoch 16650:\t Train Loss: 2.2536\n",
            ">>>> Epoch 16651:\t Train Loss: 2.6053\n",
            ">>>> Epoch 16652:\t Train Loss: 2.4337\n",
            ">>>> Epoch 16653:\t Train Loss: 2.2143\n",
            ">>>> Epoch 16654:\t Train Loss: 2.4930\n",
            ">>>> Epoch 16655:\t Train Loss: 2.2589\n",
            ">>>> Epoch 16656:\t Train Loss: 2.1631\n",
            ">>>> Epoch 16657:\t Train Loss: 2.0346\n",
            ">>>> Epoch 16658:\t Train Loss: 2.1103\n",
            ">>>> Epoch 16659:\t Train Loss: 2.2896\n",
            ">>>> Epoch 16660:\t Train Loss: 2.2584\n",
            ">>>> Epoch 16661:\t Train Loss: 2.4358\n",
            ">>>> Epoch 16662:\t Train Loss: 1.9478\n",
            ">>>> Epoch 16663:\t Train Loss: 2.0505\n",
            ">>>> Epoch 16664:\t Train Loss: 2.3517\n",
            ">>>> Epoch 16665:\t Train Loss: 2.4301\n",
            ">>>> Epoch 16666:\t Train Loss: 2.4820\n",
            ">>>> Epoch 16667:\t Train Loss: 2.0635\n",
            ">>>> Epoch 16668:\t Train Loss: 2.3743\n",
            ">>>> Epoch 16669:\t Train Loss: 2.4030\n",
            ">>>> Epoch 16670:\t Train Loss: 2.0149\n",
            ">>>> Epoch 16671:\t Train Loss: 2.0016\n",
            ">>>> Epoch 16672:\t Train Loss: 2.1442\n",
            ">>>> Epoch 16673:\t Train Loss: 2.4987\n",
            ">>>> Epoch 16674:\t Train Loss: 2.3484\n",
            ">>>> Epoch 16675:\t Train Loss: 2.1211\n",
            ">>>> Epoch 16676:\t Train Loss: 2.3429\n",
            ">>>> Epoch 16677:\t Train Loss: 2.3563\n",
            ">>>> Epoch 16678:\t Train Loss: 2.2093\n",
            ">>>> Epoch 16679:\t Train Loss: 2.2909\n",
            ">>>> Epoch 16680:\t Train Loss: 2.8123\n",
            ">>>> Epoch 16681:\t Train Loss: 2.4089\n",
            ">>>> Epoch 16682:\t Train Loss: 2.0819\n",
            ">>>> Epoch 16683:\t Train Loss: 2.8179\n",
            ">>>> Epoch 16684:\t Train Loss: 2.4506\n",
            ">>>> Epoch 16685:\t Train Loss: 2.0279\n",
            ">>>> Epoch 16686:\t Train Loss: 2.4787\n",
            ">>>> Epoch 16687:\t Train Loss: 2.2024\n",
            ">>>> Epoch 16688:\t Train Loss: 2.3521\n",
            ">>>> Epoch 16689:\t Train Loss: 2.4443\n",
            ">>>> Epoch 16690:\t Train Loss: 2.5338\n",
            ">>>> Epoch 16691:\t Train Loss: 2.0655\n",
            ">>>> Epoch 16692:\t Train Loss: 2.1318\n",
            ">>>> Epoch 16693:\t Train Loss: 1.9205\n",
            ">>>> Epoch 16694:\t Train Loss: 2.1334\n",
            ">>>> Epoch 16695:\t Train Loss: 2.2196\n",
            ">>>> Epoch 16696:\t Train Loss: 2.3628\n",
            ">>>> Epoch 16697:\t Train Loss: 2.0767\n",
            ">>>> Epoch 16698:\t Train Loss: 2.4825\n",
            ">>>> Epoch 16699:\t Train Loss: 2.3572\n",
            ">>>> Epoch 16700:\t Train Loss: 1.9575\n",
            ">>>> Epoch 16701:\t Train Loss: 2.7117\n",
            ">>>> Epoch 16702:\t Train Loss: 2.0992\n",
            ">>>> Epoch 16703:\t Train Loss: 2.0707\n",
            ">>>> Epoch 16704:\t Train Loss: 2.2015\n",
            ">>>> Epoch 16705:\t Train Loss: 2.1790\n",
            ">>>> Epoch 16706:\t Train Loss: 2.1288\n",
            ">>>> Epoch 16707:\t Train Loss: 2.3360\n",
            ">>>> Epoch 16708:\t Train Loss: 1.9730\n",
            ">>>> Epoch 16709:\t Train Loss: 1.9706\n",
            ">>>> Epoch 16710:\t Train Loss: 2.1433\n",
            ">>>> Epoch 16711:\t Train Loss: 2.0877\n",
            ">>>> Epoch 16712:\t Train Loss: 1.8478\n",
            ">>>> Epoch 16713:\t Train Loss: 2.0354\n",
            ">>>> Epoch 16714:\t Train Loss: 2.1259\n",
            ">>>> Epoch 16715:\t Train Loss: 2.2802\n",
            ">>>> Epoch 16716:\t Train Loss: 2.1660\n",
            ">>>> Epoch 16717:\t Train Loss: 2.3760\n",
            ">>>> Epoch 16718:\t Train Loss: 2.4775\n",
            ">>>> Epoch 16719:\t Train Loss: 2.4223\n",
            ">>>> Epoch 16720:\t Train Loss: 2.1593\n",
            ">>>> Epoch 16721:\t Train Loss: 2.1169\n",
            ">>>> Epoch 16722:\t Train Loss: 2.2076\n",
            ">>>> Epoch 16723:\t Train Loss: 2.0619\n",
            ">>>> Epoch 16724:\t Train Loss: 2.4433\n",
            ">>>> Epoch 16725:\t Train Loss: 2.4809\n",
            ">>>> Epoch 16726:\t Train Loss: 2.0878\n",
            ">>>> Epoch 16727:\t Train Loss: 2.3693\n",
            ">>>> Epoch 16728:\t Train Loss: 2.2458\n",
            ">>>> Epoch 16729:\t Train Loss: 2.1782\n",
            ">>>> Epoch 16730:\t Train Loss: 2.4451\n",
            ">>>> Epoch 16731:\t Train Loss: 2.3205\n",
            ">>>> Epoch 16732:\t Train Loss: 1.9311\n",
            ">>>> Epoch 16733:\t Train Loss: 2.3315\n",
            ">>>> Epoch 16734:\t Train Loss: 2.2560\n",
            ">>>> Epoch 16735:\t Train Loss: 1.9650\n",
            ">>>> Epoch 16736:\t Train Loss: 2.0211\n",
            ">>>> Epoch 16737:\t Train Loss: 1.9735\n",
            ">>>> Epoch 16738:\t Train Loss: 2.1491\n",
            ">>>> Epoch 16739:\t Train Loss: 2.0608\n",
            ">>>> Epoch 16740:\t Train Loss: 2.4279\n",
            ">>>> Epoch 16741:\t Train Loss: 2.2703\n",
            ">>>> Epoch 16742:\t Train Loss: 2.1569\n",
            ">>>> Epoch 16743:\t Train Loss: 2.4906\n",
            ">>>> Epoch 16744:\t Train Loss: 2.0703\n",
            ">>>> Epoch 16745:\t Train Loss: 2.2623\n",
            ">>>> Epoch 16746:\t Train Loss: 2.4359\n",
            ">>>> Epoch 16747:\t Train Loss: 2.6427\n",
            ">>>> Epoch 16748:\t Train Loss: 2.3142\n",
            ">>>> Epoch 16749:\t Train Loss: 2.4107\n",
            ">>>> Epoch 16750:\t Train Loss: 2.3235\n",
            ">>>> Epoch 16751:\t Train Loss: 2.4338\n",
            ">>>> Epoch 16752:\t Train Loss: 2.7776\n",
            ">>>> Epoch 16753:\t Train Loss: 2.1107\n",
            ">>>> Epoch 16754:\t Train Loss: 2.2102\n",
            ">>>> Epoch 16755:\t Train Loss: 2.1537\n",
            ">>>> Epoch 16756:\t Train Loss: 2.1856\n",
            ">>>> Epoch 16757:\t Train Loss: 2.3176\n",
            ">>>> Epoch 16758:\t Train Loss: 2.5400\n",
            ">>>> Epoch 16759:\t Train Loss: 2.3702\n",
            ">>>> Epoch 16760:\t Train Loss: 2.1133\n",
            ">>>> Epoch 16761:\t Train Loss: 2.3359\n",
            ">>>> Epoch 16762:\t Train Loss: 2.5944\n",
            ">>>> Epoch 16763:\t Train Loss: 2.1253\n",
            ">>>> Epoch 16764:\t Train Loss: 2.6754\n",
            ">>>> Epoch 16765:\t Train Loss: 2.0421\n",
            ">>>> Epoch 16766:\t Train Loss: 2.2017\n",
            ">>>> Epoch 16767:\t Train Loss: 2.2736\n",
            ">>>> Epoch 16768:\t Train Loss: 2.2123\n",
            ">>>> Epoch 16769:\t Train Loss: 2.3447\n",
            ">>>> Epoch 16770:\t Train Loss: 2.1548\n",
            ">>>> Epoch 16771:\t Train Loss: 2.0932\n",
            ">>>> Epoch 16772:\t Train Loss: 2.2213\n",
            ">>>> Epoch 16773:\t Train Loss: 2.3759\n",
            ">>>> Epoch 16774:\t Train Loss: 2.3243\n",
            ">>>> Epoch 16775:\t Train Loss: 1.8924\n",
            ">>>> Epoch 16776:\t Train Loss: 1.8960\n",
            ">>>> Epoch 16777:\t Train Loss: 2.2758\n",
            ">>>> Epoch 16778:\t Train Loss: 2.0245\n",
            ">>>> Epoch 16779:\t Train Loss: 1.9921\n",
            ">>>> Epoch 16780:\t Train Loss: 2.0041\n",
            ">>>> Epoch 16781:\t Train Loss: 2.2215\n",
            ">>>> Epoch 16782:\t Train Loss: 2.0548\n",
            ">>>> Epoch 16783:\t Train Loss: 2.0065\n",
            ">>>> Epoch 16784:\t Train Loss: 2.0832\n",
            ">>>> Epoch 16785:\t Train Loss: 2.3311\n",
            ">>>> Epoch 16786:\t Train Loss: 2.4630\n",
            ">>>> Epoch 16787:\t Train Loss: 2.1965\n",
            ">>>> Epoch 16788:\t Train Loss: 2.3674\n",
            ">>>> Epoch 16789:\t Train Loss: 2.0419\n",
            ">>>> Epoch 16790:\t Train Loss: 2.0455\n",
            ">>>> Epoch 16791:\t Train Loss: 2.2802\n",
            ">>>> Epoch 16792:\t Train Loss: 2.1288\n",
            ">>>> Epoch 16793:\t Train Loss: 2.2548\n",
            ">>>> Epoch 16794:\t Train Loss: 2.5260\n",
            ">>>> Epoch 16795:\t Train Loss: 1.9736\n",
            ">>>> Epoch 16796:\t Train Loss: 2.5416\n",
            ">>>> Epoch 16797:\t Train Loss: 2.2031\n",
            ">>>> Epoch 16798:\t Train Loss: 2.1439\n",
            ">>>> Epoch 16799:\t Train Loss: 2.1189\n",
            ">>>> Epoch 16800:\t Train Loss: 2.1878\n",
            ">>>> Epoch 16801:\t Train Loss: 2.4328\n",
            ">>>> Epoch 16802:\t Train Loss: 2.0477\n",
            ">>>> Epoch 16803:\t Train Loss: 2.1914\n",
            ">>>> Epoch 16804:\t Train Loss: 2.5950\n",
            ">>>> Epoch 16805:\t Train Loss: 2.2718\n",
            ">>>> Epoch 16806:\t Train Loss: 2.1594\n",
            ">>>> Epoch 16807:\t Train Loss: 2.2403\n",
            ">>>> Epoch 16808:\t Train Loss: 2.1363\n",
            ">>>> Epoch 16809:\t Train Loss: 2.4157\n",
            ">>>> Epoch 16810:\t Train Loss: 2.3160\n",
            ">>>> Epoch 16811:\t Train Loss: 2.3713\n",
            ">>>> Epoch 16812:\t Train Loss: 2.1730\n",
            ">>>> Epoch 16813:\t Train Loss: 2.4824\n",
            ">>>> Epoch 16814:\t Train Loss: 2.2960\n",
            ">>>> Epoch 16815:\t Train Loss: 2.2715\n",
            ">>>> Epoch 16816:\t Train Loss: 2.4235\n",
            ">>>> Epoch 16817:\t Train Loss: 2.3743\n",
            ">>>> Epoch 16818:\t Train Loss: 2.3703\n",
            ">>>> Epoch 16819:\t Train Loss: 2.1818\n",
            ">>>> Epoch 16820:\t Train Loss: 1.6280\n",
            ">>>> Epoch 16821:\t Train Loss: 2.3982\n",
            ">>>> Epoch 16822:\t Train Loss: 2.2418\n",
            ">>>> Epoch 16823:\t Train Loss: 2.2642\n",
            ">>>> Epoch 16824:\t Train Loss: 2.1946\n",
            ">>>> Epoch 16825:\t Train Loss: 2.7120\n",
            ">>>> Epoch 16826:\t Train Loss: 2.2025\n",
            ">>>> Epoch 16827:\t Train Loss: 2.2865\n",
            ">>>> Epoch 16828:\t Train Loss: 2.2941\n",
            ">>>> Epoch 16829:\t Train Loss: 2.1368\n",
            ">>>> Epoch 16830:\t Train Loss: 2.1054\n",
            ">>>> Epoch 16831:\t Train Loss: 2.2165\n",
            ">>>> Epoch 16832:\t Train Loss: 2.1611\n",
            ">>>> Epoch 16833:\t Train Loss: 2.4955\n",
            ">>>> Epoch 16834:\t Train Loss: 2.1755\n",
            ">>>> Epoch 16835:\t Train Loss: 2.4290\n",
            ">>>> Epoch 16836:\t Train Loss: 2.0724\n",
            ">>>> Epoch 16837:\t Train Loss: 2.4761\n",
            ">>>> Epoch 16838:\t Train Loss: 2.1851\n",
            ">>>> Epoch 16839:\t Train Loss: 2.3018\n",
            ">>>> Epoch 16840:\t Train Loss: 2.1794\n",
            ">>>> Epoch 16841:\t Train Loss: 2.4175\n",
            ">>>> Epoch 16842:\t Train Loss: 1.9485\n",
            ">>>> Epoch 16843:\t Train Loss: 2.2125\n",
            ">>>> Epoch 16844:\t Train Loss: 2.2999\n",
            ">>>> Epoch 16845:\t Train Loss: 2.2352\n",
            ">>>> Epoch 16846:\t Train Loss: 2.0127\n",
            ">>>> Epoch 16847:\t Train Loss: 2.0741\n",
            ">>>> Epoch 16848:\t Train Loss: 2.2063\n",
            ">>>> Epoch 16849:\t Train Loss: 2.7308\n",
            ">>>> Epoch 16850:\t Train Loss: 2.2171\n",
            ">>>> Epoch 16851:\t Train Loss: 2.1638\n",
            ">>>> Epoch 16852:\t Train Loss: 2.1344\n",
            ">>>> Epoch 16853:\t Train Loss: 2.8151\n",
            ">>>> Epoch 16854:\t Train Loss: 1.8697\n",
            ">>>> Epoch 16855:\t Train Loss: 2.5645\n",
            ">>>> Epoch 16856:\t Train Loss: 2.3972\n",
            ">>>> Epoch 16857:\t Train Loss: 2.3066\n",
            ">>>> Epoch 16858:\t Train Loss: 1.9304\n",
            ">>>> Epoch 16859:\t Train Loss: 2.1623\n",
            ">>>> Epoch 16860:\t Train Loss: 2.1324\n",
            ">>>> Epoch 16861:\t Train Loss: 2.2624\n",
            ">>>> Epoch 16862:\t Train Loss: 2.3921\n",
            ">>>> Epoch 16863:\t Train Loss: 2.4112\n",
            ">>>> Epoch 16864:\t Train Loss: 1.9978\n",
            ">>>> Epoch 16865:\t Train Loss: 2.0326\n",
            ">>>> Epoch 16866:\t Train Loss: 2.5390\n",
            ">>>> Epoch 16867:\t Train Loss: 2.1890\n",
            ">>>> Epoch 16868:\t Train Loss: 2.1498\n",
            ">>>> Epoch 16869:\t Train Loss: 2.2537\n",
            ">>>> Epoch 16870:\t Train Loss: 2.1349\n",
            ">>>> Epoch 16871:\t Train Loss: 2.0662\n",
            ">>>> Epoch 16872:\t Train Loss: 2.4551\n",
            ">>>> Epoch 16873:\t Train Loss: 2.2651\n",
            ">>>> Epoch 16874:\t Train Loss: 2.0609\n",
            ">>>> Epoch 16875:\t Train Loss: 2.2467\n",
            ">>>> Epoch 16876:\t Train Loss: 1.8779\n",
            ">>>> Epoch 16877:\t Train Loss: 2.1076\n",
            ">>>> Epoch 16878:\t Train Loss: 2.0496\n",
            ">>>> Epoch 16879:\t Train Loss: 2.5077\n",
            ">>>> Epoch 16880:\t Train Loss: 2.2594\n",
            ">>>> Epoch 16881:\t Train Loss: 2.5115\n",
            ">>>> Epoch 16882:\t Train Loss: 2.2741\n",
            ">>>> Epoch 16883:\t Train Loss: 2.1563\n",
            ">>>> Epoch 16884:\t Train Loss: 2.1507\n",
            ">>>> Epoch 16885:\t Train Loss: 2.1944\n",
            ">>>> Epoch 16886:\t Train Loss: 2.2425\n",
            ">>>> Epoch 16887:\t Train Loss: 2.2241\n",
            ">>>> Epoch 16888:\t Train Loss: 2.2217\n",
            ">>>> Epoch 16889:\t Train Loss: 2.0572\n",
            ">>>> Epoch 16890:\t Train Loss: 2.0753\n",
            ">>>> Epoch 16891:\t Train Loss: 2.1760\n",
            ">>>> Epoch 16892:\t Train Loss: 2.0696\n",
            ">>>> Epoch 16893:\t Train Loss: 2.1141\n",
            ">>>> Epoch 16894:\t Train Loss: 2.0926\n",
            ">>>> Epoch 16895:\t Train Loss: 2.2374\n",
            ">>>> Epoch 16896:\t Train Loss: 2.7320\n",
            ">>>> Epoch 16897:\t Train Loss: 2.2330\n",
            ">>>> Epoch 16898:\t Train Loss: 2.0918\n",
            ">>>> Epoch 16899:\t Train Loss: 2.4719\n",
            ">>>> Epoch 16900:\t Train Loss: 1.8420\n",
            ">>>> Epoch 16901:\t Train Loss: 2.0531\n",
            ">>>> Epoch 16902:\t Train Loss: 2.2171\n",
            ">>>> Epoch 16903:\t Train Loss: 2.4913\n",
            ">>>> Epoch 16904:\t Train Loss: 2.5459\n",
            ">>>> Epoch 16905:\t Train Loss: 2.1706\n",
            ">>>> Epoch 16906:\t Train Loss: 2.0358\n",
            ">>>> Epoch 16907:\t Train Loss: 2.2063\n",
            ">>>> Epoch 16908:\t Train Loss: 2.4628\n",
            ">>>> Epoch 16909:\t Train Loss: 2.2797\n",
            ">>>> Epoch 16910:\t Train Loss: 1.8191\n",
            ">>>> Epoch 16911:\t Train Loss: 2.1526\n",
            ">>>> Epoch 16912:\t Train Loss: 2.5484\n",
            ">>>> Epoch 16913:\t Train Loss: 2.4255\n",
            ">>>> Epoch 16914:\t Train Loss: 1.9093\n",
            ">>>> Epoch 16915:\t Train Loss: 2.2851\n",
            ">>>> Epoch 16916:\t Train Loss: 2.5389\n",
            ">>>> Epoch 16917:\t Train Loss: 2.3531\n",
            ">>>> Epoch 16918:\t Train Loss: 2.2432\n",
            ">>>> Epoch 16919:\t Train Loss: 2.1024\n",
            ">>>> Epoch 16920:\t Train Loss: 2.0206\n",
            ">>>> Epoch 16921:\t Train Loss: 2.2345\n",
            ">>>> Epoch 16922:\t Train Loss: 2.1209\n",
            ">>>> Epoch 16923:\t Train Loss: 2.0571\n",
            ">>>> Epoch 16924:\t Train Loss: 2.3033\n",
            ">>>> Epoch 16925:\t Train Loss: 2.1103\n",
            ">>>> Epoch 16926:\t Train Loss: 2.0694\n",
            ">>>> Epoch 16927:\t Train Loss: 1.9091\n",
            ">>>> Epoch 16928:\t Train Loss: 1.8646\n",
            ">>>> Epoch 16929:\t Train Loss: 2.4417\n",
            ">>>> Epoch 16930:\t Train Loss: 2.5641\n",
            ">>>> Epoch 16931:\t Train Loss: 2.3266\n",
            ">>>> Epoch 16932:\t Train Loss: 2.3654\n",
            ">>>> Epoch 16933:\t Train Loss: 2.4671\n",
            ">>>> Epoch 16934:\t Train Loss: 2.3575\n",
            ">>>> Epoch 16935:\t Train Loss: 1.9995\n",
            ">>>> Epoch 16936:\t Train Loss: 2.1445\n",
            ">>>> Epoch 16937:\t Train Loss: 2.4693\n",
            ">>>> Epoch 16938:\t Train Loss: 2.1759\n",
            ">>>> Epoch 16939:\t Train Loss: 2.0491\n",
            ">>>> Epoch 16940:\t Train Loss: 2.0677\n",
            ">>>> Epoch 16941:\t Train Loss: 2.1502\n",
            ">>>> Epoch 16942:\t Train Loss: 2.0638\n",
            ">>>> Epoch 16943:\t Train Loss: 1.9375\n",
            ">>>> Epoch 16944:\t Train Loss: 2.4034\n",
            ">>>> Epoch 16945:\t Train Loss: 2.1669\n",
            ">>>> Epoch 16946:\t Train Loss: 2.3802\n",
            ">>>> Epoch 16947:\t Train Loss: 2.1872\n",
            ">>>> Epoch 16948:\t Train Loss: 2.2412\n",
            ">>>> Epoch 16949:\t Train Loss: 2.2238\n",
            ">>>> Epoch 16950:\t Train Loss: 2.3610\n",
            ">>>> Epoch 16951:\t Train Loss: 2.0084\n",
            ">>>> Epoch 16952:\t Train Loss: 2.1536\n",
            ">>>> Epoch 16953:\t Train Loss: 2.3697\n",
            ">>>> Epoch 16954:\t Train Loss: 2.1099\n",
            ">>>> Epoch 16955:\t Train Loss: 2.1712\n",
            ">>>> Epoch 16956:\t Train Loss: 2.2203\n",
            ">>>> Epoch 16957:\t Train Loss: 2.4238\n",
            ">>>> Epoch 16958:\t Train Loss: 2.0050\n",
            ">>>> Epoch 16959:\t Train Loss: 2.3921\n",
            ">>>> Epoch 16960:\t Train Loss: 2.1249\n",
            ">>>> Epoch 16961:\t Train Loss: 2.5641\n",
            ">>>> Epoch 16962:\t Train Loss: 2.0853\n",
            ">>>> Epoch 16963:\t Train Loss: 2.0062\n",
            ">>>> Epoch 16964:\t Train Loss: 2.2600\n",
            ">>>> Epoch 16965:\t Train Loss: 2.7664\n",
            ">>>> Epoch 16966:\t Train Loss: 2.3973\n",
            ">>>> Epoch 16967:\t Train Loss: 2.2508\n",
            ">>>> Epoch 16968:\t Train Loss: 2.2161\n",
            ">>>> Epoch 16969:\t Train Loss: 2.2021\n",
            ">>>> Epoch 16970:\t Train Loss: 2.1879\n",
            ">>>> Epoch 16971:\t Train Loss: 2.2141\n",
            ">>>> Epoch 16972:\t Train Loss: 1.9091\n",
            ">>>> Epoch 16973:\t Train Loss: 1.9952\n",
            ">>>> Epoch 16974:\t Train Loss: 2.4654\n",
            ">>>> Epoch 16975:\t Train Loss: 1.9023\n",
            ">>>> Epoch 16976:\t Train Loss: 2.5380\n",
            ">>>> Epoch 16977:\t Train Loss: 1.7177\n",
            ">>>> Epoch 16978:\t Train Loss: 2.0292\n",
            ">>>> Epoch 16979:\t Train Loss: 2.0407\n",
            ">>>> Epoch 16980:\t Train Loss: 2.1469\n",
            ">>>> Epoch 16981:\t Train Loss: 1.8938\n",
            ">>>> Epoch 16982:\t Train Loss: 1.9804\n",
            ">>>> Epoch 16983:\t Train Loss: 2.2134\n",
            ">>>> Epoch 16984:\t Train Loss: 1.9719\n",
            ">>>> Epoch 16985:\t Train Loss: 2.3679\n",
            ">>>> Epoch 16986:\t Train Loss: 2.0663\n",
            ">>>> Epoch 16987:\t Train Loss: 2.2070\n",
            ">>>> Epoch 16988:\t Train Loss: 2.2208\n",
            ">>>> Epoch 16989:\t Train Loss: 2.3354\n",
            ">>>> Epoch 16990:\t Train Loss: 2.0464\n",
            ">>>> Epoch 16991:\t Train Loss: 2.0512\n",
            ">>>> Epoch 16992:\t Train Loss: 2.1257\n",
            ">>>> Epoch 16993:\t Train Loss: 2.1258\n",
            ">>>> Epoch 16994:\t Train Loss: 2.3218\n",
            ">>>> Epoch 16995:\t Train Loss: 1.9715\n",
            ">>>> Epoch 16996:\t Train Loss: 1.8483\n",
            ">>>> Epoch 16997:\t Train Loss: 2.1907\n",
            ">>>> Epoch 16998:\t Train Loss: 1.9108\n",
            ">>>> Epoch 16999:\t Train Loss: 1.8318\n",
            ">>>> Epoch 17000:\t Train Loss: 2.0078\n",
            ">>>> Epoch 17001:\t Train Loss: 1.9615\n",
            ">>>> Epoch 17002:\t Train Loss: 2.1386\n",
            ">>>> Epoch 17003:\t Train Loss: 2.2644\n",
            ">>>> Epoch 17004:\t Train Loss: 1.6893\n",
            ">>>> Epoch 17005:\t Train Loss: 2.2233\n",
            ">>>> Epoch 17006:\t Train Loss: 2.0157\n",
            ">>>> Epoch 17007:\t Train Loss: 2.6823\n",
            ">>>> Epoch 17008:\t Train Loss: 1.9466\n",
            ">>>> Epoch 17009:\t Train Loss: 2.4405\n",
            ">>>> Epoch 17010:\t Train Loss: 1.8155\n",
            ">>>> Epoch 17011:\t Train Loss: 2.2560\n",
            ">>>> Epoch 17012:\t Train Loss: 2.2591\n",
            ">>>> Epoch 17013:\t Train Loss: 2.0015\n",
            ">>>> Epoch 17014:\t Train Loss: 2.2149\n",
            ">>>> Epoch 17015:\t Train Loss: 2.1741\n",
            ">>>> Epoch 17016:\t Train Loss: 2.3022\n",
            ">>>> Epoch 17017:\t Train Loss: 2.5920\n",
            ">>>> Epoch 17018:\t Train Loss: 2.3941\n",
            ">>>> Epoch 17019:\t Train Loss: 2.7761\n",
            ">>>> Epoch 17020:\t Train Loss: 1.9873\n",
            ">>>> Epoch 17021:\t Train Loss: 2.4111\n",
            ">>>> Epoch 17022:\t Train Loss: 2.1150\n",
            ">>>> Epoch 17023:\t Train Loss: 2.3592\n",
            ">>>> Epoch 17024:\t Train Loss: 2.5477\n",
            ">>>> Epoch 17025:\t Train Loss: 2.3007\n",
            ">>>> Epoch 17026:\t Train Loss: 1.8016\n",
            ">>>> Epoch 17027:\t Train Loss: 2.4402\n",
            ">>>> Epoch 17028:\t Train Loss: 1.8310\n",
            ">>>> Epoch 17029:\t Train Loss: 2.1362\n",
            ">>>> Epoch 17030:\t Train Loss: 1.9000\n",
            ">>>> Epoch 17031:\t Train Loss: 2.4715\n",
            ">>>> Epoch 17032:\t Train Loss: 2.2639\n",
            ">>>> Epoch 17033:\t Train Loss: 2.4036\n",
            ">>>> Epoch 17034:\t Train Loss: 2.2681\n",
            ">>>> Epoch 17035:\t Train Loss: 2.2765\n",
            ">>>> Epoch 17036:\t Train Loss: 2.1743\n",
            ">>>> Epoch 17037:\t Train Loss: 2.3702\n",
            ">>>> Epoch 17038:\t Train Loss: 2.1464\n",
            ">>>> Epoch 17039:\t Train Loss: 2.3421\n",
            ">>>> Epoch 17040:\t Train Loss: 2.3274\n",
            ">>>> Epoch 17041:\t Train Loss: 2.5301\n",
            ">>>> Epoch 17042:\t Train Loss: 2.2701\n",
            ">>>> Epoch 17043:\t Train Loss: 2.7745\n",
            ">>>> Epoch 17044:\t Train Loss: 2.3289\n",
            ">>>> Epoch 17045:\t Train Loss: 2.1702\n",
            ">>>> Epoch 17046:\t Train Loss: 2.1363\n",
            ">>>> Epoch 17047:\t Train Loss: 2.7530\n",
            ">>>> Epoch 17048:\t Train Loss: 1.9751\n",
            ">>>> Epoch 17049:\t Train Loss: 2.5126\n",
            ">>>> Epoch 17050:\t Train Loss: 2.4238\n",
            ">>>> Epoch 17051:\t Train Loss: 1.9606\n",
            ">>>> Epoch 17052:\t Train Loss: 2.2119\n",
            ">>>> Epoch 17053:\t Train Loss: 1.8987\n",
            ">>>> Epoch 17054:\t Train Loss: 2.3221\n",
            ">>>> Epoch 17055:\t Train Loss: 2.4449\n",
            ">>>> Epoch 17056:\t Train Loss: 2.6220\n",
            ">>>> Epoch 17057:\t Train Loss: 2.0163\n",
            ">>>> Epoch 17058:\t Train Loss: 1.8756\n",
            ">>>> Epoch 17059:\t Train Loss: 2.2225\n",
            ">>>> Epoch 17060:\t Train Loss: 2.4926\n",
            ">>>> Epoch 17061:\t Train Loss: 2.4084\n",
            ">>>> Epoch 17062:\t Train Loss: 2.0600\n",
            ">>>> Epoch 17063:\t Train Loss: 2.3205\n",
            ">>>> Epoch 17064:\t Train Loss: 2.4212\n",
            ">>>> Epoch 17065:\t Train Loss: 2.3974\n",
            ">>>> Epoch 17066:\t Train Loss: 2.2113\n",
            ">>>> Epoch 17067:\t Train Loss: 2.6652\n",
            ">>>> Epoch 17068:\t Train Loss: 2.1891\n",
            ">>>> Epoch 17069:\t Train Loss: 1.9654\n",
            ">>>> Epoch 17070:\t Train Loss: 1.9375\n",
            ">>>> Epoch 17071:\t Train Loss: 2.7439\n",
            ">>>> Epoch 17072:\t Train Loss: 2.0867\n",
            ">>>> Epoch 17073:\t Train Loss: 2.1316\n",
            ">>>> Epoch 17074:\t Train Loss: 1.9247\n",
            ">>>> Epoch 17075:\t Train Loss: 2.1636\n",
            ">>>> Epoch 17076:\t Train Loss: 2.5191\n",
            ">>>> Epoch 17077:\t Train Loss: 2.1685\n",
            ">>>> Epoch 17078:\t Train Loss: 2.2422\n",
            ">>>> Epoch 17079:\t Train Loss: 2.1624\n",
            ">>>> Epoch 17080:\t Train Loss: 2.3858\n",
            ">>>> Epoch 17081:\t Train Loss: 2.5531\n",
            ">>>> Epoch 17082:\t Train Loss: 2.3731\n",
            ">>>> Epoch 17083:\t Train Loss: 2.0945\n",
            ">>>> Epoch 17084:\t Train Loss: 2.0540\n",
            ">>>> Epoch 17085:\t Train Loss: 2.1131\n",
            ">>>> Epoch 17086:\t Train Loss: 1.9327\n",
            ">>>> Epoch 17087:\t Train Loss: 2.2409\n",
            ">>>> Epoch 17088:\t Train Loss: 2.1374\n",
            ">>>> Epoch 17089:\t Train Loss: 2.2513\n",
            ">>>> Epoch 17090:\t Train Loss: 2.3384\n",
            ">>>> Epoch 17091:\t Train Loss: 2.3558\n",
            ">>>> Epoch 17092:\t Train Loss: 2.5959\n",
            ">>>> Epoch 17093:\t Train Loss: 2.1578\n",
            ">>>> Epoch 17094:\t Train Loss: 2.2162\n",
            ">>>> Epoch 17095:\t Train Loss: 2.0059\n",
            ">>>> Epoch 17096:\t Train Loss: 2.4837\n",
            ">>>> Epoch 17097:\t Train Loss: 2.2048\n",
            ">>>> Epoch 17098:\t Train Loss: 2.6095\n",
            ">>>> Epoch 17099:\t Train Loss: 2.1214\n",
            ">>>> Epoch 17100:\t Train Loss: 1.9362\n",
            ">>>> Epoch 17101:\t Train Loss: 2.1692\n",
            ">>>> Epoch 17102:\t Train Loss: 2.3208\n",
            ">>>> Epoch 17103:\t Train Loss: 2.1632\n",
            ">>>> Epoch 17104:\t Train Loss: 2.1864\n",
            ">>>> Epoch 17105:\t Train Loss: 2.2458\n",
            ">>>> Epoch 17106:\t Train Loss: 1.9913\n",
            ">>>> Epoch 17107:\t Train Loss: 2.3140\n",
            ">>>> Epoch 17108:\t Train Loss: 2.2144\n",
            ">>>> Epoch 17109:\t Train Loss: 2.5901\n",
            ">>>> Epoch 17110:\t Train Loss: 2.1120\n",
            ">>>> Epoch 17111:\t Train Loss: 2.4843\n",
            ">>>> Epoch 17112:\t Train Loss: 1.7920\n",
            ">>>> Epoch 17113:\t Train Loss: 1.8546\n",
            ">>>> Epoch 17114:\t Train Loss: 2.1980\n",
            ">>>> Epoch 17115:\t Train Loss: 2.2430\n",
            ">>>> Epoch 17116:\t Train Loss: 1.9260\n",
            ">>>> Epoch 17117:\t Train Loss: 2.3003\n",
            ">>>> Epoch 17118:\t Train Loss: 2.2005\n",
            ">>>> Epoch 17119:\t Train Loss: 1.9099\n",
            ">>>> Epoch 17120:\t Train Loss: 2.1013\n",
            ">>>> Epoch 17121:\t Train Loss: 2.1318\n",
            ">>>> Epoch 17122:\t Train Loss: 2.0295\n",
            ">>>> Epoch 17123:\t Train Loss: 2.1244\n",
            ">>>> Epoch 17124:\t Train Loss: 1.9174\n",
            ">>>> Epoch 17125:\t Train Loss: 1.9541\n",
            ">>>> Epoch 17126:\t Train Loss: 2.2496\n",
            ">>>> Epoch 17127:\t Train Loss: 2.1841\n",
            ">>>> Epoch 17128:\t Train Loss: 2.2997\n",
            ">>>> Epoch 17129:\t Train Loss: 2.0880\n",
            ">>>> Epoch 17130:\t Train Loss: 2.4046\n",
            ">>>> Epoch 17131:\t Train Loss: 1.9777\n",
            ">>>> Epoch 17132:\t Train Loss: 2.7661\n",
            ">>>> Epoch 17133:\t Train Loss: 2.1533\n",
            ">>>> Epoch 17134:\t Train Loss: 2.2176\n",
            ">>>> Epoch 17135:\t Train Loss: 1.8874\n",
            ">>>> Epoch 17136:\t Train Loss: 2.3202\n",
            ">>>> Epoch 17137:\t Train Loss: 2.2878\n",
            ">>>> Epoch 17138:\t Train Loss: 2.0529\n",
            ">>>> Epoch 17139:\t Train Loss: 2.2052\n",
            ">>>> Epoch 17140:\t Train Loss: 1.9389\n",
            ">>>> Epoch 17141:\t Train Loss: 2.0616\n",
            ">>>> Epoch 17142:\t Train Loss: 2.0936\n",
            ">>>> Epoch 17143:\t Train Loss: 2.1752\n",
            ">>>> Epoch 17144:\t Train Loss: 1.9124\n",
            ">>>> Epoch 17145:\t Train Loss: 2.3441\n",
            ">>>> Epoch 17146:\t Train Loss: 2.3260\n",
            ">>>> Epoch 17147:\t Train Loss: 2.0989\n",
            ">>>> Epoch 17148:\t Train Loss: 2.0095\n",
            ">>>> Epoch 17149:\t Train Loss: 2.2265\n",
            ">>>> Epoch 17150:\t Train Loss: 2.2640\n",
            ">>>> Epoch 17151:\t Train Loss: 2.2452\n",
            ">>>> Epoch 17152:\t Train Loss: 2.0703\n",
            ">>>> Epoch 17153:\t Train Loss: 1.8823\n",
            ">>>> Epoch 17154:\t Train Loss: 2.2739\n",
            ">>>> Epoch 17155:\t Train Loss: 2.3180\n",
            ">>>> Epoch 17156:\t Train Loss: 1.6659\n",
            ">>>> Epoch 17157:\t Train Loss: 1.8925\n",
            ">>>> Epoch 17158:\t Train Loss: 2.0087\n",
            ">>>> Epoch 17159:\t Train Loss: 2.6481\n",
            ">>>> Epoch 17160:\t Train Loss: 2.4257\n",
            ">>>> Epoch 17161:\t Train Loss: 2.4683\n",
            ">>>> Epoch 17162:\t Train Loss: 2.1595\n",
            ">>>> Epoch 17163:\t Train Loss: 2.3178\n",
            ">>>> Epoch 17164:\t Train Loss: 1.9344\n",
            ">>>> Epoch 17165:\t Train Loss: 1.9132\n",
            ">>>> Epoch 17166:\t Train Loss: 2.4413\n",
            ">>>> Epoch 17167:\t Train Loss: 2.2161\n",
            ">>>> Epoch 17168:\t Train Loss: 2.2970\n",
            ">>>> Epoch 17169:\t Train Loss: 2.0444\n",
            ">>>> Epoch 17170:\t Train Loss: 2.3070\n",
            ">>>> Epoch 17171:\t Train Loss: 1.8602\n",
            ">>>> Epoch 17172:\t Train Loss: 2.2618\n",
            ">>>> Epoch 17173:\t Train Loss: 2.0099\n",
            ">>>> Epoch 17174:\t Train Loss: 1.6795\n",
            ">>>> Epoch 17175:\t Train Loss: 2.2759\n",
            ">>>> Epoch 17176:\t Train Loss: 2.0059\n",
            ">>>> Epoch 17177:\t Train Loss: 2.2661\n",
            ">>>> Epoch 17178:\t Train Loss: 2.0346\n",
            ">>>> Epoch 17179:\t Train Loss: 2.1374\n",
            ">>>> Epoch 17180:\t Train Loss: 2.1781\n",
            ">>>> Epoch 17181:\t Train Loss: 1.9177\n",
            ">>>> Epoch 17182:\t Train Loss: 2.2524\n",
            ">>>> Epoch 17183:\t Train Loss: 2.2868\n",
            ">>>> Epoch 17184:\t Train Loss: 2.5747\n",
            ">>>> Epoch 17185:\t Train Loss: 2.2464\n",
            ">>>> Epoch 17186:\t Train Loss: 2.1618\n",
            ">>>> Epoch 17187:\t Train Loss: 2.4954\n",
            ">>>> Epoch 17188:\t Train Loss: 2.1108\n",
            ">>>> Epoch 17189:\t Train Loss: 2.1179\n",
            ">>>> Epoch 17190:\t Train Loss: 1.8383\n",
            ">>>> Epoch 17191:\t Train Loss: 2.0812\n",
            ">>>> Epoch 17192:\t Train Loss: 1.9839\n",
            ">>>> Epoch 17193:\t Train Loss: 2.8183\n",
            ">>>> Epoch 17194:\t Train Loss: 2.3477\n",
            ">>>> Epoch 17195:\t Train Loss: 1.9838\n",
            ">>>> Epoch 17196:\t Train Loss: 1.7805\n",
            ">>>> Epoch 17197:\t Train Loss: 2.0356\n",
            ">>>> Epoch 17198:\t Train Loss: 2.2889\n",
            ">>>> Epoch 17199:\t Train Loss: 1.7823\n",
            ">>>> Epoch 17200:\t Train Loss: 1.9975\n",
            ">>>> Epoch 17201:\t Train Loss: 2.3278\n",
            ">>>> Epoch 17202:\t Train Loss: 2.5469\n",
            ">>>> Epoch 17203:\t Train Loss: 2.2306\n",
            ">>>> Epoch 17204:\t Train Loss: 1.9458\n",
            ">>>> Epoch 17205:\t Train Loss: 2.1369\n",
            ">>>> Epoch 17206:\t Train Loss: 2.3665\n",
            ">>>> Epoch 17207:\t Train Loss: 1.5783\n",
            ">>>> Epoch 17208:\t Train Loss: 2.3892\n",
            ">>>> Epoch 17209:\t Train Loss: 1.9238\n",
            ">>>> Epoch 17210:\t Train Loss: 1.7880\n",
            ">>>> Epoch 17211:\t Train Loss: 1.9877\n",
            ">>>> Epoch 17212:\t Train Loss: 2.4312\n",
            ">>>> Epoch 17213:\t Train Loss: 2.4519\n",
            ">>>> Epoch 17214:\t Train Loss: 2.4329\n",
            ">>>> Epoch 17215:\t Train Loss: 1.9542\n",
            ">>>> Epoch 17216:\t Train Loss: 2.3529\n",
            ">>>> Epoch 17217:\t Train Loss: 2.1735\n",
            ">>>> Epoch 17218:\t Train Loss: 2.1797\n",
            ">>>> Epoch 17219:\t Train Loss: 2.2486\n",
            ">>>> Epoch 17220:\t Train Loss: 2.2590\n",
            ">>>> Epoch 17221:\t Train Loss: 2.3837\n",
            ">>>> Epoch 17222:\t Train Loss: 2.6742\n",
            ">>>> Epoch 17223:\t Train Loss: 2.0894\n",
            ">>>> Epoch 17224:\t Train Loss: 2.0833\n",
            ">>>> Epoch 17225:\t Train Loss: 2.2940\n",
            ">>>> Epoch 17226:\t Train Loss: 2.1319\n",
            ">>>> Epoch 17227:\t Train Loss: 2.1371\n",
            ">>>> Epoch 17228:\t Train Loss: 1.9756\n",
            ">>>> Epoch 17229:\t Train Loss: 2.5552\n",
            ">>>> Epoch 17230:\t Train Loss: 2.1100\n",
            ">>>> Epoch 17231:\t Train Loss: 2.0485\n",
            ">>>> Epoch 17232:\t Train Loss: 2.3507\n",
            ">>>> Epoch 17233:\t Train Loss: 2.5331\n",
            ">>>> Epoch 17234:\t Train Loss: 2.1791\n",
            ">>>> Epoch 17235:\t Train Loss: 1.9826\n",
            ">>>> Epoch 17236:\t Train Loss: 2.0089\n",
            ">>>> Epoch 17237:\t Train Loss: 2.4140\n",
            ">>>> Epoch 17238:\t Train Loss: 2.2935\n",
            ">>>> Epoch 17239:\t Train Loss: 2.2124\n",
            ">>>> Epoch 17240:\t Train Loss: 2.2909\n",
            ">>>> Epoch 17241:\t Train Loss: 2.2307\n",
            ">>>> Epoch 17242:\t Train Loss: 2.2007\n",
            ">>>> Epoch 17243:\t Train Loss: 2.2679\n",
            ">>>> Epoch 17244:\t Train Loss: 2.4255\n",
            ">>>> Epoch 17245:\t Train Loss: 2.4196\n",
            ">>>> Epoch 17246:\t Train Loss: 1.9124\n",
            ">>>> Epoch 17247:\t Train Loss: 2.1921\n",
            ">>>> Epoch 17248:\t Train Loss: 2.5801\n",
            ">>>> Epoch 17249:\t Train Loss: 2.2340\n",
            ">>>> Epoch 17250:\t Train Loss: 2.0261\n",
            ">>>> Epoch 17251:\t Train Loss: 2.7425\n",
            ">>>> Epoch 17252:\t Train Loss: 2.1719\n",
            ">>>> Epoch 17253:\t Train Loss: 2.4495\n",
            ">>>> Epoch 17254:\t Train Loss: 2.1867\n",
            ">>>> Epoch 17255:\t Train Loss: 2.0194\n",
            ">>>> Epoch 17256:\t Train Loss: 2.0952\n",
            ">>>> Epoch 17257:\t Train Loss: 2.0456\n",
            ">>>> Epoch 17258:\t Train Loss: 2.4953\n",
            ">>>> Epoch 17259:\t Train Loss: 2.3982\n",
            ">>>> Epoch 17260:\t Train Loss: 2.0967\n",
            ">>>> Epoch 17261:\t Train Loss: 2.2253\n",
            ">>>> Epoch 17262:\t Train Loss: 2.3416\n",
            ">>>> Epoch 17263:\t Train Loss: 2.2214\n",
            ">>>> Epoch 17264:\t Train Loss: 2.2855\n",
            ">>>> Epoch 17265:\t Train Loss: 2.1736\n",
            ">>>> Epoch 17266:\t Train Loss: 2.0192\n",
            ">>>> Epoch 17267:\t Train Loss: 1.7603\n",
            ">>>> Epoch 17268:\t Train Loss: 2.4123\n",
            ">>>> Epoch 17269:\t Train Loss: 2.5115\n",
            ">>>> Epoch 17270:\t Train Loss: 2.4424\n",
            ">>>> Epoch 17271:\t Train Loss: 2.3926\n",
            ">>>> Epoch 17272:\t Train Loss: 2.1268\n",
            ">>>> Epoch 17273:\t Train Loss: 2.2360\n",
            ">>>> Epoch 17274:\t Train Loss: 2.2145\n",
            ">>>> Epoch 17275:\t Train Loss: 2.1653\n",
            ">>>> Epoch 17276:\t Train Loss: 2.3416\n",
            ">>>> Epoch 17277:\t Train Loss: 2.1304\n",
            ">>>> Epoch 17278:\t Train Loss: 2.2977\n",
            ">>>> Epoch 17279:\t Train Loss: 2.1033\n",
            ">>>> Epoch 17280:\t Train Loss: 2.6171\n",
            ">>>> Epoch 17281:\t Train Loss: 2.2822\n",
            ">>>> Epoch 17282:\t Train Loss: 2.3727\n",
            ">>>> Epoch 17283:\t Train Loss: 2.1754\n",
            ">>>> Epoch 17284:\t Train Loss: 2.6028\n",
            ">>>> Epoch 17285:\t Train Loss: 1.9521\n",
            ">>>> Epoch 17286:\t Train Loss: 2.5801\n",
            ">>>> Epoch 17287:\t Train Loss: 2.2555\n",
            ">>>> Epoch 17288:\t Train Loss: 2.1133\n",
            ">>>> Epoch 17289:\t Train Loss: 2.5320\n",
            ">>>> Epoch 17290:\t Train Loss: 2.4223\n",
            ">>>> Epoch 17291:\t Train Loss: 2.1431\n",
            ">>>> Epoch 17292:\t Train Loss: 2.4738\n",
            ">>>> Epoch 17293:\t Train Loss: 1.8806\n",
            ">>>> Epoch 17294:\t Train Loss: 2.1182\n",
            ">>>> Epoch 17295:\t Train Loss: 2.3500\n",
            ">>>> Epoch 17296:\t Train Loss: 2.0614\n",
            ">>>> Epoch 17297:\t Train Loss: 2.0302\n",
            ">>>> Epoch 17298:\t Train Loss: 2.3319\n",
            ">>>> Epoch 17299:\t Train Loss: 1.9280\n",
            ">>>> Epoch 17300:\t Train Loss: 1.9207\n",
            ">>>> Epoch 17301:\t Train Loss: 2.4597\n",
            ">>>> Epoch 17302:\t Train Loss: 2.1717\n",
            ">>>> Epoch 17303:\t Train Loss: 2.6047\n",
            ">>>> Epoch 17304:\t Train Loss: 2.2782\n",
            ">>>> Epoch 17305:\t Train Loss: 2.0906\n",
            ">>>> Epoch 17306:\t Train Loss: 2.1717\n",
            ">>>> Epoch 17307:\t Train Loss: 2.3515\n",
            ">>>> Epoch 17308:\t Train Loss: 2.2180\n",
            ">>>> Epoch 17309:\t Train Loss: 2.4550\n",
            ">>>> Epoch 17310:\t Train Loss: 2.1578\n",
            ">>>> Epoch 17311:\t Train Loss: 2.2468\n",
            ">>>> Epoch 17312:\t Train Loss: 2.0102\n",
            ">>>> Epoch 17313:\t Train Loss: 2.4070\n",
            ">>>> Epoch 17314:\t Train Loss: 2.1023\n",
            ">>>> Epoch 17315:\t Train Loss: 2.6160\n",
            ">>>> Epoch 17316:\t Train Loss: 2.3589\n",
            ">>>> Epoch 17317:\t Train Loss: 2.5085\n",
            ">>>> Epoch 17318:\t Train Loss: 2.1133\n",
            ">>>> Epoch 17319:\t Train Loss: 2.0995\n",
            ">>>> Epoch 17320:\t Train Loss: 2.4334\n",
            ">>>> Epoch 17321:\t Train Loss: 2.3511\n",
            ">>>> Epoch 17322:\t Train Loss: 2.1053\n",
            ">>>> Epoch 17323:\t Train Loss: 2.0760\n",
            ">>>> Epoch 17324:\t Train Loss: 2.2303\n",
            ">>>> Epoch 17325:\t Train Loss: 2.0728\n",
            ">>>> Epoch 17326:\t Train Loss: 2.2831\n",
            ">>>> Epoch 17327:\t Train Loss: 2.2315\n",
            ">>>> Epoch 17328:\t Train Loss: 2.2900\n",
            ">>>> Epoch 17329:\t Train Loss: 2.4135\n",
            ">>>> Epoch 17330:\t Train Loss: 1.9735\n",
            ">>>> Epoch 17331:\t Train Loss: 2.3397\n",
            ">>>> Epoch 17332:\t Train Loss: 2.2796\n",
            ">>>> Epoch 17333:\t Train Loss: 2.1353\n",
            ">>>> Epoch 17334:\t Train Loss: 2.4640\n",
            ">>>> Epoch 17335:\t Train Loss: 2.3948\n",
            ">>>> Epoch 17336:\t Train Loss: 2.1486\n",
            ">>>> Epoch 17337:\t Train Loss: 2.1007\n",
            ">>>> Epoch 17338:\t Train Loss: 2.1837\n",
            ">>>> Epoch 17339:\t Train Loss: 2.0486\n",
            ">>>> Epoch 17340:\t Train Loss: 2.1742\n",
            ">>>> Epoch 17341:\t Train Loss: 2.1671\n",
            ">>>> Epoch 17342:\t Train Loss: 2.0233\n",
            ">>>> Epoch 17343:\t Train Loss: 2.2844\n",
            ">>>> Epoch 17344:\t Train Loss: 2.2222\n",
            ">>>> Epoch 17345:\t Train Loss: 2.2763\n",
            ">>>> Epoch 17346:\t Train Loss: 2.2138\n",
            ">>>> Epoch 17347:\t Train Loss: 2.1797\n",
            ">>>> Epoch 17348:\t Train Loss: 2.5473\n",
            ">>>> Epoch 17349:\t Train Loss: 2.3183\n",
            ">>>> Epoch 17350:\t Train Loss: 2.4265\n",
            ">>>> Epoch 17351:\t Train Loss: 1.8910\n",
            ">>>> Epoch 17352:\t Train Loss: 1.9080\n",
            ">>>> Epoch 17353:\t Train Loss: 1.7317\n",
            ">>>> Epoch 17354:\t Train Loss: 1.8914\n",
            ">>>> Epoch 17355:\t Train Loss: 2.0082\n",
            ">>>> Epoch 17356:\t Train Loss: 2.3773\n",
            ">>>> Epoch 17357:\t Train Loss: 2.2765\n",
            ">>>> Epoch 17358:\t Train Loss: 2.3020\n",
            ">>>> Epoch 17359:\t Train Loss: 2.2377\n",
            ">>>> Epoch 17360:\t Train Loss: 1.8798\n",
            ">>>> Epoch 17361:\t Train Loss: 2.5598\n",
            ">>>> Epoch 17362:\t Train Loss: 1.7547\n",
            ">>>> Epoch 17363:\t Train Loss: 2.1650\n",
            ">>>> Epoch 17364:\t Train Loss: 1.9009\n",
            ">>>> Epoch 17365:\t Train Loss: 2.3884\n",
            ">>>> Epoch 17366:\t Train Loss: 2.5825\n",
            ">>>> Epoch 17367:\t Train Loss: 2.3826\n",
            ">>>> Epoch 17368:\t Train Loss: 2.4330\n",
            ">>>> Epoch 17369:\t Train Loss: 2.9285\n",
            ">>>> Epoch 17370:\t Train Loss: 2.1742\n",
            ">>>> Epoch 17371:\t Train Loss: 2.3184\n",
            ">>>> Epoch 17372:\t Train Loss: 2.1857\n",
            ">>>> Epoch 17373:\t Train Loss: 2.7185\n",
            ">>>> Epoch 17374:\t Train Loss: 2.6237\n",
            ">>>> Epoch 17375:\t Train Loss: 2.2655\n",
            ">>>> Epoch 17376:\t Train Loss: 2.6931\n",
            ">>>> Epoch 17377:\t Train Loss: 2.0477\n",
            ">>>> Epoch 17378:\t Train Loss: 2.3480\n",
            ">>>> Epoch 17379:\t Train Loss: 2.3122\n",
            ">>>> Epoch 17380:\t Train Loss: 2.1148\n",
            ">>>> Epoch 17381:\t Train Loss: 2.0015\n",
            ">>>> Epoch 17382:\t Train Loss: 2.2382\n",
            ">>>> Epoch 17383:\t Train Loss: 1.9566\n",
            ">>>> Epoch 17384:\t Train Loss: 2.3848\n",
            ">>>> Epoch 17385:\t Train Loss: 2.3917\n",
            ">>>> Epoch 17386:\t Train Loss: 2.2578\n",
            ">>>> Epoch 17387:\t Train Loss: 2.0362\n",
            ">>>> Epoch 17388:\t Train Loss: 1.8421\n",
            ">>>> Epoch 17389:\t Train Loss: 2.2901\n",
            ">>>> Epoch 17390:\t Train Loss: 2.6267\n",
            ">>>> Epoch 17391:\t Train Loss: 2.5646\n",
            ">>>> Epoch 17392:\t Train Loss: 2.5784\n",
            ">>>> Epoch 17393:\t Train Loss: 2.0805\n",
            ">>>> Epoch 17394:\t Train Loss: 2.1812\n",
            ">>>> Epoch 17395:\t Train Loss: 2.3375\n",
            ">>>> Epoch 17396:\t Train Loss: 1.9651\n",
            ">>>> Epoch 17397:\t Train Loss: 2.1989\n",
            ">>>> Epoch 17398:\t Train Loss: 2.2795\n",
            ">>>> Epoch 17399:\t Train Loss: 2.4720\n",
            ">>>> Epoch 17400:\t Train Loss: 2.0094\n",
            ">>>> Epoch 17401:\t Train Loss: 1.9281\n",
            ">>>> Epoch 17402:\t Train Loss: 2.6347\n",
            ">>>> Epoch 17403:\t Train Loss: 2.2482\n",
            ">>>> Epoch 17404:\t Train Loss: 2.3228\n",
            ">>>> Epoch 17405:\t Train Loss: 2.0640\n",
            ">>>> Epoch 17406:\t Train Loss: 2.6821\n",
            ">>>> Epoch 17407:\t Train Loss: 2.4286\n",
            ">>>> Epoch 17408:\t Train Loss: 2.3548\n",
            ">>>> Epoch 17409:\t Train Loss: 2.2177\n",
            ">>>> Epoch 17410:\t Train Loss: 2.0423\n",
            ">>>> Epoch 17411:\t Train Loss: 2.2523\n",
            ">>>> Epoch 17412:\t Train Loss: 2.2428\n",
            ">>>> Epoch 17413:\t Train Loss: 1.9149\n",
            ">>>> Epoch 17414:\t Train Loss: 2.3838\n",
            ">>>> Epoch 17415:\t Train Loss: 1.9146\n",
            ">>>> Epoch 17416:\t Train Loss: 2.7090\n",
            ">>>> Epoch 17417:\t Train Loss: 2.4320\n",
            ">>>> Epoch 17418:\t Train Loss: 2.1833\n",
            ">>>> Epoch 17419:\t Train Loss: 1.9745\n",
            ">>>> Epoch 17420:\t Train Loss: 2.5414\n",
            ">>>> Epoch 17421:\t Train Loss: 2.2721\n",
            ">>>> Epoch 17422:\t Train Loss: 2.4625\n",
            ">>>> Epoch 17423:\t Train Loss: 2.0323\n",
            ">>>> Epoch 17424:\t Train Loss: 1.9650\n",
            ">>>> Epoch 17425:\t Train Loss: 2.3142\n",
            ">>>> Epoch 17426:\t Train Loss: 2.1237\n",
            ">>>> Epoch 17427:\t Train Loss: 1.9990\n",
            ">>>> Epoch 17428:\t Train Loss: 1.9973\n",
            ">>>> Epoch 17429:\t Train Loss: 2.3483\n",
            ">>>> Epoch 17430:\t Train Loss: 2.2731\n",
            ">>>> Epoch 17431:\t Train Loss: 2.2828\n",
            ">>>> Epoch 17432:\t Train Loss: 2.2594\n",
            ">>>> Epoch 17433:\t Train Loss: 1.8972\n",
            ">>>> Epoch 17434:\t Train Loss: 2.2329\n",
            ">>>> Epoch 17435:\t Train Loss: 2.5517\n",
            ">>>> Epoch 17436:\t Train Loss: 2.7535\n",
            ">>>> Epoch 17437:\t Train Loss: 2.0682\n",
            ">>>> Epoch 17438:\t Train Loss: 2.0974\n",
            ">>>> Epoch 17439:\t Train Loss: 2.2266\n",
            ">>>> Epoch 17440:\t Train Loss: 2.4717\n",
            ">>>> Epoch 17441:\t Train Loss: 2.2468\n",
            ">>>> Epoch 17442:\t Train Loss: 2.3773\n",
            ">>>> Epoch 17443:\t Train Loss: 2.3737\n",
            ">>>> Epoch 17444:\t Train Loss: 2.0225\n",
            ">>>> Epoch 17445:\t Train Loss: 2.4244\n",
            ">>>> Epoch 17446:\t Train Loss: 2.2541\n",
            ">>>> Epoch 17447:\t Train Loss: 2.6804\n",
            ">>>> Epoch 17448:\t Train Loss: 2.1218\n",
            ">>>> Epoch 17449:\t Train Loss: 2.4503\n",
            ">>>> Epoch 17450:\t Train Loss: 2.1227\n",
            ">>>> Epoch 17451:\t Train Loss: 1.9829\n",
            ">>>> Epoch 17452:\t Train Loss: 2.3393\n",
            ">>>> Epoch 17453:\t Train Loss: 2.1051\n",
            ">>>> Epoch 17454:\t Train Loss: 2.6115\n",
            ">>>> Epoch 17455:\t Train Loss: 2.4519\n",
            ">>>> Epoch 17456:\t Train Loss: 2.1610\n",
            ">>>> Epoch 17457:\t Train Loss: 2.1029\n",
            ">>>> Epoch 17458:\t Train Loss: 2.6687\n",
            ">>>> Epoch 17459:\t Train Loss: 2.1838\n",
            ">>>> Epoch 17460:\t Train Loss: 2.3049\n",
            ">>>> Epoch 17461:\t Train Loss: 1.9950\n",
            ">>>> Epoch 17462:\t Train Loss: 2.0276\n",
            ">>>> Epoch 17463:\t Train Loss: 2.2007\n",
            ">>>> Epoch 17464:\t Train Loss: 2.3580\n",
            ">>>> Epoch 17465:\t Train Loss: 2.0562\n",
            ">>>> Epoch 17466:\t Train Loss: 2.1859\n",
            ">>>> Epoch 17467:\t Train Loss: 2.7130\n",
            ">>>> Epoch 17468:\t Train Loss: 2.6410\n",
            ">>>> Epoch 17469:\t Train Loss: 2.4234\n",
            ">>>> Epoch 17470:\t Train Loss: 2.3972\n",
            ">>>> Epoch 17471:\t Train Loss: 2.0524\n",
            ">>>> Epoch 17472:\t Train Loss: 2.1216\n",
            ">>>> Epoch 17473:\t Train Loss: 2.1094\n",
            ">>>> Epoch 17474:\t Train Loss: 2.1448\n",
            ">>>> Epoch 17475:\t Train Loss: 2.4543\n",
            ">>>> Epoch 17476:\t Train Loss: 2.6880\n",
            ">>>> Epoch 17477:\t Train Loss: 2.1834\n",
            ">>>> Epoch 17478:\t Train Loss: 2.3840\n",
            ">>>> Epoch 17479:\t Train Loss: 2.1650\n",
            ">>>> Epoch 17480:\t Train Loss: 2.2482\n",
            ">>>> Epoch 17481:\t Train Loss: 2.0794\n",
            ">>>> Epoch 17482:\t Train Loss: 1.8164\n",
            ">>>> Epoch 17483:\t Train Loss: 2.2855\n",
            ">>>> Epoch 17484:\t Train Loss: 2.6263\n",
            ">>>> Epoch 17485:\t Train Loss: 2.4448\n",
            ">>>> Epoch 17486:\t Train Loss: 2.2018\n",
            ">>>> Epoch 17487:\t Train Loss: 2.0080\n",
            ">>>> Epoch 17488:\t Train Loss: 2.9536\n",
            ">>>> Epoch 17489:\t Train Loss: 2.2214\n",
            ">>>> Epoch 17490:\t Train Loss: 2.5438\n",
            ">>>> Epoch 17491:\t Train Loss: 2.2615\n",
            ">>>> Epoch 17492:\t Train Loss: 1.9457\n",
            ">>>> Epoch 17493:\t Train Loss: 2.4252\n",
            ">>>> Epoch 17494:\t Train Loss: 2.2167\n",
            ">>>> Epoch 17495:\t Train Loss: 2.0639\n",
            ">>>> Epoch 17496:\t Train Loss: 1.9586\n",
            ">>>> Epoch 17497:\t Train Loss: 2.0625\n",
            ">>>> Epoch 17498:\t Train Loss: 2.3345\n",
            ">>>> Epoch 17499:\t Train Loss: 2.2345\n",
            ">>>> Epoch 17500:\t Train Loss: 1.9942\n",
            ">>>> Epoch 17501:\t Train Loss: 2.4961\n",
            ">>>> Epoch 17502:\t Train Loss: 2.3583\n",
            ">>>> Epoch 17503:\t Train Loss: 2.4070\n",
            ">>>> Epoch 17504:\t Train Loss: 2.4242\n",
            ">>>> Epoch 17505:\t Train Loss: 2.2101\n",
            ">>>> Epoch 17506:\t Train Loss: 2.3687\n",
            ">>>> Epoch 17507:\t Train Loss: 2.2937\n",
            ">>>> Epoch 17508:\t Train Loss: 2.4234\n",
            ">>>> Epoch 17509:\t Train Loss: 2.2017\n",
            ">>>> Epoch 17510:\t Train Loss: 2.1748\n",
            ">>>> Epoch 17511:\t Train Loss: 2.5118\n",
            ">>>> Epoch 17512:\t Train Loss: 1.8846\n",
            ">>>> Epoch 17513:\t Train Loss: 2.2072\n",
            ">>>> Epoch 17514:\t Train Loss: 2.3339\n",
            ">>>> Epoch 17515:\t Train Loss: 2.7939\n",
            ">>>> Epoch 17516:\t Train Loss: 2.2296\n",
            ">>>> Epoch 17517:\t Train Loss: 2.2120\n",
            ">>>> Epoch 17518:\t Train Loss: 1.9980\n",
            ">>>> Epoch 17519:\t Train Loss: 1.9759\n",
            ">>>> Epoch 17520:\t Train Loss: 2.3087\n",
            ">>>> Epoch 17521:\t Train Loss: 2.3137\n",
            ">>>> Epoch 17522:\t Train Loss: 2.3543\n",
            ">>>> Epoch 17523:\t Train Loss: 2.5052\n",
            ">>>> Epoch 17524:\t Train Loss: 2.5032\n",
            ">>>> Epoch 17525:\t Train Loss: 2.3102\n",
            ">>>> Epoch 17526:\t Train Loss: 2.4275\n",
            ">>>> Epoch 17527:\t Train Loss: 2.2236\n",
            ">>>> Epoch 17528:\t Train Loss: 2.2085\n",
            ">>>> Epoch 17529:\t Train Loss: 1.9931\n",
            ">>>> Epoch 17530:\t Train Loss: 2.2076\n",
            ">>>> Epoch 17531:\t Train Loss: 2.6166\n",
            ">>>> Epoch 17532:\t Train Loss: 2.1365\n",
            ">>>> Epoch 17533:\t Train Loss: 2.2968\n",
            ">>>> Epoch 17534:\t Train Loss: 2.0797\n",
            ">>>> Epoch 17535:\t Train Loss: 2.0557\n",
            ">>>> Epoch 17536:\t Train Loss: 1.9449\n",
            ">>>> Epoch 17537:\t Train Loss: 2.3488\n",
            ">>>> Epoch 17538:\t Train Loss: 2.3635\n",
            ">>>> Epoch 17539:\t Train Loss: 2.1464\n",
            ">>>> Epoch 17540:\t Train Loss: 2.5338\n",
            ">>>> Epoch 17541:\t Train Loss: 1.9447\n",
            ">>>> Epoch 17542:\t Train Loss: 2.5430\n",
            ">>>> Epoch 17543:\t Train Loss: 2.6624\n",
            ">>>> Epoch 17544:\t Train Loss: 1.8123\n",
            ">>>> Epoch 17545:\t Train Loss: 2.4050\n",
            ">>>> Epoch 17546:\t Train Loss: 2.4015\n",
            ">>>> Epoch 17547:\t Train Loss: 2.2086\n",
            ">>>> Epoch 17548:\t Train Loss: 1.8800\n",
            ">>>> Epoch 17549:\t Train Loss: 2.1251\n",
            ">>>> Epoch 17550:\t Train Loss: 2.2663\n",
            ">>>> Epoch 17551:\t Train Loss: 2.0745\n",
            ">>>> Epoch 17552:\t Train Loss: 2.4215\n",
            ">>>> Epoch 17553:\t Train Loss: 2.1916\n",
            ">>>> Epoch 17554:\t Train Loss: 1.7250\n",
            ">>>> Epoch 17555:\t Train Loss: 2.3517\n",
            ">>>> Epoch 17556:\t Train Loss: 2.5665\n",
            ">>>> Epoch 17557:\t Train Loss: 2.4118\n",
            ">>>> Epoch 17558:\t Train Loss: 1.9495\n",
            ">>>> Epoch 17559:\t Train Loss: 2.2025\n",
            ">>>> Epoch 17560:\t Train Loss: 2.3642\n",
            ">>>> Epoch 17561:\t Train Loss: 2.2188\n",
            ">>>> Epoch 17562:\t Train Loss: 2.1360\n",
            ">>>> Epoch 17563:\t Train Loss: 2.0847\n",
            ">>>> Epoch 17564:\t Train Loss: 2.1469\n",
            ">>>> Epoch 17565:\t Train Loss: 2.4061\n",
            ">>>> Epoch 17566:\t Train Loss: 2.0628\n",
            ">>>> Epoch 17567:\t Train Loss: 2.2783\n",
            ">>>> Epoch 17568:\t Train Loss: 2.3233\n",
            ">>>> Epoch 17569:\t Train Loss: 2.0940\n",
            ">>>> Epoch 17570:\t Train Loss: 2.1694\n",
            ">>>> Epoch 17571:\t Train Loss: 1.8927\n",
            ">>>> Epoch 17572:\t Train Loss: 2.2766\n",
            ">>>> Epoch 17573:\t Train Loss: 2.2856\n",
            ">>>> Epoch 17574:\t Train Loss: 1.9910\n",
            ">>>> Epoch 17575:\t Train Loss: 2.2485\n",
            ">>>> Epoch 17576:\t Train Loss: 2.3371\n",
            ">>>> Epoch 17577:\t Train Loss: 2.4764\n",
            ">>>> Epoch 17578:\t Train Loss: 2.1803\n",
            ">>>> Epoch 17579:\t Train Loss: 2.6818\n",
            ">>>> Epoch 17580:\t Train Loss: 2.2760\n",
            ">>>> Epoch 17581:\t Train Loss: 2.1865\n",
            ">>>> Epoch 17582:\t Train Loss: 2.5409\n",
            ">>>> Epoch 17583:\t Train Loss: 1.9297\n",
            ">>>> Epoch 17584:\t Train Loss: 2.2014\n",
            ">>>> Epoch 17585:\t Train Loss: 2.2111\n",
            ">>>> Epoch 17586:\t Train Loss: 1.9641\n",
            ">>>> Epoch 17587:\t Train Loss: 2.4683\n",
            ">>>> Epoch 17588:\t Train Loss: 2.3256\n",
            ">>>> Epoch 17589:\t Train Loss: 2.1797\n",
            ">>>> Epoch 17590:\t Train Loss: 2.2979\n",
            ">>>> Epoch 17591:\t Train Loss: 2.4073\n",
            ">>>> Epoch 17592:\t Train Loss: 2.0673\n",
            ">>>> Epoch 17593:\t Train Loss: 2.4070\n",
            ">>>> Epoch 17594:\t Train Loss: 2.0188\n",
            ">>>> Epoch 17595:\t Train Loss: 2.5200\n",
            ">>>> Epoch 17596:\t Train Loss: 2.4998\n",
            ">>>> Epoch 17597:\t Train Loss: 2.2995\n",
            ">>>> Epoch 17598:\t Train Loss: 2.0931\n",
            ">>>> Epoch 17599:\t Train Loss: 2.0284\n",
            ">>>> Epoch 17600:\t Train Loss: 2.3296\n",
            ">>>> Epoch 17601:\t Train Loss: 2.5469\n",
            ">>>> Epoch 17602:\t Train Loss: 2.2134\n",
            ">>>> Epoch 17603:\t Train Loss: 2.4323\n",
            ">>>> Epoch 17604:\t Train Loss: 2.0016\n",
            ">>>> Epoch 17605:\t Train Loss: 2.2766\n",
            ">>>> Epoch 17606:\t Train Loss: 2.2822\n",
            ">>>> Epoch 17607:\t Train Loss: 2.4165\n",
            ">>>> Epoch 17608:\t Train Loss: 2.0216\n",
            ">>>> Epoch 17609:\t Train Loss: 2.4802\n",
            ">>>> Epoch 17610:\t Train Loss: 2.4888\n",
            ">>>> Epoch 17611:\t Train Loss: 2.5291\n",
            ">>>> Epoch 17612:\t Train Loss: 2.0003\n",
            ">>>> Epoch 17613:\t Train Loss: 2.1807\n",
            ">>>> Epoch 17614:\t Train Loss: 2.2876\n",
            ">>>> Epoch 17615:\t Train Loss: 2.2012\n",
            ">>>> Epoch 17616:\t Train Loss: 2.1272\n",
            ">>>> Epoch 17617:\t Train Loss: 2.0986\n",
            ">>>> Epoch 17618:\t Train Loss: 2.4578\n",
            ">>>> Epoch 17619:\t Train Loss: 2.4180\n",
            ">>>> Epoch 17620:\t Train Loss: 2.0118\n",
            ">>>> Epoch 17621:\t Train Loss: 2.5190\n",
            ">>>> Epoch 17622:\t Train Loss: 2.1823\n",
            ">>>> Epoch 17623:\t Train Loss: 2.2133\n",
            ">>>> Epoch 17624:\t Train Loss: 2.0628\n",
            ">>>> Epoch 17625:\t Train Loss: 2.3876\n",
            ">>>> Epoch 17626:\t Train Loss: 2.1743\n",
            ">>>> Epoch 17627:\t Train Loss: 2.1696\n",
            ">>>> Epoch 17628:\t Train Loss: 2.3150\n",
            ">>>> Epoch 17629:\t Train Loss: 2.2934\n",
            ">>>> Epoch 17630:\t Train Loss: 2.2390\n",
            ">>>> Epoch 17631:\t Train Loss: 2.2774\n",
            ">>>> Epoch 17632:\t Train Loss: 1.9778\n",
            ">>>> Epoch 17633:\t Train Loss: 2.0162\n",
            ">>>> Epoch 17634:\t Train Loss: 2.3388\n",
            ">>>> Epoch 17635:\t Train Loss: 2.5511\n",
            ">>>> Epoch 17636:\t Train Loss: 2.0664\n",
            ">>>> Epoch 17637:\t Train Loss: 2.4816\n",
            ">>>> Epoch 17638:\t Train Loss: 2.1003\n",
            ">>>> Epoch 17639:\t Train Loss: 2.5498\n",
            ">>>> Epoch 17640:\t Train Loss: 2.6533\n",
            ">>>> Epoch 17641:\t Train Loss: 2.4409\n",
            ">>>> Epoch 17642:\t Train Loss: 2.3116\n",
            ">>>> Epoch 17643:\t Train Loss: 2.0560\n",
            ">>>> Epoch 17644:\t Train Loss: 2.3137\n",
            ">>>> Epoch 17645:\t Train Loss: 1.9860\n",
            ">>>> Epoch 17646:\t Train Loss: 1.6984\n",
            ">>>> Epoch 17647:\t Train Loss: 2.3452\n",
            ">>>> Epoch 17648:\t Train Loss: 1.9275\n",
            ">>>> Epoch 17649:\t Train Loss: 2.4524\n",
            ">>>> Epoch 17650:\t Train Loss: 2.2710\n",
            ">>>> Epoch 17651:\t Train Loss: 2.1149\n",
            ">>>> Epoch 17652:\t Train Loss: 2.2118\n",
            ">>>> Epoch 17653:\t Train Loss: 2.4067\n",
            ">>>> Epoch 17654:\t Train Loss: 2.0543\n",
            ">>>> Epoch 17655:\t Train Loss: 2.3141\n",
            ">>>> Epoch 17656:\t Train Loss: 2.1032\n",
            ">>>> Epoch 17657:\t Train Loss: 2.3393\n",
            ">>>> Epoch 17658:\t Train Loss: 1.9338\n",
            ">>>> Epoch 17659:\t Train Loss: 1.8436\n",
            ">>>> Epoch 17660:\t Train Loss: 2.5327\n",
            ">>>> Epoch 17661:\t Train Loss: 2.1696\n",
            ">>>> Epoch 17662:\t Train Loss: 2.3683\n",
            ">>>> Epoch 17663:\t Train Loss: 2.1884\n",
            ">>>> Epoch 17664:\t Train Loss: 1.9117\n",
            ">>>> Epoch 17665:\t Train Loss: 2.1712\n",
            ">>>> Epoch 17666:\t Train Loss: 2.2468\n",
            ">>>> Epoch 17667:\t Train Loss: 2.2141\n",
            ">>>> Epoch 17668:\t Train Loss: 2.0511\n",
            ">>>> Epoch 17669:\t Train Loss: 2.4403\n",
            ">>>> Epoch 17670:\t Train Loss: 2.4204\n",
            ">>>> Epoch 17671:\t Train Loss: 2.0251\n",
            ">>>> Epoch 17672:\t Train Loss: 1.7310\n",
            ">>>> Epoch 17673:\t Train Loss: 1.8347\n",
            ">>>> Epoch 17674:\t Train Loss: 2.0658\n",
            ">>>> Epoch 17675:\t Train Loss: 2.7767\n",
            ">>>> Epoch 17676:\t Train Loss: 2.0880\n",
            ">>>> Epoch 17677:\t Train Loss: 2.3098\n",
            ">>>> Epoch 17678:\t Train Loss: 2.2456\n",
            ">>>> Epoch 17679:\t Train Loss: 2.2565\n",
            ">>>> Epoch 17680:\t Train Loss: 2.3496\n",
            ">>>> Epoch 17681:\t Train Loss: 2.1840\n",
            ">>>> Epoch 17682:\t Train Loss: 1.9146\n",
            ">>>> Epoch 17683:\t Train Loss: 2.3594\n",
            ">>>> Epoch 17684:\t Train Loss: 2.2433\n",
            ">>>> Epoch 17685:\t Train Loss: 2.3219\n",
            ">>>> Epoch 17686:\t Train Loss: 2.5257\n",
            ">>>> Epoch 17687:\t Train Loss: 2.3393\n",
            ">>>> Epoch 17688:\t Train Loss: 2.1020\n",
            ">>>> Epoch 17689:\t Train Loss: 2.2158\n",
            ">>>> Epoch 17690:\t Train Loss: 2.3613\n",
            ">>>> Epoch 17691:\t Train Loss: 2.1227\n",
            ">>>> Epoch 17692:\t Train Loss: 2.3801\n",
            ">>>> Epoch 17693:\t Train Loss: 2.8352\n",
            ">>>> Epoch 17694:\t Train Loss: 2.4628\n",
            ">>>> Epoch 17695:\t Train Loss: 2.0190\n",
            ">>>> Epoch 17696:\t Train Loss: 2.0120\n",
            ">>>> Epoch 17697:\t Train Loss: 2.1365\n",
            ">>>> Epoch 17698:\t Train Loss: 2.1528\n",
            ">>>> Epoch 17699:\t Train Loss: 1.9872\n",
            ">>>> Epoch 17700:\t Train Loss: 2.0322\n",
            ">>>> Epoch 17701:\t Train Loss: 1.9206\n",
            ">>>> Epoch 17702:\t Train Loss: 2.4189\n",
            ">>>> Epoch 17703:\t Train Loss: 2.0220\n",
            ">>>> Epoch 17704:\t Train Loss: 2.2372\n",
            ">>>> Epoch 17705:\t Train Loss: 2.2871\n",
            ">>>> Epoch 17706:\t Train Loss: 2.1274\n",
            ">>>> Epoch 17707:\t Train Loss: 2.0460\n",
            ">>>> Epoch 17708:\t Train Loss: 2.2935\n",
            ">>>> Epoch 17709:\t Train Loss: 2.4270\n",
            ">>>> Epoch 17710:\t Train Loss: 2.0837\n",
            ">>>> Epoch 17711:\t Train Loss: 1.8195\n",
            ">>>> Epoch 17712:\t Train Loss: 2.5634\n",
            ">>>> Epoch 17713:\t Train Loss: 2.3550\n",
            ">>>> Epoch 17714:\t Train Loss: 2.3117\n",
            ">>>> Epoch 17715:\t Train Loss: 1.9889\n",
            ">>>> Epoch 17716:\t Train Loss: 2.4139\n",
            ">>>> Epoch 17717:\t Train Loss: 2.3450\n",
            ">>>> Epoch 17718:\t Train Loss: 2.0980\n",
            ">>>> Epoch 17719:\t Train Loss: 2.1872\n",
            ">>>> Epoch 17720:\t Train Loss: 2.1481\n",
            ">>>> Epoch 17721:\t Train Loss: 2.1329\n",
            ">>>> Epoch 17722:\t Train Loss: 2.3328\n",
            ">>>> Epoch 17723:\t Train Loss: 2.0611\n",
            ">>>> Epoch 17724:\t Train Loss: 2.1283\n",
            ">>>> Epoch 17725:\t Train Loss: 2.2061\n",
            ">>>> Epoch 17726:\t Train Loss: 2.1270\n",
            ">>>> Epoch 17727:\t Train Loss: 2.1179\n",
            ">>>> Epoch 17728:\t Train Loss: 2.5292\n",
            ">>>> Epoch 17729:\t Train Loss: 2.5554\n",
            ">>>> Epoch 17730:\t Train Loss: 2.4451\n",
            ">>>> Epoch 17731:\t Train Loss: 2.1498\n",
            ">>>> Epoch 17732:\t Train Loss: 2.0296\n",
            ">>>> Epoch 17733:\t Train Loss: 2.2429\n",
            ">>>> Epoch 17734:\t Train Loss: 1.9488\n",
            ">>>> Epoch 17735:\t Train Loss: 2.3257\n",
            ">>>> Epoch 17736:\t Train Loss: 2.1609\n",
            ">>>> Epoch 17737:\t Train Loss: 2.1567\n",
            ">>>> Epoch 17738:\t Train Loss: 1.8484\n",
            ">>>> Epoch 17739:\t Train Loss: 2.2152\n",
            ">>>> Epoch 17740:\t Train Loss: 2.3329\n",
            ">>>> Epoch 17741:\t Train Loss: 2.3353\n",
            ">>>> Epoch 17742:\t Train Loss: 2.1448\n",
            ">>>> Epoch 17743:\t Train Loss: 2.3233\n",
            ">>>> Epoch 17744:\t Train Loss: 1.8694\n",
            ">>>> Epoch 17745:\t Train Loss: 1.9727\n",
            ">>>> Epoch 17746:\t Train Loss: 2.3340\n",
            ">>>> Epoch 17747:\t Train Loss: 2.0786\n",
            ">>>> Epoch 17748:\t Train Loss: 1.9649\n",
            ">>>> Epoch 17749:\t Train Loss: 2.4084\n",
            ">>>> Epoch 17750:\t Train Loss: 1.9759\n",
            ">>>> Epoch 17751:\t Train Loss: 2.6297\n",
            ">>>> Epoch 17752:\t Train Loss: 2.1479\n",
            ">>>> Epoch 17753:\t Train Loss: 2.1398\n",
            ">>>> Epoch 17754:\t Train Loss: 1.7983\n",
            ">>>> Epoch 17755:\t Train Loss: 2.0988\n",
            ">>>> Epoch 17756:\t Train Loss: 2.2133\n",
            ">>>> Epoch 17757:\t Train Loss: 2.4979\n",
            ">>>> Epoch 17758:\t Train Loss: 2.0683\n",
            ">>>> Epoch 17759:\t Train Loss: 2.3591\n",
            ">>>> Epoch 17760:\t Train Loss: 2.4290\n",
            ">>>> Epoch 17761:\t Train Loss: 2.1593\n",
            ">>>> Epoch 17762:\t Train Loss: 2.3628\n",
            ">>>> Epoch 17763:\t Train Loss: 2.0946\n",
            ">>>> Epoch 17764:\t Train Loss: 2.4567\n",
            ">>>> Epoch 17765:\t Train Loss: 2.0317\n",
            ">>>> Epoch 17766:\t Train Loss: 2.5570\n",
            ">>>> Epoch 17767:\t Train Loss: 2.2880\n",
            ">>>> Epoch 17768:\t Train Loss: 2.3129\n",
            ">>>> Epoch 17769:\t Train Loss: 2.7318\n",
            ">>>> Epoch 17770:\t Train Loss: 2.0187\n",
            ">>>> Epoch 17771:\t Train Loss: 2.4884\n",
            ">>>> Epoch 17772:\t Train Loss: 2.0633\n",
            ">>>> Epoch 17773:\t Train Loss: 2.0236\n",
            ">>>> Epoch 17774:\t Train Loss: 2.1361\n",
            ">>>> Epoch 17775:\t Train Loss: 2.2371\n",
            ">>>> Epoch 17776:\t Train Loss: 2.2988\n",
            ">>>> Epoch 17777:\t Train Loss: 2.1563\n",
            ">>>> Epoch 17778:\t Train Loss: 1.9644\n",
            ">>>> Epoch 17779:\t Train Loss: 1.9086\n",
            ">>>> Epoch 17780:\t Train Loss: 2.2696\n",
            ">>>> Epoch 17781:\t Train Loss: 2.0876\n",
            ">>>> Epoch 17782:\t Train Loss: 2.1272\n",
            ">>>> Epoch 17783:\t Train Loss: 2.2372\n",
            ">>>> Epoch 17784:\t Train Loss: 2.3262\n",
            ">>>> Epoch 17785:\t Train Loss: 2.2548\n",
            ">>>> Epoch 17786:\t Train Loss: 2.0195\n",
            ">>>> Epoch 17787:\t Train Loss: 1.8900\n",
            ">>>> Epoch 17788:\t Train Loss: 2.2447\n",
            ">>>> Epoch 17789:\t Train Loss: 2.2637\n",
            ">>>> Epoch 17790:\t Train Loss: 2.0673\n",
            ">>>> Epoch 17791:\t Train Loss: 2.4848\n",
            ">>>> Epoch 17792:\t Train Loss: 1.9538\n",
            ">>>> Epoch 17793:\t Train Loss: 2.3336\n",
            ">>>> Epoch 17794:\t Train Loss: 2.2697\n",
            ">>>> Epoch 17795:\t Train Loss: 2.3735\n",
            ">>>> Epoch 17796:\t Train Loss: 2.6123\n",
            ">>>> Epoch 17797:\t Train Loss: 1.9019\n",
            ">>>> Epoch 17798:\t Train Loss: 2.2478\n",
            ">>>> Epoch 17799:\t Train Loss: 2.5390\n",
            ">>>> Epoch 17800:\t Train Loss: 2.2284\n",
            ">>>> Epoch 17801:\t Train Loss: 2.7851\n",
            ">>>> Epoch 17802:\t Train Loss: 1.9177\n",
            ">>>> Epoch 17803:\t Train Loss: 2.3034\n",
            ">>>> Epoch 17804:\t Train Loss: 1.9047\n",
            ">>>> Epoch 17805:\t Train Loss: 2.1476\n",
            ">>>> Epoch 17806:\t Train Loss: 1.8454\n",
            ">>>> Epoch 17807:\t Train Loss: 2.4450\n",
            ">>>> Epoch 17808:\t Train Loss: 1.8817\n",
            ">>>> Epoch 17809:\t Train Loss: 2.3709\n",
            ">>>> Epoch 17810:\t Train Loss: 2.0629\n",
            ">>>> Epoch 17811:\t Train Loss: 3.2252\n",
            ">>>> Epoch 17812:\t Train Loss: 2.4319\n",
            ">>>> Epoch 17813:\t Train Loss: 2.3060\n",
            ">>>> Epoch 17814:\t Train Loss: 1.9055\n",
            ">>>> Epoch 17815:\t Train Loss: 1.9879\n",
            ">>>> Epoch 17816:\t Train Loss: 2.8606\n",
            ">>>> Epoch 17817:\t Train Loss: 2.2134\n",
            ">>>> Epoch 17818:\t Train Loss: 1.9207\n",
            ">>>> Epoch 17819:\t Train Loss: 2.1414\n",
            ">>>> Epoch 17820:\t Train Loss: 2.2204\n",
            ">>>> Epoch 17821:\t Train Loss: 2.2733\n",
            ">>>> Epoch 17822:\t Train Loss: 2.3649\n",
            ">>>> Epoch 17823:\t Train Loss: 2.2586\n",
            ">>>> Epoch 17824:\t Train Loss: 1.8356\n",
            ">>>> Epoch 17825:\t Train Loss: 2.2528\n",
            ">>>> Epoch 17826:\t Train Loss: 2.4534\n",
            ">>>> Epoch 17827:\t Train Loss: 2.1917\n",
            ">>>> Epoch 17828:\t Train Loss: 2.3148\n",
            ">>>> Epoch 17829:\t Train Loss: 2.1189\n",
            ">>>> Epoch 17830:\t Train Loss: 1.9458\n",
            ">>>> Epoch 17831:\t Train Loss: 2.0358\n",
            ">>>> Epoch 17832:\t Train Loss: 2.1810\n",
            ">>>> Epoch 17833:\t Train Loss: 2.4822\n",
            ">>>> Epoch 17834:\t Train Loss: 2.0795\n",
            ">>>> Epoch 17835:\t Train Loss: 2.3216\n",
            ">>>> Epoch 17836:\t Train Loss: 2.2590\n",
            ">>>> Epoch 17837:\t Train Loss: 2.4487\n",
            ">>>> Epoch 17838:\t Train Loss: 2.0935\n",
            ">>>> Epoch 17839:\t Train Loss: 1.9198\n",
            ">>>> Epoch 17840:\t Train Loss: 2.3796\n",
            ">>>> Epoch 17841:\t Train Loss: 2.3573\n",
            ">>>> Epoch 17842:\t Train Loss: 2.4396\n",
            ">>>> Epoch 17843:\t Train Loss: 2.3362\n",
            ">>>> Epoch 17844:\t Train Loss: 2.4365\n",
            ">>>> Epoch 17845:\t Train Loss: 2.4272\n",
            ">>>> Epoch 17846:\t Train Loss: 2.3309\n",
            ">>>> Epoch 17847:\t Train Loss: 2.2944\n",
            ">>>> Epoch 17848:\t Train Loss: 2.1589\n",
            ">>>> Epoch 17849:\t Train Loss: 2.2146\n",
            ">>>> Epoch 17850:\t Train Loss: 2.1637\n",
            ">>>> Epoch 17851:\t Train Loss: 2.2955\n",
            ">>>> Epoch 17852:\t Train Loss: 2.0439\n",
            ">>>> Epoch 17853:\t Train Loss: 1.9537\n",
            ">>>> Epoch 17854:\t Train Loss: 1.6759\n",
            ">>>> Epoch 17855:\t Train Loss: 2.3893\n",
            ">>>> Epoch 17856:\t Train Loss: 2.6937\n",
            ">>>> Epoch 17857:\t Train Loss: 2.1168\n",
            ">>>> Epoch 17858:\t Train Loss: 2.2475\n",
            ">>>> Epoch 17859:\t Train Loss: 2.3964\n",
            ">>>> Epoch 17860:\t Train Loss: 1.9210\n",
            ">>>> Epoch 17861:\t Train Loss: 2.4354\n",
            ">>>> Epoch 17862:\t Train Loss: 2.4392\n",
            ">>>> Epoch 17863:\t Train Loss: 2.1866\n",
            ">>>> Epoch 17864:\t Train Loss: 2.0214\n",
            ">>>> Epoch 17865:\t Train Loss: 2.0675\n",
            ">>>> Epoch 17866:\t Train Loss: 2.0822\n",
            ">>>> Epoch 17867:\t Train Loss: 2.3370\n",
            ">>>> Epoch 17868:\t Train Loss: 2.3218\n",
            ">>>> Epoch 17869:\t Train Loss: 2.0666\n",
            ">>>> Epoch 17870:\t Train Loss: 2.0101\n",
            ">>>> Epoch 17871:\t Train Loss: 2.3356\n",
            ">>>> Epoch 17872:\t Train Loss: 2.3991\n",
            ">>>> Epoch 17873:\t Train Loss: 1.8814\n",
            ">>>> Epoch 17874:\t Train Loss: 1.8944\n",
            ">>>> Epoch 17875:\t Train Loss: 2.1735\n",
            ">>>> Epoch 17876:\t Train Loss: 2.1787\n",
            ">>>> Epoch 17877:\t Train Loss: 1.9871\n",
            ">>>> Epoch 17878:\t Train Loss: 2.2607\n",
            ">>>> Epoch 17879:\t Train Loss: 2.2995\n",
            ">>>> Epoch 17880:\t Train Loss: 2.2978\n",
            ">>>> Epoch 17881:\t Train Loss: 2.2423\n",
            ">>>> Epoch 17882:\t Train Loss: 2.6499\n",
            ">>>> Epoch 17883:\t Train Loss: 2.3413\n",
            ">>>> Epoch 17884:\t Train Loss: 2.0182\n",
            ">>>> Epoch 17885:\t Train Loss: 2.5074\n",
            ">>>> Epoch 17886:\t Train Loss: 2.3098\n",
            ">>>> Epoch 17887:\t Train Loss: 2.2067\n",
            ">>>> Epoch 17888:\t Train Loss: 2.2329\n",
            ">>>> Epoch 17889:\t Train Loss: 2.3861\n",
            ">>>> Epoch 17890:\t Train Loss: 2.1749\n",
            ">>>> Epoch 17891:\t Train Loss: 2.0780\n",
            ">>>> Epoch 17892:\t Train Loss: 2.1611\n",
            ">>>> Epoch 17893:\t Train Loss: 2.6956\n",
            ">>>> Epoch 17894:\t Train Loss: 2.0901\n",
            ">>>> Epoch 17895:\t Train Loss: 2.7153\n",
            ">>>> Epoch 17896:\t Train Loss: 2.1464\n",
            ">>>> Epoch 17897:\t Train Loss: 2.0635\n",
            ">>>> Epoch 17898:\t Train Loss: 2.3624\n",
            ">>>> Epoch 17899:\t Train Loss: 2.3060\n",
            ">>>> Epoch 17900:\t Train Loss: 2.1715\n",
            ">>>> Epoch 17901:\t Train Loss: 2.3535\n",
            ">>>> Epoch 17902:\t Train Loss: 2.4596\n",
            ">>>> Epoch 17903:\t Train Loss: 1.9626\n",
            ">>>> Epoch 17904:\t Train Loss: 1.8793\n",
            ">>>> Epoch 17905:\t Train Loss: 2.1301\n",
            ">>>> Epoch 17906:\t Train Loss: 2.1535\n",
            ">>>> Epoch 17907:\t Train Loss: 1.8360\n",
            ">>>> Epoch 17908:\t Train Loss: 2.4083\n",
            ">>>> Epoch 17909:\t Train Loss: 2.5047\n",
            ">>>> Epoch 17910:\t Train Loss: 1.9706\n",
            ">>>> Epoch 17911:\t Train Loss: 2.3166\n",
            ">>>> Epoch 17912:\t Train Loss: 2.2474\n",
            ">>>> Epoch 17913:\t Train Loss: 2.1555\n",
            ">>>> Epoch 17914:\t Train Loss: 1.9241\n",
            ">>>> Epoch 17915:\t Train Loss: 2.2716\n",
            ">>>> Epoch 17916:\t Train Loss: 2.4477\n",
            ">>>> Epoch 17917:\t Train Loss: 2.5806\n",
            ">>>> Epoch 17918:\t Train Loss: 1.8167\n",
            ">>>> Epoch 17919:\t Train Loss: 2.2953\n",
            ">>>> Epoch 17920:\t Train Loss: 2.5423\n",
            ">>>> Epoch 17921:\t Train Loss: 2.3308\n",
            ">>>> Epoch 17922:\t Train Loss: 2.3109\n",
            ">>>> Epoch 17923:\t Train Loss: 2.1938\n",
            ">>>> Epoch 17924:\t Train Loss: 2.3475\n",
            ">>>> Epoch 17925:\t Train Loss: 2.5706\n",
            ">>>> Epoch 17926:\t Train Loss: 2.3064\n",
            ">>>> Epoch 17927:\t Train Loss: 2.2033\n",
            ">>>> Epoch 17928:\t Train Loss: 2.3265\n",
            ">>>> Epoch 17929:\t Train Loss: 2.2603\n",
            ">>>> Epoch 17930:\t Train Loss: 2.2806\n",
            ">>>> Epoch 17931:\t Train Loss: 1.9993\n",
            ">>>> Epoch 17932:\t Train Loss: 2.3799\n",
            ">>>> Epoch 17933:\t Train Loss: 2.4492\n",
            ">>>> Epoch 17934:\t Train Loss: 2.1635\n",
            ">>>> Epoch 17935:\t Train Loss: 2.0534\n",
            ">>>> Epoch 17936:\t Train Loss: 2.0532\n",
            ">>>> Epoch 17937:\t Train Loss: 2.5423\n",
            ">>>> Epoch 17938:\t Train Loss: 2.5770\n",
            ">>>> Epoch 17939:\t Train Loss: 2.0050\n",
            ">>>> Epoch 17940:\t Train Loss: 2.0742\n",
            ">>>> Epoch 17941:\t Train Loss: 2.4247\n",
            ">>>> Epoch 17942:\t Train Loss: 2.5219\n",
            ">>>> Epoch 17943:\t Train Loss: 2.3763\n",
            ">>>> Epoch 17944:\t Train Loss: 2.0436\n",
            ">>>> Epoch 17945:\t Train Loss: 2.6458\n",
            ">>>> Epoch 17946:\t Train Loss: 2.1814\n",
            ">>>> Epoch 17947:\t Train Loss: 2.1413\n",
            ">>>> Epoch 17948:\t Train Loss: 2.0663\n",
            ">>>> Epoch 17949:\t Train Loss: 2.0331\n",
            ">>>> Epoch 17950:\t Train Loss: 2.4970\n",
            ">>>> Epoch 17951:\t Train Loss: 2.3096\n",
            ">>>> Epoch 17952:\t Train Loss: 2.3465\n",
            ">>>> Epoch 17953:\t Train Loss: 2.3956\n",
            ">>>> Epoch 17954:\t Train Loss: 2.0388\n",
            ">>>> Epoch 17955:\t Train Loss: 2.2844\n",
            ">>>> Epoch 17956:\t Train Loss: 2.1359\n",
            ">>>> Epoch 17957:\t Train Loss: 2.3672\n",
            ">>>> Epoch 17958:\t Train Loss: 2.4575\n",
            ">>>> Epoch 17959:\t Train Loss: 2.1596\n",
            ">>>> Epoch 17960:\t Train Loss: 2.1150\n",
            ">>>> Epoch 17961:\t Train Loss: 2.4652\n",
            ">>>> Epoch 17962:\t Train Loss: 2.2626\n",
            ">>>> Epoch 17963:\t Train Loss: 2.0589\n",
            ">>>> Epoch 17964:\t Train Loss: 2.2781\n",
            ">>>> Epoch 17965:\t Train Loss: 1.9850\n",
            ">>>> Epoch 17966:\t Train Loss: 1.8383\n",
            ">>>> Epoch 17967:\t Train Loss: 2.0533\n",
            ">>>> Epoch 17968:\t Train Loss: 2.6075\n",
            ">>>> Epoch 17969:\t Train Loss: 2.1760\n",
            ">>>> Epoch 17970:\t Train Loss: 2.5993\n",
            ">>>> Epoch 17971:\t Train Loss: 2.3145\n",
            ">>>> Epoch 17972:\t Train Loss: 2.3154\n",
            ">>>> Epoch 17973:\t Train Loss: 2.1906\n",
            ">>>> Epoch 17974:\t Train Loss: 2.0841\n",
            ">>>> Epoch 17975:\t Train Loss: 2.1391\n",
            ">>>> Epoch 17976:\t Train Loss: 1.9546\n",
            ">>>> Epoch 17977:\t Train Loss: 2.3293\n",
            ">>>> Epoch 17978:\t Train Loss: 2.2634\n",
            ">>>> Epoch 17979:\t Train Loss: 2.3126\n",
            ">>>> Epoch 17980:\t Train Loss: 1.9513\n",
            ">>>> Epoch 17981:\t Train Loss: 2.1300\n",
            ">>>> Epoch 17982:\t Train Loss: 2.3013\n",
            ">>>> Epoch 17983:\t Train Loss: 2.3108\n",
            ">>>> Epoch 17984:\t Train Loss: 1.9093\n",
            ">>>> Epoch 17985:\t Train Loss: 2.2438\n",
            ">>>> Epoch 17986:\t Train Loss: 2.2427\n",
            ">>>> Epoch 17987:\t Train Loss: 2.9497\n",
            ">>>> Epoch 17988:\t Train Loss: 2.3274\n",
            ">>>> Epoch 17989:\t Train Loss: 2.2489\n",
            ">>>> Epoch 17990:\t Train Loss: 2.0315\n",
            ">>>> Epoch 17991:\t Train Loss: 2.2397\n",
            ">>>> Epoch 17992:\t Train Loss: 1.9759\n",
            ">>>> Epoch 17993:\t Train Loss: 1.9242\n",
            ">>>> Epoch 17994:\t Train Loss: 1.8524\n",
            ">>>> Epoch 17995:\t Train Loss: 2.1698\n",
            ">>>> Epoch 17996:\t Train Loss: 2.3849\n",
            ">>>> Epoch 17997:\t Train Loss: 2.1563\n",
            ">>>> Epoch 17998:\t Train Loss: 2.1659\n",
            ">>>> Epoch 17999:\t Train Loss: 2.2956\n",
            ">>>> Epoch 18000:\t Train Loss: 2.5926\n",
            ">>>> Epoch 18001:\t Train Loss: 2.1511\n",
            ">>>> Epoch 18002:\t Train Loss: 2.1466\n",
            ">>>> Epoch 18003:\t Train Loss: 2.0271\n",
            ">>>> Epoch 18004:\t Train Loss: 2.8737\n",
            ">>>> Epoch 18005:\t Train Loss: 2.6073\n",
            ">>>> Epoch 18006:\t Train Loss: 2.0960\n",
            ">>>> Epoch 18007:\t Train Loss: 2.2627\n",
            ">>>> Epoch 18008:\t Train Loss: 2.4246\n",
            ">>>> Epoch 18009:\t Train Loss: 2.3857\n",
            ">>>> Epoch 18010:\t Train Loss: 2.2152\n",
            ">>>> Epoch 18011:\t Train Loss: 2.2930\n",
            ">>>> Epoch 18012:\t Train Loss: 2.7243\n",
            ">>>> Epoch 18013:\t Train Loss: 2.0537\n",
            ">>>> Epoch 18014:\t Train Loss: 2.2002\n",
            ">>>> Epoch 18015:\t Train Loss: 2.4673\n",
            ">>>> Epoch 18016:\t Train Loss: 2.4149\n",
            ">>>> Epoch 18017:\t Train Loss: 2.1797\n",
            ">>>> Epoch 18018:\t Train Loss: 2.1718\n",
            ">>>> Epoch 18019:\t Train Loss: 1.9279\n",
            ">>>> Epoch 18020:\t Train Loss: 1.8481\n",
            ">>>> Epoch 18021:\t Train Loss: 2.2171\n",
            ">>>> Epoch 18022:\t Train Loss: 2.1193\n",
            ">>>> Epoch 18023:\t Train Loss: 2.6853\n",
            ">>>> Epoch 18024:\t Train Loss: 1.7961\n",
            ">>>> Epoch 18025:\t Train Loss: 2.2928\n",
            ">>>> Epoch 18026:\t Train Loss: 2.4002\n",
            ">>>> Epoch 18027:\t Train Loss: 2.0610\n",
            ">>>> Epoch 18028:\t Train Loss: 2.2069\n",
            ">>>> Epoch 18029:\t Train Loss: 2.2784\n",
            ">>>> Epoch 18030:\t Train Loss: 2.3285\n",
            ">>>> Epoch 18031:\t Train Loss: 2.3480\n",
            ">>>> Epoch 18032:\t Train Loss: 1.7701\n",
            ">>>> Epoch 18033:\t Train Loss: 2.2152\n",
            ">>>> Epoch 18034:\t Train Loss: 2.1460\n",
            ">>>> Epoch 18035:\t Train Loss: 2.0191\n",
            ">>>> Epoch 18036:\t Train Loss: 1.9460\n",
            ">>>> Epoch 18037:\t Train Loss: 2.0550\n",
            ">>>> Epoch 18038:\t Train Loss: 2.3740\n",
            ">>>> Epoch 18039:\t Train Loss: 2.0870\n",
            ">>>> Epoch 18040:\t Train Loss: 2.1441\n",
            ">>>> Epoch 18041:\t Train Loss: 2.0323\n",
            ">>>> Epoch 18042:\t Train Loss: 2.2228\n",
            ">>>> Epoch 18043:\t Train Loss: 1.9018\n",
            ">>>> Epoch 18044:\t Train Loss: 2.4277\n",
            ">>>> Epoch 18045:\t Train Loss: 2.2159\n",
            ">>>> Epoch 18046:\t Train Loss: 2.6993\n",
            ">>>> Epoch 18047:\t Train Loss: 2.6572\n",
            ">>>> Epoch 18048:\t Train Loss: 1.8628\n",
            ">>>> Epoch 18049:\t Train Loss: 2.2864\n",
            ">>>> Epoch 18050:\t Train Loss: 2.0816\n",
            ">>>> Epoch 18051:\t Train Loss: 2.1411\n",
            ">>>> Epoch 18052:\t Train Loss: 2.0674\n",
            ">>>> Epoch 18053:\t Train Loss: 2.5709\n",
            ">>>> Epoch 18054:\t Train Loss: 2.4017\n",
            ">>>> Epoch 18055:\t Train Loss: 2.4102\n",
            ">>>> Epoch 18056:\t Train Loss: 2.2869\n",
            ">>>> Epoch 18057:\t Train Loss: 2.1406\n",
            ">>>> Epoch 18058:\t Train Loss: 2.0578\n",
            ">>>> Epoch 18059:\t Train Loss: 2.0119\n",
            ">>>> Epoch 18060:\t Train Loss: 2.4611\n",
            ">>>> Epoch 18061:\t Train Loss: 2.2063\n",
            ">>>> Epoch 18062:\t Train Loss: 2.3467\n",
            ">>>> Epoch 18063:\t Train Loss: 2.3824\n",
            ">>>> Epoch 18064:\t Train Loss: 2.3951\n",
            ">>>> Epoch 18065:\t Train Loss: 2.0295\n",
            ">>>> Epoch 18066:\t Train Loss: 2.3856\n",
            ">>>> Epoch 18067:\t Train Loss: 2.2438\n",
            ">>>> Epoch 18068:\t Train Loss: 2.3968\n",
            ">>>> Epoch 18069:\t Train Loss: 2.1795\n",
            ">>>> Epoch 18070:\t Train Loss: 2.4022\n",
            ">>>> Epoch 18071:\t Train Loss: 2.4686\n",
            ">>>> Epoch 18072:\t Train Loss: 2.6899\n",
            ">>>> Epoch 18073:\t Train Loss: 1.8419\n",
            ">>>> Epoch 18074:\t Train Loss: 2.1994\n",
            ">>>> Epoch 18075:\t Train Loss: 2.0705\n",
            ">>>> Epoch 18076:\t Train Loss: 2.1420\n",
            ">>>> Epoch 18077:\t Train Loss: 2.1984\n",
            ">>>> Epoch 18078:\t Train Loss: 2.3395\n",
            ">>>> Epoch 18079:\t Train Loss: 2.3414\n",
            ">>>> Epoch 18080:\t Train Loss: 2.3269\n",
            ">>>> Epoch 18081:\t Train Loss: 2.2086\n",
            ">>>> Epoch 18082:\t Train Loss: 1.7996\n",
            ">>>> Epoch 18083:\t Train Loss: 2.4432\n",
            ">>>> Epoch 18084:\t Train Loss: 2.0244\n",
            ">>>> Epoch 18085:\t Train Loss: 1.9426\n",
            ">>>> Epoch 18086:\t Train Loss: 2.3426\n",
            ">>>> Epoch 18087:\t Train Loss: 2.1241\n",
            ">>>> Epoch 18088:\t Train Loss: 1.8611\n",
            ">>>> Epoch 18089:\t Train Loss: 2.0226\n",
            ">>>> Epoch 18090:\t Train Loss: 2.2656\n",
            ">>>> Epoch 18091:\t Train Loss: 2.2087\n",
            ">>>> Epoch 18092:\t Train Loss: 2.6744\n",
            ">>>> Epoch 18093:\t Train Loss: 1.7247\n",
            ">>>> Epoch 18094:\t Train Loss: 1.6726\n",
            ">>>> Epoch 18095:\t Train Loss: 2.4135\n",
            ">>>> Epoch 18096:\t Train Loss: 2.4081\n",
            ">>>> Epoch 18097:\t Train Loss: 2.3853\n",
            ">>>> Epoch 18098:\t Train Loss: 1.8712\n",
            ">>>> Epoch 18099:\t Train Loss: 2.3210\n",
            ">>>> Epoch 18100:\t Train Loss: 2.3772\n",
            ">>>> Epoch 18101:\t Train Loss: 2.3323\n",
            ">>>> Epoch 18102:\t Train Loss: 2.0734\n",
            ">>>> Epoch 18103:\t Train Loss: 2.4373\n",
            ">>>> Epoch 18104:\t Train Loss: 2.0782\n",
            ">>>> Epoch 18105:\t Train Loss: 2.1590\n",
            ">>>> Epoch 18106:\t Train Loss: 2.4106\n",
            ">>>> Epoch 18107:\t Train Loss: 2.3505\n",
            ">>>> Epoch 18108:\t Train Loss: 2.1543\n",
            ">>>> Epoch 18109:\t Train Loss: 2.5980\n",
            ">>>> Epoch 18110:\t Train Loss: 2.1751\n",
            ">>>> Epoch 18111:\t Train Loss: 2.1834\n",
            ">>>> Epoch 18112:\t Train Loss: 2.2199\n",
            ">>>> Epoch 18113:\t Train Loss: 1.9989\n",
            ">>>> Epoch 18114:\t Train Loss: 1.7352\n",
            ">>>> Epoch 18115:\t Train Loss: 2.2035\n",
            ">>>> Epoch 18116:\t Train Loss: 2.1870\n",
            ">>>> Epoch 18117:\t Train Loss: 2.1737\n",
            ">>>> Epoch 18118:\t Train Loss: 2.1876\n",
            ">>>> Epoch 18119:\t Train Loss: 2.0076\n",
            ">>>> Epoch 18120:\t Train Loss: 2.4258\n",
            ">>>> Epoch 18121:\t Train Loss: 2.2031\n",
            ">>>> Epoch 18122:\t Train Loss: 1.9884\n",
            ">>>> Epoch 18123:\t Train Loss: 1.9511\n",
            ">>>> Epoch 18124:\t Train Loss: 2.4246\n",
            ">>>> Epoch 18125:\t Train Loss: 2.1674\n",
            ">>>> Epoch 18126:\t Train Loss: 1.8924\n",
            ">>>> Epoch 18127:\t Train Loss: 2.3002\n",
            ">>>> Epoch 18128:\t Train Loss: 2.5409\n",
            ">>>> Epoch 18129:\t Train Loss: 2.2610\n",
            ">>>> Epoch 18130:\t Train Loss: 1.9854\n",
            ">>>> Epoch 18131:\t Train Loss: 2.1737\n",
            ">>>> Epoch 18132:\t Train Loss: 2.4735\n",
            ">>>> Epoch 18133:\t Train Loss: 2.3281\n",
            ">>>> Epoch 18134:\t Train Loss: 2.2115\n",
            ">>>> Epoch 18135:\t Train Loss: 2.4098\n",
            ">>>> Epoch 18136:\t Train Loss: 2.1739\n",
            ">>>> Epoch 18137:\t Train Loss: 2.1450\n",
            ">>>> Epoch 18138:\t Train Loss: 2.2177\n",
            ">>>> Epoch 18139:\t Train Loss: 2.2631\n",
            ">>>> Epoch 18140:\t Train Loss: 2.5476\n",
            ">>>> Epoch 18141:\t Train Loss: 2.8231\n",
            ">>>> Epoch 18142:\t Train Loss: 2.3635\n",
            ">>>> Epoch 18143:\t Train Loss: 2.0208\n",
            ">>>> Epoch 18144:\t Train Loss: 2.3732\n",
            ">>>> Epoch 18145:\t Train Loss: 2.3596\n",
            ">>>> Epoch 18146:\t Train Loss: 2.4028\n",
            ">>>> Epoch 18147:\t Train Loss: 2.3077\n",
            ">>>> Epoch 18148:\t Train Loss: 2.0547\n",
            ">>>> Epoch 18149:\t Train Loss: 2.2097\n",
            ">>>> Epoch 18150:\t Train Loss: 2.0726\n",
            ">>>> Epoch 18151:\t Train Loss: 2.3487\n",
            ">>>> Epoch 18152:\t Train Loss: 2.3768\n",
            ">>>> Epoch 18153:\t Train Loss: 1.9711\n",
            ">>>> Epoch 18154:\t Train Loss: 2.4303\n",
            ">>>> Epoch 18155:\t Train Loss: 2.1900\n",
            ">>>> Epoch 18156:\t Train Loss: 2.1846\n",
            ">>>> Epoch 18157:\t Train Loss: 2.1287\n",
            ">>>> Epoch 18158:\t Train Loss: 2.6763\n",
            ">>>> Epoch 18159:\t Train Loss: 2.0785\n",
            ">>>> Epoch 18160:\t Train Loss: 2.5950\n",
            ">>>> Epoch 18161:\t Train Loss: 2.0500\n",
            ">>>> Epoch 18162:\t Train Loss: 1.8683\n",
            ">>>> Epoch 18163:\t Train Loss: 2.0994\n",
            ">>>> Epoch 18164:\t Train Loss: 2.2359\n",
            ">>>> Epoch 18165:\t Train Loss: 2.8375\n",
            ">>>> Epoch 18166:\t Train Loss: 2.2982\n",
            ">>>> Epoch 18167:\t Train Loss: 2.2103\n",
            ">>>> Epoch 18168:\t Train Loss: 2.7632\n",
            ">>>> Epoch 18169:\t Train Loss: 2.1363\n",
            ">>>> Epoch 18170:\t Train Loss: 2.2508\n",
            ">>>> Epoch 18171:\t Train Loss: 2.1847\n",
            ">>>> Epoch 18172:\t Train Loss: 1.7290\n",
            ">>>> Epoch 18173:\t Train Loss: 1.9303\n",
            ">>>> Epoch 18174:\t Train Loss: 2.0812\n",
            ">>>> Epoch 18175:\t Train Loss: 2.0426\n",
            ">>>> Epoch 18176:\t Train Loss: 2.6278\n",
            ">>>> Epoch 18177:\t Train Loss: 2.2256\n",
            ">>>> Epoch 18178:\t Train Loss: 2.2851\n",
            ">>>> Epoch 18179:\t Train Loss: 2.3234\n",
            ">>>> Epoch 18180:\t Train Loss: 2.2354\n",
            ">>>> Epoch 18181:\t Train Loss: 2.0070\n",
            ">>>> Epoch 18182:\t Train Loss: 2.3068\n",
            ">>>> Epoch 18183:\t Train Loss: 2.0385\n",
            ">>>> Epoch 18184:\t Train Loss: 1.8358\n",
            ">>>> Epoch 18185:\t Train Loss: 2.3889\n",
            ">>>> Epoch 18186:\t Train Loss: 2.4837\n",
            ">>>> Epoch 18187:\t Train Loss: 2.3487\n",
            ">>>> Epoch 18188:\t Train Loss: 2.0996\n",
            ">>>> Epoch 18189:\t Train Loss: 2.0933\n",
            ">>>> Epoch 18190:\t Train Loss: 2.1320\n",
            ">>>> Epoch 18191:\t Train Loss: 2.2809\n",
            ">>>> Epoch 18192:\t Train Loss: 2.3828\n",
            ">>>> Epoch 18193:\t Train Loss: 2.1827\n",
            ">>>> Epoch 18194:\t Train Loss: 2.3132\n",
            ">>>> Epoch 18195:\t Train Loss: 2.0694\n",
            ">>>> Epoch 18196:\t Train Loss: 2.3751\n",
            ">>>> Epoch 18197:\t Train Loss: 2.1841\n",
            ">>>> Epoch 18198:\t Train Loss: 2.0136\n",
            ">>>> Epoch 18199:\t Train Loss: 2.1089\n",
            ">>>> Epoch 18200:\t Train Loss: 2.2464\n",
            ">>>> Epoch 18201:\t Train Loss: 2.4854\n",
            ">>>> Epoch 18202:\t Train Loss: 2.2282\n",
            ">>>> Epoch 18203:\t Train Loss: 2.6101\n",
            ">>>> Epoch 18204:\t Train Loss: 1.7770\n",
            ">>>> Epoch 18205:\t Train Loss: 2.3857\n",
            ">>>> Epoch 18206:\t Train Loss: 1.9216\n",
            ">>>> Epoch 18207:\t Train Loss: 2.1174\n",
            ">>>> Epoch 18208:\t Train Loss: 2.1416\n",
            ">>>> Epoch 18209:\t Train Loss: 2.1722\n",
            ">>>> Epoch 18210:\t Train Loss: 2.1676\n",
            ">>>> Epoch 18211:\t Train Loss: 2.1459\n",
            ">>>> Epoch 18212:\t Train Loss: 2.0853\n",
            ">>>> Epoch 18213:\t Train Loss: 2.2705\n",
            ">>>> Epoch 18214:\t Train Loss: 2.2931\n",
            ">>>> Epoch 18215:\t Train Loss: 2.1580\n",
            ">>>> Epoch 18216:\t Train Loss: 2.2503\n",
            ">>>> Epoch 18217:\t Train Loss: 2.0918\n",
            ">>>> Epoch 18218:\t Train Loss: 2.3538\n",
            ">>>> Epoch 18219:\t Train Loss: 2.1313\n",
            ">>>> Epoch 18220:\t Train Loss: 2.1230\n",
            ">>>> Epoch 18221:\t Train Loss: 2.2854\n",
            ">>>> Epoch 18222:\t Train Loss: 2.2836\n",
            ">>>> Epoch 18223:\t Train Loss: 2.4183\n",
            ">>>> Epoch 18224:\t Train Loss: 2.2848\n",
            ">>>> Epoch 18225:\t Train Loss: 1.9402\n",
            ">>>> Epoch 18226:\t Train Loss: 2.4533\n",
            ">>>> Epoch 18227:\t Train Loss: 2.8100\n",
            ">>>> Epoch 18228:\t Train Loss: 2.1625\n",
            ">>>> Epoch 18229:\t Train Loss: 2.3497\n",
            ">>>> Epoch 18230:\t Train Loss: 2.4099\n",
            ">>>> Epoch 18231:\t Train Loss: 2.2484\n",
            ">>>> Epoch 18232:\t Train Loss: 2.1465\n",
            ">>>> Epoch 18233:\t Train Loss: 2.3048\n",
            ">>>> Epoch 18234:\t Train Loss: 2.1197\n",
            ">>>> Epoch 18235:\t Train Loss: 1.9718\n",
            ">>>> Epoch 18236:\t Train Loss: 2.5564\n",
            ">>>> Epoch 18237:\t Train Loss: 1.9735\n",
            ">>>> Epoch 18238:\t Train Loss: 2.2916\n",
            ">>>> Epoch 18239:\t Train Loss: 2.3179\n",
            ">>>> Epoch 18240:\t Train Loss: 2.4604\n",
            ">>>> Epoch 18241:\t Train Loss: 2.5062\n",
            ">>>> Epoch 18242:\t Train Loss: 2.2180\n",
            ">>>> Epoch 18243:\t Train Loss: 2.3176\n",
            ">>>> Epoch 18244:\t Train Loss: 2.2896\n",
            ">>>> Epoch 18245:\t Train Loss: 2.3307\n",
            ">>>> Epoch 18246:\t Train Loss: 2.3299\n",
            ">>>> Epoch 18247:\t Train Loss: 1.8717\n",
            ">>>> Epoch 18248:\t Train Loss: 2.2094\n",
            ">>>> Epoch 18249:\t Train Loss: 2.3607\n",
            ">>>> Epoch 18250:\t Train Loss: 2.1487\n",
            ">>>> Epoch 18251:\t Train Loss: 2.1473\n",
            ">>>> Epoch 18252:\t Train Loss: 1.6996\n",
            ">>>> Epoch 18253:\t Train Loss: 2.2167\n",
            ">>>> Epoch 18254:\t Train Loss: 2.0508\n",
            ">>>> Epoch 18255:\t Train Loss: 2.1109\n",
            ">>>> Epoch 18256:\t Train Loss: 2.5113\n",
            ">>>> Epoch 18257:\t Train Loss: 1.8560\n",
            ">>>> Epoch 18258:\t Train Loss: 2.1526\n",
            ">>>> Epoch 18259:\t Train Loss: 2.4404\n",
            ">>>> Epoch 18260:\t Train Loss: 2.1692\n",
            ">>>> Epoch 18261:\t Train Loss: 1.9748\n",
            ">>>> Epoch 18262:\t Train Loss: 1.8878\n",
            ">>>> Epoch 18263:\t Train Loss: 2.4573\n",
            ">>>> Epoch 18264:\t Train Loss: 2.3277\n",
            ">>>> Epoch 18265:\t Train Loss: 2.6537\n",
            ">>>> Epoch 18266:\t Train Loss: 2.2821\n",
            ">>>> Epoch 18267:\t Train Loss: 2.5379\n",
            ">>>> Epoch 18268:\t Train Loss: 2.1963\n",
            ">>>> Epoch 18269:\t Train Loss: 2.1514\n",
            ">>>> Epoch 18270:\t Train Loss: 2.3251\n",
            ">>>> Epoch 18271:\t Train Loss: 2.3882\n",
            ">>>> Epoch 18272:\t Train Loss: 2.1438\n",
            ">>>> Epoch 18273:\t Train Loss: 2.3551\n",
            ">>>> Epoch 18274:\t Train Loss: 2.6200\n",
            ">>>> Epoch 18275:\t Train Loss: 2.2411\n",
            ">>>> Epoch 18276:\t Train Loss: 2.3582\n",
            ">>>> Epoch 18277:\t Train Loss: 2.2687\n",
            ">>>> Epoch 18278:\t Train Loss: 2.6749\n",
            ">>>> Epoch 18279:\t Train Loss: 1.8076\n",
            ">>>> Epoch 18280:\t Train Loss: 2.5455\n",
            ">>>> Epoch 18281:\t Train Loss: 2.2691\n",
            ">>>> Epoch 18282:\t Train Loss: 2.2980\n",
            ">>>> Epoch 18283:\t Train Loss: 2.2027\n",
            ">>>> Epoch 18284:\t Train Loss: 2.4188\n",
            ">>>> Epoch 18285:\t Train Loss: 2.4084\n",
            ">>>> Epoch 18286:\t Train Loss: 2.2185\n",
            ">>>> Epoch 18287:\t Train Loss: 2.4031\n",
            ">>>> Epoch 18288:\t Train Loss: 2.1324\n",
            ">>>> Epoch 18289:\t Train Loss: 2.2780\n",
            ">>>> Epoch 18290:\t Train Loss: 2.1375\n",
            ">>>> Epoch 18291:\t Train Loss: 2.1630\n",
            ">>>> Epoch 18292:\t Train Loss: 2.0247\n",
            ">>>> Epoch 18293:\t Train Loss: 2.4395\n",
            ">>>> Epoch 18294:\t Train Loss: 2.5365\n",
            ">>>> Epoch 18295:\t Train Loss: 2.0990\n",
            ">>>> Epoch 18296:\t Train Loss: 2.1445\n",
            ">>>> Epoch 18297:\t Train Loss: 2.9053\n",
            ">>>> Epoch 18298:\t Train Loss: 2.0449\n",
            ">>>> Epoch 18299:\t Train Loss: 2.3189\n",
            ">>>> Epoch 18300:\t Train Loss: 2.0707\n",
            ">>>> Epoch 18301:\t Train Loss: 2.8551\n",
            ">>>> Epoch 18302:\t Train Loss: 2.6292\n",
            ">>>> Epoch 18303:\t Train Loss: 2.3257\n",
            ">>>> Epoch 18304:\t Train Loss: 2.4273\n",
            ">>>> Epoch 18305:\t Train Loss: 2.1110\n",
            ">>>> Epoch 18306:\t Train Loss: 2.5505\n",
            ">>>> Epoch 18307:\t Train Loss: 2.2434\n",
            ">>>> Epoch 18308:\t Train Loss: 2.2981\n",
            ">>>> Epoch 18309:\t Train Loss: 2.4596\n",
            ">>>> Epoch 18310:\t Train Loss: 2.5781\n",
            ">>>> Epoch 18311:\t Train Loss: 2.0107\n",
            ">>>> Epoch 18312:\t Train Loss: 1.9250\n",
            ">>>> Epoch 18313:\t Train Loss: 2.2403\n",
            ">>>> Epoch 18314:\t Train Loss: 2.7631\n",
            ">>>> Epoch 18315:\t Train Loss: 2.2594\n",
            ">>>> Epoch 18316:\t Train Loss: 2.3526\n",
            ">>>> Epoch 18317:\t Train Loss: 2.5823\n",
            ">>>> Epoch 18318:\t Train Loss: 2.3242\n",
            ">>>> Epoch 18319:\t Train Loss: 2.2603\n",
            ">>>> Epoch 18320:\t Train Loss: 2.1687\n",
            ">>>> Epoch 18321:\t Train Loss: 1.9507\n",
            ">>>> Epoch 18322:\t Train Loss: 2.2473\n",
            ">>>> Epoch 18323:\t Train Loss: 2.9063\n",
            ">>>> Epoch 18324:\t Train Loss: 2.1241\n",
            ">>>> Epoch 18325:\t Train Loss: 2.0388\n",
            ">>>> Epoch 18326:\t Train Loss: 1.9532\n",
            ">>>> Epoch 18327:\t Train Loss: 2.6198\n",
            ">>>> Epoch 18328:\t Train Loss: 2.2603\n",
            ">>>> Epoch 18329:\t Train Loss: 2.3747\n",
            ">>>> Epoch 18330:\t Train Loss: 2.5422\n",
            ">>>> Epoch 18331:\t Train Loss: 2.2977\n",
            ">>>> Epoch 18332:\t Train Loss: 2.0398\n",
            ">>>> Epoch 18333:\t Train Loss: 2.0386\n",
            ">>>> Epoch 18334:\t Train Loss: 2.0637\n",
            ">>>> Epoch 18335:\t Train Loss: 2.2095\n",
            ">>>> Epoch 18336:\t Train Loss: 2.1936\n",
            ">>>> Epoch 18337:\t Train Loss: 2.2846\n",
            ">>>> Epoch 18338:\t Train Loss: 2.1189\n",
            ">>>> Epoch 18339:\t Train Loss: 1.9854\n",
            ">>>> Epoch 18340:\t Train Loss: 2.1895\n",
            ">>>> Epoch 18341:\t Train Loss: 2.3005\n",
            ">>>> Epoch 18342:\t Train Loss: 2.0098\n",
            ">>>> Epoch 18343:\t Train Loss: 2.2837\n",
            ">>>> Epoch 18344:\t Train Loss: 2.2278\n",
            ">>>> Epoch 18345:\t Train Loss: 2.2970\n",
            ">>>> Epoch 18346:\t Train Loss: 1.9334\n",
            ">>>> Epoch 18347:\t Train Loss: 2.0032\n",
            ">>>> Epoch 18348:\t Train Loss: 2.2890\n",
            ">>>> Epoch 18349:\t Train Loss: 2.2077\n",
            ">>>> Epoch 18350:\t Train Loss: 2.0644\n",
            ">>>> Epoch 18351:\t Train Loss: 2.0848\n",
            ">>>> Epoch 18352:\t Train Loss: 2.2047\n",
            ">>>> Epoch 18353:\t Train Loss: 1.9732\n",
            ">>>> Epoch 18354:\t Train Loss: 2.5913\n",
            ">>>> Epoch 18355:\t Train Loss: 2.1493\n",
            ">>>> Epoch 18356:\t Train Loss: 2.1681\n",
            ">>>> Epoch 18357:\t Train Loss: 2.6944\n",
            ">>>> Epoch 18358:\t Train Loss: 2.3681\n",
            ">>>> Epoch 18359:\t Train Loss: 1.9411\n",
            ">>>> Epoch 18360:\t Train Loss: 2.3525\n",
            ">>>> Epoch 18361:\t Train Loss: 2.1514\n",
            ">>>> Epoch 18362:\t Train Loss: 2.1348\n",
            ">>>> Epoch 18363:\t Train Loss: 2.5340\n",
            ">>>> Epoch 18364:\t Train Loss: 2.4709\n",
            ">>>> Epoch 18365:\t Train Loss: 2.2152\n",
            ">>>> Epoch 18366:\t Train Loss: 2.3231\n",
            ">>>> Epoch 18367:\t Train Loss: 2.5752\n",
            ">>>> Epoch 18368:\t Train Loss: 2.3524\n",
            ">>>> Epoch 18369:\t Train Loss: 2.2687\n",
            ">>>> Epoch 18370:\t Train Loss: 2.2796\n",
            ">>>> Epoch 18371:\t Train Loss: 2.3465\n",
            ">>>> Epoch 18372:\t Train Loss: 2.2645\n",
            ">>>> Epoch 18373:\t Train Loss: 2.4522\n",
            ">>>> Epoch 18374:\t Train Loss: 2.2482\n",
            ">>>> Epoch 18375:\t Train Loss: 2.1979\n",
            ">>>> Epoch 18376:\t Train Loss: 2.5221\n",
            ">>>> Epoch 18377:\t Train Loss: 2.2399\n",
            ">>>> Epoch 18378:\t Train Loss: 2.2561\n",
            ">>>> Epoch 18379:\t Train Loss: 2.6696\n",
            ">>>> Epoch 18380:\t Train Loss: 2.4719\n",
            ">>>> Epoch 18381:\t Train Loss: 2.1236\n",
            ">>>> Epoch 18382:\t Train Loss: 2.0865\n",
            ">>>> Epoch 18383:\t Train Loss: 2.2599\n",
            ">>>> Epoch 18384:\t Train Loss: 2.1474\n",
            ">>>> Epoch 18385:\t Train Loss: 2.1836\n",
            ">>>> Epoch 18386:\t Train Loss: 2.0462\n",
            ">>>> Epoch 18387:\t Train Loss: 1.8714\n",
            ">>>> Epoch 18388:\t Train Loss: 2.0617\n",
            ">>>> Epoch 18389:\t Train Loss: 2.3933\n",
            ">>>> Epoch 18390:\t Train Loss: 1.9931\n",
            ">>>> Epoch 18391:\t Train Loss: 2.1939\n",
            ">>>> Epoch 18392:\t Train Loss: 2.2800\n",
            ">>>> Epoch 18393:\t Train Loss: 2.2044\n",
            ">>>> Epoch 18394:\t Train Loss: 2.1428\n",
            ">>>> Epoch 18395:\t Train Loss: 2.6069\n",
            ">>>> Epoch 18396:\t Train Loss: 2.3099\n",
            ">>>> Epoch 18397:\t Train Loss: 2.0872\n",
            ">>>> Epoch 18398:\t Train Loss: 2.2193\n",
            ">>>> Epoch 18399:\t Train Loss: 2.4445\n",
            ">>>> Epoch 18400:\t Train Loss: 2.2997\n",
            ">>>> Epoch 18401:\t Train Loss: 2.3213\n",
            ">>>> Epoch 18402:\t Train Loss: 1.8476\n",
            ">>>> Epoch 18403:\t Train Loss: 2.1509\n",
            ">>>> Epoch 18404:\t Train Loss: 2.1879\n",
            ">>>> Epoch 18405:\t Train Loss: 2.1336\n",
            ">>>> Epoch 18406:\t Train Loss: 2.1287\n",
            ">>>> Epoch 18407:\t Train Loss: 2.1495\n",
            ">>>> Epoch 18408:\t Train Loss: 2.0592\n",
            ">>>> Epoch 18409:\t Train Loss: 1.9589\n",
            ">>>> Epoch 18410:\t Train Loss: 1.9714\n",
            ">>>> Epoch 18411:\t Train Loss: 2.2605\n",
            ">>>> Epoch 18412:\t Train Loss: 2.0350\n",
            ">>>> Epoch 18413:\t Train Loss: 2.4341\n",
            ">>>> Epoch 18414:\t Train Loss: 2.2616\n",
            ">>>> Epoch 18415:\t Train Loss: 2.1461\n",
            ">>>> Epoch 18416:\t Train Loss: 2.3559\n",
            ">>>> Epoch 18417:\t Train Loss: 2.4200\n",
            ">>>> Epoch 18418:\t Train Loss: 2.1604\n",
            ">>>> Epoch 18419:\t Train Loss: 1.7336\n",
            ">>>> Epoch 18420:\t Train Loss: 1.9819\n",
            ">>>> Epoch 18421:\t Train Loss: 2.4540\n",
            ">>>> Epoch 18422:\t Train Loss: 2.2595\n",
            ">>>> Epoch 18423:\t Train Loss: 2.0451\n",
            ">>>> Epoch 18424:\t Train Loss: 2.5562\n",
            ">>>> Epoch 18425:\t Train Loss: 2.6478\n",
            ">>>> Epoch 18426:\t Train Loss: 2.2266\n",
            ">>>> Epoch 18427:\t Train Loss: 1.9864\n",
            ">>>> Epoch 18428:\t Train Loss: 2.2309\n",
            ">>>> Epoch 18429:\t Train Loss: 2.5258\n",
            ">>>> Epoch 18430:\t Train Loss: 1.7770\n",
            ">>>> Epoch 18431:\t Train Loss: 2.0714\n",
            ">>>> Epoch 18432:\t Train Loss: 2.2220\n",
            ">>>> Epoch 18433:\t Train Loss: 2.1466\n",
            ">>>> Epoch 18434:\t Train Loss: 2.3394\n",
            ">>>> Epoch 18435:\t Train Loss: 1.8558\n",
            ">>>> Epoch 18436:\t Train Loss: 2.0927\n",
            ">>>> Epoch 18437:\t Train Loss: 2.1220\n",
            ">>>> Epoch 18438:\t Train Loss: 2.1777\n",
            ">>>> Epoch 18439:\t Train Loss: 2.0950\n",
            ">>>> Epoch 18440:\t Train Loss: 2.0767\n",
            ">>>> Epoch 18441:\t Train Loss: 2.1768\n",
            ">>>> Epoch 18442:\t Train Loss: 2.1744\n",
            ">>>> Epoch 18443:\t Train Loss: 2.0137\n",
            ">>>> Epoch 18444:\t Train Loss: 2.0417\n",
            ">>>> Epoch 18445:\t Train Loss: 2.2332\n",
            ">>>> Epoch 18446:\t Train Loss: 2.2025\n",
            ">>>> Epoch 18447:\t Train Loss: 2.1353\n",
            ">>>> Epoch 18448:\t Train Loss: 2.3688\n",
            ">>>> Epoch 18449:\t Train Loss: 2.2916\n",
            ">>>> Epoch 18450:\t Train Loss: 2.4922\n",
            ">>>> Epoch 18451:\t Train Loss: 2.0070\n",
            ">>>> Epoch 18452:\t Train Loss: 2.0805\n",
            ">>>> Epoch 18453:\t Train Loss: 2.1800\n",
            ">>>> Epoch 18454:\t Train Loss: 2.2000\n",
            ">>>> Epoch 18455:\t Train Loss: 2.4789\n",
            ">>>> Epoch 18456:\t Train Loss: 2.0790\n",
            ">>>> Epoch 18457:\t Train Loss: 2.0840\n",
            ">>>> Epoch 18458:\t Train Loss: 2.4426\n",
            ">>>> Epoch 18459:\t Train Loss: 2.2307\n",
            ">>>> Epoch 18460:\t Train Loss: 2.3216\n",
            ">>>> Epoch 18461:\t Train Loss: 2.7244\n",
            ">>>> Epoch 18462:\t Train Loss: 2.3147\n",
            ">>>> Epoch 18463:\t Train Loss: 2.1904\n",
            ">>>> Epoch 18464:\t Train Loss: 2.2195\n",
            ">>>> Epoch 18465:\t Train Loss: 2.2113\n",
            ">>>> Epoch 18466:\t Train Loss: 2.0192\n",
            ">>>> Epoch 18467:\t Train Loss: 2.0266\n",
            ">>>> Epoch 18468:\t Train Loss: 2.1819\n",
            ">>>> Epoch 18469:\t Train Loss: 2.4210\n",
            ">>>> Epoch 18470:\t Train Loss: 2.5967\n",
            ">>>> Epoch 18471:\t Train Loss: 2.1948\n",
            ">>>> Epoch 18472:\t Train Loss: 2.5258\n",
            ">>>> Epoch 18473:\t Train Loss: 2.0924\n",
            ">>>> Epoch 18474:\t Train Loss: 2.2414\n",
            ">>>> Epoch 18475:\t Train Loss: 1.9710\n",
            ">>>> Epoch 18476:\t Train Loss: 2.1027\n",
            ">>>> Epoch 18477:\t Train Loss: 2.1203\n",
            ">>>> Epoch 18478:\t Train Loss: 1.9842\n",
            ">>>> Epoch 18479:\t Train Loss: 2.1288\n",
            ">>>> Epoch 18480:\t Train Loss: 2.5051\n",
            ">>>> Epoch 18481:\t Train Loss: 2.2244\n",
            ">>>> Epoch 18482:\t Train Loss: 2.0609\n",
            ">>>> Epoch 18483:\t Train Loss: 2.3032\n",
            ">>>> Epoch 18484:\t Train Loss: 1.7149\n",
            ">>>> Epoch 18485:\t Train Loss: 2.1422\n",
            ">>>> Epoch 18486:\t Train Loss: 2.2898\n",
            ">>>> Epoch 18487:\t Train Loss: 2.1830\n",
            ">>>> Epoch 18488:\t Train Loss: 2.1351\n",
            ">>>> Epoch 18489:\t Train Loss: 2.2122\n",
            ">>>> Epoch 18490:\t Train Loss: 2.4584\n",
            ">>>> Epoch 18491:\t Train Loss: 2.0066\n",
            ">>>> Epoch 18492:\t Train Loss: 2.3076\n",
            ">>>> Epoch 18493:\t Train Loss: 2.2868\n",
            ">>>> Epoch 18494:\t Train Loss: 2.4793\n",
            ">>>> Epoch 18495:\t Train Loss: 2.5924\n",
            ">>>> Epoch 18496:\t Train Loss: 2.1946\n",
            ">>>> Epoch 18497:\t Train Loss: 2.2039\n",
            ">>>> Epoch 18498:\t Train Loss: 1.9026\n",
            ">>>> Epoch 18499:\t Train Loss: 2.4176\n",
            ">>>> Epoch 18500:\t Train Loss: 2.3733\n",
            ">>>> Epoch 18501:\t Train Loss: 2.2522\n",
            ">>>> Epoch 18502:\t Train Loss: 2.4076\n",
            ">>>> Epoch 18503:\t Train Loss: 2.2547\n",
            ">>>> Epoch 18504:\t Train Loss: 2.1478\n",
            ">>>> Epoch 18505:\t Train Loss: 2.2545\n",
            ">>>> Epoch 18506:\t Train Loss: 2.2184\n",
            ">>>> Epoch 18507:\t Train Loss: 2.2472\n",
            ">>>> Epoch 18508:\t Train Loss: 2.2998\n",
            ">>>> Epoch 18509:\t Train Loss: 2.5978\n",
            ">>>> Epoch 18510:\t Train Loss: 2.2529\n",
            ">>>> Epoch 18511:\t Train Loss: 2.1051\n",
            ">>>> Epoch 18512:\t Train Loss: 2.2499\n",
            ">>>> Epoch 18513:\t Train Loss: 2.0932\n",
            ">>>> Epoch 18514:\t Train Loss: 2.2830\n",
            ">>>> Epoch 18515:\t Train Loss: 2.4876\n",
            ">>>> Epoch 18516:\t Train Loss: 2.8069\n",
            ">>>> Epoch 18517:\t Train Loss: 2.1753\n",
            ">>>> Epoch 18518:\t Train Loss: 2.2881\n",
            ">>>> Epoch 18519:\t Train Loss: 2.2829\n",
            ">>>> Epoch 18520:\t Train Loss: 2.1825\n",
            ">>>> Epoch 18521:\t Train Loss: 2.4157\n",
            ">>>> Epoch 18522:\t Train Loss: 1.9783\n",
            ">>>> Epoch 18523:\t Train Loss: 2.0903\n",
            ">>>> Epoch 18524:\t Train Loss: 2.3063\n",
            ">>>> Epoch 18525:\t Train Loss: 1.9345\n",
            ">>>> Epoch 18526:\t Train Loss: 2.2649\n",
            ">>>> Epoch 18527:\t Train Loss: 2.3762\n",
            ">>>> Epoch 18528:\t Train Loss: 2.1106\n",
            ">>>> Epoch 18529:\t Train Loss: 2.5515\n",
            ">>>> Epoch 18530:\t Train Loss: 2.0376\n",
            ">>>> Epoch 18531:\t Train Loss: 1.9587\n",
            ">>>> Epoch 18532:\t Train Loss: 2.0758\n",
            ">>>> Epoch 18533:\t Train Loss: 2.3191\n",
            ">>>> Epoch 18534:\t Train Loss: 2.5691\n",
            ">>>> Epoch 18535:\t Train Loss: 2.1218\n",
            ">>>> Epoch 18536:\t Train Loss: 1.9028\n",
            ">>>> Epoch 18537:\t Train Loss: 2.1629\n",
            ">>>> Epoch 18538:\t Train Loss: 2.1585\n",
            ">>>> Epoch 18539:\t Train Loss: 2.0900\n",
            ">>>> Epoch 18540:\t Train Loss: 2.5283\n",
            ">>>> Epoch 18541:\t Train Loss: 2.0614\n",
            ">>>> Epoch 18542:\t Train Loss: 2.1037\n",
            ">>>> Epoch 18543:\t Train Loss: 2.2774\n",
            ">>>> Epoch 18544:\t Train Loss: 1.8082\n",
            ">>>> Epoch 18545:\t Train Loss: 2.1215\n",
            ">>>> Epoch 18546:\t Train Loss: 2.0126\n",
            ">>>> Epoch 18547:\t Train Loss: 2.3569\n",
            ">>>> Epoch 18548:\t Train Loss: 2.2988\n",
            ">>>> Epoch 18549:\t Train Loss: 2.1807\n",
            ">>>> Epoch 18550:\t Train Loss: 1.8392\n",
            ">>>> Epoch 18551:\t Train Loss: 2.2497\n",
            ">>>> Epoch 18552:\t Train Loss: 2.5605\n",
            ">>>> Epoch 18553:\t Train Loss: 1.9736\n",
            ">>>> Epoch 18554:\t Train Loss: 2.1063\n",
            ">>>> Epoch 18555:\t Train Loss: 2.1283\n",
            ">>>> Epoch 18556:\t Train Loss: 2.3848\n",
            ">>>> Epoch 18557:\t Train Loss: 2.1130\n",
            ">>>> Epoch 18558:\t Train Loss: 2.4042\n",
            ">>>> Epoch 18559:\t Train Loss: 2.5476\n",
            ">>>> Epoch 18560:\t Train Loss: 1.8934\n",
            ">>>> Epoch 18561:\t Train Loss: 2.3265\n",
            ">>>> Epoch 18562:\t Train Loss: 2.3811\n",
            ">>>> Epoch 18563:\t Train Loss: 2.1169\n",
            ">>>> Epoch 18564:\t Train Loss: 1.9788\n",
            ">>>> Epoch 18565:\t Train Loss: 2.0596\n",
            ">>>> Epoch 18566:\t Train Loss: 2.0403\n",
            ">>>> Epoch 18567:\t Train Loss: 2.0927\n",
            ">>>> Epoch 18568:\t Train Loss: 2.1890\n",
            ">>>> Epoch 18569:\t Train Loss: 2.1301\n",
            ">>>> Epoch 18570:\t Train Loss: 2.6246\n",
            ">>>> Epoch 18571:\t Train Loss: 2.2982\n",
            ">>>> Epoch 18572:\t Train Loss: 2.1750\n",
            ">>>> Epoch 18573:\t Train Loss: 1.8301\n",
            ">>>> Epoch 18574:\t Train Loss: 2.3097\n",
            ">>>> Epoch 18575:\t Train Loss: 2.1488\n",
            ">>>> Epoch 18576:\t Train Loss: 2.1148\n",
            ">>>> Epoch 18577:\t Train Loss: 2.2330\n",
            ">>>> Epoch 18578:\t Train Loss: 2.3568\n",
            ">>>> Epoch 18579:\t Train Loss: 2.5399\n",
            ">>>> Epoch 18580:\t Train Loss: 2.2578\n",
            ">>>> Epoch 18581:\t Train Loss: 1.7578\n",
            ">>>> Epoch 18582:\t Train Loss: 2.1409\n",
            ">>>> Epoch 18583:\t Train Loss: 1.8646\n",
            ">>>> Epoch 18584:\t Train Loss: 2.1603\n",
            ">>>> Epoch 18585:\t Train Loss: 2.1846\n",
            ">>>> Epoch 18586:\t Train Loss: 2.3161\n",
            ">>>> Epoch 18587:\t Train Loss: 2.3557\n",
            ">>>> Epoch 18588:\t Train Loss: 2.1408\n",
            ">>>> Epoch 18589:\t Train Loss: 2.1814\n",
            ">>>> Epoch 18590:\t Train Loss: 2.3108\n",
            ">>>> Epoch 18591:\t Train Loss: 2.4175\n",
            ">>>> Epoch 18592:\t Train Loss: 2.0023\n",
            ">>>> Epoch 18593:\t Train Loss: 2.0506\n",
            ">>>> Epoch 18594:\t Train Loss: 2.3266\n",
            ">>>> Epoch 18595:\t Train Loss: 2.0960\n",
            ">>>> Epoch 18596:\t Train Loss: 2.1359\n",
            ">>>> Epoch 18597:\t Train Loss: 2.2500\n",
            ">>>> Epoch 18598:\t Train Loss: 2.1973\n",
            ">>>> Epoch 18599:\t Train Loss: 2.1507\n",
            ">>>> Epoch 18600:\t Train Loss: 2.0721\n",
            ">>>> Epoch 18601:\t Train Loss: 2.0837\n",
            ">>>> Epoch 18602:\t Train Loss: 2.4658\n",
            ">>>> Epoch 18603:\t Train Loss: 2.2184\n",
            ">>>> Epoch 18604:\t Train Loss: 2.3131\n",
            ">>>> Epoch 18605:\t Train Loss: 2.4786\n",
            ">>>> Epoch 18606:\t Train Loss: 2.2851\n",
            ">>>> Epoch 18607:\t Train Loss: 2.7075\n",
            ">>>> Epoch 18608:\t Train Loss: 2.0176\n",
            ">>>> Epoch 18609:\t Train Loss: 2.2260\n",
            ">>>> Epoch 18610:\t Train Loss: 2.7277\n",
            ">>>> Epoch 18611:\t Train Loss: 2.0214\n",
            ">>>> Epoch 18612:\t Train Loss: 2.6598\n",
            ">>>> Epoch 18613:\t Train Loss: 2.0631\n",
            ">>>> Epoch 18614:\t Train Loss: 1.9461\n",
            ">>>> Epoch 18615:\t Train Loss: 2.3567\n",
            ">>>> Epoch 18616:\t Train Loss: 2.3374\n",
            ">>>> Epoch 18617:\t Train Loss: 2.2608\n",
            ">>>> Epoch 18618:\t Train Loss: 2.6160\n",
            ">>>> Epoch 18619:\t Train Loss: 2.2956\n",
            ">>>> Epoch 18620:\t Train Loss: 2.2207\n",
            ">>>> Epoch 18621:\t Train Loss: 2.4294\n",
            ">>>> Epoch 18622:\t Train Loss: 2.5909\n",
            ">>>> Epoch 18623:\t Train Loss: 2.1755\n",
            ">>>> Epoch 18624:\t Train Loss: 2.2517\n",
            ">>>> Epoch 18625:\t Train Loss: 1.8078\n",
            ">>>> Epoch 18626:\t Train Loss: 2.1655\n",
            ">>>> Epoch 18627:\t Train Loss: 2.0366\n",
            ">>>> Epoch 18628:\t Train Loss: 2.4546\n",
            ">>>> Epoch 18629:\t Train Loss: 2.0595\n",
            ">>>> Epoch 18630:\t Train Loss: 2.3862\n",
            ">>>> Epoch 18631:\t Train Loss: 2.3814\n",
            ">>>> Epoch 18632:\t Train Loss: 1.9498\n",
            ">>>> Epoch 18633:\t Train Loss: 1.9034\n",
            ">>>> Epoch 18634:\t Train Loss: 2.3749\n",
            ">>>> Epoch 18635:\t Train Loss: 2.2210\n",
            ">>>> Epoch 18636:\t Train Loss: 2.3474\n",
            ">>>> Epoch 18637:\t Train Loss: 2.3367\n",
            ">>>> Epoch 18638:\t Train Loss: 2.2779\n",
            ">>>> Epoch 18639:\t Train Loss: 2.2293\n",
            ">>>> Epoch 18640:\t Train Loss: 2.4824\n",
            ">>>> Epoch 18641:\t Train Loss: 1.7394\n",
            ">>>> Epoch 18642:\t Train Loss: 1.8432\n",
            ">>>> Epoch 18643:\t Train Loss: 2.4833\n",
            ">>>> Epoch 18644:\t Train Loss: 2.0259\n",
            ">>>> Epoch 18645:\t Train Loss: 2.4414\n",
            ">>>> Epoch 18646:\t Train Loss: 1.9550\n",
            ">>>> Epoch 18647:\t Train Loss: 1.9673\n",
            ">>>> Epoch 18648:\t Train Loss: 2.2873\n",
            ">>>> Epoch 18649:\t Train Loss: 2.0172\n",
            ">>>> Epoch 18650:\t Train Loss: 2.3470\n",
            ">>>> Epoch 18651:\t Train Loss: 2.2997\n",
            ">>>> Epoch 18652:\t Train Loss: 1.9766\n",
            ">>>> Epoch 18653:\t Train Loss: 2.4117\n",
            ">>>> Epoch 18654:\t Train Loss: 2.3837\n",
            ">>>> Epoch 18655:\t Train Loss: 2.5765\n",
            ">>>> Epoch 18656:\t Train Loss: 2.8384\n",
            ">>>> Epoch 18657:\t Train Loss: 2.3861\n",
            ">>>> Epoch 18658:\t Train Loss: 2.3447\n",
            ">>>> Epoch 18659:\t Train Loss: 2.3890\n",
            ">>>> Epoch 18660:\t Train Loss: 2.0250\n",
            ">>>> Epoch 18661:\t Train Loss: 2.2808\n",
            ">>>> Epoch 18662:\t Train Loss: 2.2913\n",
            ">>>> Epoch 18663:\t Train Loss: 1.9893\n",
            ">>>> Epoch 18664:\t Train Loss: 2.3030\n",
            ">>>> Epoch 18665:\t Train Loss: 1.7398\n",
            ">>>> Epoch 18666:\t Train Loss: 2.1670\n",
            ">>>> Epoch 18667:\t Train Loss: 2.0001\n",
            ">>>> Epoch 18668:\t Train Loss: 1.9260\n",
            ">>>> Epoch 18669:\t Train Loss: 2.3855\n",
            ">>>> Epoch 18670:\t Train Loss: 2.2673\n",
            ">>>> Epoch 18671:\t Train Loss: 2.1999\n",
            ">>>> Epoch 18672:\t Train Loss: 2.1353\n",
            ">>>> Epoch 18673:\t Train Loss: 2.1998\n",
            ">>>> Epoch 18674:\t Train Loss: 2.1342\n",
            ">>>> Epoch 18675:\t Train Loss: 2.3894\n",
            ">>>> Epoch 18676:\t Train Loss: 2.2736\n",
            ">>>> Epoch 18677:\t Train Loss: 2.2361\n",
            ">>>> Epoch 18678:\t Train Loss: 2.1121\n",
            ">>>> Epoch 18679:\t Train Loss: 2.1554\n",
            ">>>> Epoch 18680:\t Train Loss: 2.2340\n",
            ">>>> Epoch 18681:\t Train Loss: 1.9991\n",
            ">>>> Epoch 18682:\t Train Loss: 2.4188\n",
            ">>>> Epoch 18683:\t Train Loss: 2.2461\n",
            ">>>> Epoch 18684:\t Train Loss: 2.2847\n",
            ">>>> Epoch 18685:\t Train Loss: 2.2018\n",
            ">>>> Epoch 18686:\t Train Loss: 2.2209\n",
            ">>>> Epoch 18687:\t Train Loss: 2.2368\n",
            ">>>> Epoch 18688:\t Train Loss: 2.1285\n",
            ">>>> Epoch 18689:\t Train Loss: 2.3036\n",
            ">>>> Epoch 18690:\t Train Loss: 2.0754\n",
            ">>>> Epoch 18691:\t Train Loss: 2.2005\n",
            ">>>> Epoch 18692:\t Train Loss: 2.0451\n",
            ">>>> Epoch 18693:\t Train Loss: 2.3180\n",
            ">>>> Epoch 18694:\t Train Loss: 2.3581\n",
            ">>>> Epoch 18695:\t Train Loss: 1.9644\n",
            ">>>> Epoch 18696:\t Train Loss: 2.4347\n",
            ">>>> Epoch 18697:\t Train Loss: 2.2526\n",
            ">>>> Epoch 18698:\t Train Loss: 2.0767\n",
            ">>>> Epoch 18699:\t Train Loss: 2.4429\n",
            ">>>> Epoch 18700:\t Train Loss: 1.9874\n",
            ">>>> Epoch 18701:\t Train Loss: 2.4570\n",
            ">>>> Epoch 18702:\t Train Loss: 2.4983\n",
            ">>>> Epoch 18703:\t Train Loss: 1.8630\n",
            ">>>> Epoch 18704:\t Train Loss: 2.0975\n",
            ">>>> Epoch 18705:\t Train Loss: 2.1893\n",
            ">>>> Epoch 18706:\t Train Loss: 2.2803\n",
            ">>>> Epoch 18707:\t Train Loss: 2.0442\n",
            ">>>> Epoch 18708:\t Train Loss: 1.7719\n",
            ">>>> Epoch 18709:\t Train Loss: 2.3694\n",
            ">>>> Epoch 18710:\t Train Loss: 2.3059\n",
            ">>>> Epoch 18711:\t Train Loss: 2.1214\n",
            ">>>> Epoch 18712:\t Train Loss: 2.2353\n",
            ">>>> Epoch 18713:\t Train Loss: 2.2460\n",
            ">>>> Epoch 18714:\t Train Loss: 2.3245\n",
            ">>>> Epoch 18715:\t Train Loss: 2.3443\n",
            ">>>> Epoch 18716:\t Train Loss: 1.7478\n",
            ">>>> Epoch 18717:\t Train Loss: 2.5895\n",
            ">>>> Epoch 18718:\t Train Loss: 2.0959\n",
            ">>>> Epoch 18719:\t Train Loss: 2.3231\n",
            ">>>> Epoch 18720:\t Train Loss: 2.3143\n",
            ">>>> Epoch 18721:\t Train Loss: 1.8373\n",
            ">>>> Epoch 18722:\t Train Loss: 2.7144\n",
            ">>>> Epoch 18723:\t Train Loss: 2.1956\n",
            ">>>> Epoch 18724:\t Train Loss: 1.8300\n",
            ">>>> Epoch 18725:\t Train Loss: 2.3886\n",
            ">>>> Epoch 18726:\t Train Loss: 2.3585\n",
            ">>>> Epoch 18727:\t Train Loss: 2.1431\n",
            ">>>> Epoch 18728:\t Train Loss: 2.3699\n",
            ">>>> Epoch 18729:\t Train Loss: 2.0676\n",
            ">>>> Epoch 18730:\t Train Loss: 2.3446\n",
            ">>>> Epoch 18731:\t Train Loss: 2.1984\n",
            ">>>> Epoch 18732:\t Train Loss: 2.1526\n",
            ">>>> Epoch 18733:\t Train Loss: 1.9988\n",
            ">>>> Epoch 18734:\t Train Loss: 1.9488\n",
            ">>>> Epoch 18735:\t Train Loss: 2.2100\n",
            ">>>> Epoch 18736:\t Train Loss: 2.3560\n",
            ">>>> Epoch 18737:\t Train Loss: 2.2667\n",
            ">>>> Epoch 18738:\t Train Loss: 2.1531\n",
            ">>>> Epoch 18739:\t Train Loss: 1.7194\n",
            ">>>> Epoch 18740:\t Train Loss: 2.2001\n",
            ">>>> Epoch 18741:\t Train Loss: 2.1226\n",
            ">>>> Epoch 18742:\t Train Loss: 2.0937\n",
            ">>>> Epoch 18743:\t Train Loss: 2.4539\n",
            ">>>> Epoch 18744:\t Train Loss: 1.7610\n",
            ">>>> Epoch 18745:\t Train Loss: 2.0752\n",
            ">>>> Epoch 18746:\t Train Loss: 2.2810\n",
            ">>>> Epoch 18747:\t Train Loss: 2.3256\n",
            ">>>> Epoch 18748:\t Train Loss: 2.2623\n",
            ">>>> Epoch 18749:\t Train Loss: 2.0601\n",
            ">>>> Epoch 18750:\t Train Loss: 2.2498\n",
            ">>>> Epoch 18751:\t Train Loss: 2.1106\n",
            ">>>> Epoch 18752:\t Train Loss: 2.2701\n",
            ">>>> Epoch 18753:\t Train Loss: 2.4577\n",
            ">>>> Epoch 18754:\t Train Loss: 2.2771\n",
            ">>>> Epoch 18755:\t Train Loss: 2.1355\n",
            ">>>> Epoch 18756:\t Train Loss: 2.4629\n",
            ">>>> Epoch 18757:\t Train Loss: 2.4394\n",
            ">>>> Epoch 18758:\t Train Loss: 2.4544\n",
            ">>>> Epoch 18759:\t Train Loss: 2.0805\n",
            ">>>> Epoch 18760:\t Train Loss: 2.7891\n",
            ">>>> Epoch 18761:\t Train Loss: 2.3775\n",
            ">>>> Epoch 18762:\t Train Loss: 2.3743\n",
            ">>>> Epoch 18763:\t Train Loss: 2.3080\n",
            ">>>> Epoch 18764:\t Train Loss: 2.1196\n",
            ">>>> Epoch 18765:\t Train Loss: 2.4168\n",
            ">>>> Epoch 18766:\t Train Loss: 2.0542\n",
            ">>>> Epoch 18767:\t Train Loss: 2.2731\n",
            ">>>> Epoch 18768:\t Train Loss: 2.3214\n",
            ">>>> Epoch 18769:\t Train Loss: 2.7110\n",
            ">>>> Epoch 18770:\t Train Loss: 2.0714\n",
            ">>>> Epoch 18771:\t Train Loss: 2.0074\n",
            ">>>> Epoch 18772:\t Train Loss: 2.1534\n",
            ">>>> Epoch 18773:\t Train Loss: 2.3115\n",
            ">>>> Epoch 18774:\t Train Loss: 2.1317\n",
            ">>>> Epoch 18775:\t Train Loss: 2.1722\n",
            ">>>> Epoch 18776:\t Train Loss: 1.9879\n",
            ">>>> Epoch 18777:\t Train Loss: 2.0952\n",
            ">>>> Epoch 18778:\t Train Loss: 2.0466\n",
            ">>>> Epoch 18779:\t Train Loss: 2.1149\n",
            ">>>> Epoch 18780:\t Train Loss: 2.2343\n",
            ">>>> Epoch 18781:\t Train Loss: 2.1552\n",
            ">>>> Epoch 18782:\t Train Loss: 2.2388\n",
            ">>>> Epoch 18783:\t Train Loss: 1.8271\n",
            ">>>> Epoch 18784:\t Train Loss: 2.3385\n",
            ">>>> Epoch 18785:\t Train Loss: 2.1003\n",
            ">>>> Epoch 18786:\t Train Loss: 2.3922\n",
            ">>>> Epoch 18787:\t Train Loss: 2.4619\n",
            ">>>> Epoch 18788:\t Train Loss: 2.2219\n",
            ">>>> Epoch 18789:\t Train Loss: 2.0857\n",
            ">>>> Epoch 18790:\t Train Loss: 2.1419\n",
            ">>>> Epoch 18791:\t Train Loss: 2.4497\n",
            ">>>> Epoch 18792:\t Train Loss: 2.1967\n",
            ">>>> Epoch 18793:\t Train Loss: 2.3659\n",
            ">>>> Epoch 18794:\t Train Loss: 2.0767\n",
            ">>>> Epoch 18795:\t Train Loss: 2.1113\n",
            ">>>> Epoch 18796:\t Train Loss: 2.0261\n",
            ">>>> Epoch 18797:\t Train Loss: 2.1136\n",
            ">>>> Epoch 18798:\t Train Loss: 2.3045\n",
            ">>>> Epoch 18799:\t Train Loss: 1.9503\n",
            ">>>> Epoch 18800:\t Train Loss: 2.1071\n",
            ">>>> Epoch 18801:\t Train Loss: 2.7863\n",
            ">>>> Epoch 18802:\t Train Loss: 2.0476\n",
            ">>>> Epoch 18803:\t Train Loss: 2.9371\n",
            ">>>> Epoch 18804:\t Train Loss: 2.3380\n",
            ">>>> Epoch 18805:\t Train Loss: 2.2174\n",
            ">>>> Epoch 18806:\t Train Loss: 1.7795\n",
            ">>>> Epoch 18807:\t Train Loss: 2.3441\n",
            ">>>> Epoch 18808:\t Train Loss: 2.2100\n",
            ">>>> Epoch 18809:\t Train Loss: 2.3169\n",
            ">>>> Epoch 18810:\t Train Loss: 2.0676\n",
            ">>>> Epoch 18811:\t Train Loss: 2.4366\n",
            ">>>> Epoch 18812:\t Train Loss: 2.1375\n",
            ">>>> Epoch 18813:\t Train Loss: 2.1433\n",
            ">>>> Epoch 18814:\t Train Loss: 2.2416\n",
            ">>>> Epoch 18815:\t Train Loss: 1.9765\n",
            ">>>> Epoch 18816:\t Train Loss: 2.2599\n",
            ">>>> Epoch 18817:\t Train Loss: 2.4288\n",
            ">>>> Epoch 18818:\t Train Loss: 2.1150\n",
            ">>>> Epoch 18819:\t Train Loss: 2.0805\n",
            ">>>> Epoch 18820:\t Train Loss: 2.0855\n",
            ">>>> Epoch 18821:\t Train Loss: 2.3552\n",
            ">>>> Epoch 18822:\t Train Loss: 2.5465\n",
            ">>>> Epoch 18823:\t Train Loss: 2.2699\n",
            ">>>> Epoch 18824:\t Train Loss: 2.2687\n",
            ">>>> Epoch 18825:\t Train Loss: 2.3763\n",
            ">>>> Epoch 18826:\t Train Loss: 2.0088\n",
            ">>>> Epoch 18827:\t Train Loss: 2.1167\n",
            ">>>> Epoch 18828:\t Train Loss: 2.2054\n",
            ">>>> Epoch 18829:\t Train Loss: 2.3126\n",
            ">>>> Epoch 18830:\t Train Loss: 2.1820\n",
            ">>>> Epoch 18831:\t Train Loss: 2.0744\n",
            ">>>> Epoch 18832:\t Train Loss: 2.4343\n",
            ">>>> Epoch 18833:\t Train Loss: 2.0076\n",
            ">>>> Epoch 18834:\t Train Loss: 2.4150\n",
            ">>>> Epoch 18835:\t Train Loss: 2.4608\n",
            ">>>> Epoch 18836:\t Train Loss: 2.3915\n",
            ">>>> Epoch 18837:\t Train Loss: 2.2651\n",
            ">>>> Epoch 18838:\t Train Loss: 2.0434\n",
            ">>>> Epoch 18839:\t Train Loss: 2.2432\n",
            ">>>> Epoch 18840:\t Train Loss: 2.0874\n",
            ">>>> Epoch 18841:\t Train Loss: 2.1104\n",
            ">>>> Epoch 18842:\t Train Loss: 2.6747\n",
            ">>>> Epoch 18843:\t Train Loss: 2.0938\n",
            ">>>> Epoch 18844:\t Train Loss: 2.3301\n",
            ">>>> Epoch 18845:\t Train Loss: 2.4982\n",
            ">>>> Epoch 18846:\t Train Loss: 1.8752\n",
            ">>>> Epoch 18847:\t Train Loss: 1.9345\n",
            ">>>> Epoch 18848:\t Train Loss: 1.9851\n",
            ">>>> Epoch 18849:\t Train Loss: 2.5215\n",
            ">>>> Epoch 18850:\t Train Loss: 2.3187\n",
            ">>>> Epoch 18851:\t Train Loss: 2.2725\n",
            ">>>> Epoch 18852:\t Train Loss: 2.4001\n",
            ">>>> Epoch 18853:\t Train Loss: 2.3424\n",
            ">>>> Epoch 18854:\t Train Loss: 1.8377\n",
            ">>>> Epoch 18855:\t Train Loss: 2.6668\n",
            ">>>> Epoch 18856:\t Train Loss: 2.2369\n",
            ">>>> Epoch 18857:\t Train Loss: 2.0626\n",
            ">>>> Epoch 18858:\t Train Loss: 2.0907\n",
            ">>>> Epoch 18859:\t Train Loss: 2.3710\n",
            ">>>> Epoch 18860:\t Train Loss: 2.2182\n",
            ">>>> Epoch 18861:\t Train Loss: 2.3590\n",
            ">>>> Epoch 18862:\t Train Loss: 2.3102\n",
            ">>>> Epoch 18863:\t Train Loss: 2.1938\n",
            ">>>> Epoch 18864:\t Train Loss: 2.6022\n",
            ">>>> Epoch 18865:\t Train Loss: 1.9750\n",
            ">>>> Epoch 18866:\t Train Loss: 2.3800\n",
            ">>>> Epoch 18867:\t Train Loss: 2.3395\n",
            ">>>> Epoch 18868:\t Train Loss: 1.9978\n",
            ">>>> Epoch 18869:\t Train Loss: 2.3082\n",
            ">>>> Epoch 18870:\t Train Loss: 2.2502\n",
            ">>>> Epoch 18871:\t Train Loss: 2.1315\n",
            ">>>> Epoch 18872:\t Train Loss: 2.7242\n",
            ">>>> Epoch 18873:\t Train Loss: 2.2093\n",
            ">>>> Epoch 18874:\t Train Loss: 2.0337\n",
            ">>>> Epoch 18875:\t Train Loss: 2.3461\n",
            ">>>> Epoch 18876:\t Train Loss: 1.7170\n",
            ">>>> Epoch 18877:\t Train Loss: 2.2268\n",
            ">>>> Epoch 18878:\t Train Loss: 2.1820\n",
            ">>>> Epoch 18879:\t Train Loss: 2.4565\n",
            ">>>> Epoch 18880:\t Train Loss: 2.3752\n",
            ">>>> Epoch 18881:\t Train Loss: 2.3385\n",
            ">>>> Epoch 18882:\t Train Loss: 2.2693\n",
            ">>>> Epoch 18883:\t Train Loss: 2.0642\n",
            ">>>> Epoch 18884:\t Train Loss: 2.2184\n",
            ">>>> Epoch 18885:\t Train Loss: 2.2145\n",
            ">>>> Epoch 18886:\t Train Loss: 2.0956\n",
            ">>>> Epoch 18887:\t Train Loss: 2.4144\n",
            ">>>> Epoch 18888:\t Train Loss: 2.1359\n",
            ">>>> Epoch 18889:\t Train Loss: 2.0968\n",
            ">>>> Epoch 18890:\t Train Loss: 2.2159\n",
            ">>>> Epoch 18891:\t Train Loss: 2.0339\n",
            ">>>> Epoch 18892:\t Train Loss: 1.9950\n",
            ">>>> Epoch 18893:\t Train Loss: 2.0882\n",
            ">>>> Epoch 18894:\t Train Loss: 2.4006\n",
            ">>>> Epoch 18895:\t Train Loss: 2.4660\n",
            ">>>> Epoch 18896:\t Train Loss: 1.7548\n",
            ">>>> Epoch 18897:\t Train Loss: 2.0415\n",
            ">>>> Epoch 18898:\t Train Loss: 2.1741\n",
            ">>>> Epoch 18899:\t Train Loss: 2.1709\n",
            ">>>> Epoch 18900:\t Train Loss: 2.1029\n",
            ">>>> Epoch 18901:\t Train Loss: 2.2674\n",
            ">>>> Epoch 18902:\t Train Loss: 2.2553\n",
            ">>>> Epoch 18903:\t Train Loss: 2.2228\n",
            ">>>> Epoch 18904:\t Train Loss: 2.1264\n",
            ">>>> Epoch 18905:\t Train Loss: 2.0726\n",
            ">>>> Epoch 18906:\t Train Loss: 2.0571\n",
            ">>>> Epoch 18907:\t Train Loss: 1.9574\n",
            ">>>> Epoch 18908:\t Train Loss: 2.4998\n",
            ">>>> Epoch 18909:\t Train Loss: 2.2034\n",
            ">>>> Epoch 18910:\t Train Loss: 2.5339\n",
            ">>>> Epoch 18911:\t Train Loss: 1.9949\n",
            ">>>> Epoch 18912:\t Train Loss: 2.5805\n",
            ">>>> Epoch 18913:\t Train Loss: 2.3225\n",
            ">>>> Epoch 18914:\t Train Loss: 2.2638\n",
            ">>>> Epoch 18915:\t Train Loss: 2.4412\n",
            ">>>> Epoch 18916:\t Train Loss: 2.0646\n",
            ">>>> Epoch 18917:\t Train Loss: 2.2170\n",
            ">>>> Epoch 18918:\t Train Loss: 2.2496\n",
            ">>>> Epoch 18919:\t Train Loss: 2.1199\n",
            ">>>> Epoch 18920:\t Train Loss: 2.3463\n",
            ">>>> Epoch 18921:\t Train Loss: 2.4851\n",
            ">>>> Epoch 18922:\t Train Loss: 2.6099\n",
            ">>>> Epoch 18923:\t Train Loss: 2.4061\n",
            ">>>> Epoch 18924:\t Train Loss: 1.9395\n",
            ">>>> Epoch 18925:\t Train Loss: 2.7609\n",
            ">>>> Epoch 18926:\t Train Loss: 2.6647\n",
            ">>>> Epoch 18927:\t Train Loss: 2.4146\n",
            ">>>> Epoch 18928:\t Train Loss: 1.9611\n",
            ">>>> Epoch 18929:\t Train Loss: 2.2982\n",
            ">>>> Epoch 18930:\t Train Loss: 2.2457\n",
            ">>>> Epoch 18931:\t Train Loss: 2.2467\n",
            ">>>> Epoch 18932:\t Train Loss: 2.4701\n",
            ">>>> Epoch 18933:\t Train Loss: 2.3804\n",
            ">>>> Epoch 18934:\t Train Loss: 2.1835\n",
            ">>>> Epoch 18935:\t Train Loss: 2.4256\n",
            ">>>> Epoch 18936:\t Train Loss: 1.9317\n",
            ">>>> Epoch 18937:\t Train Loss: 2.4341\n",
            ">>>> Epoch 18938:\t Train Loss: 1.9144\n",
            ">>>> Epoch 18939:\t Train Loss: 2.3412\n",
            ">>>> Epoch 18940:\t Train Loss: 2.9290\n",
            ">>>> Epoch 18941:\t Train Loss: 2.0655\n",
            ">>>> Epoch 18942:\t Train Loss: 2.0877\n",
            ">>>> Epoch 18943:\t Train Loss: 2.2554\n",
            ">>>> Epoch 18944:\t Train Loss: 2.5498\n",
            ">>>> Epoch 18945:\t Train Loss: 2.1857\n",
            ">>>> Epoch 18946:\t Train Loss: 2.3942\n",
            ">>>> Epoch 18947:\t Train Loss: 2.3425\n",
            ">>>> Epoch 18948:\t Train Loss: 2.2061\n",
            ">>>> Epoch 18949:\t Train Loss: 2.4297\n",
            ">>>> Epoch 18950:\t Train Loss: 2.6050\n",
            ">>>> Epoch 18951:\t Train Loss: 1.9106\n",
            ">>>> Epoch 18952:\t Train Loss: 2.1682\n",
            ">>>> Epoch 18953:\t Train Loss: 2.2085\n",
            ">>>> Epoch 18954:\t Train Loss: 2.4231\n",
            ">>>> Epoch 18955:\t Train Loss: 2.1998\n",
            ">>>> Epoch 18956:\t Train Loss: 2.3349\n",
            ">>>> Epoch 18957:\t Train Loss: 2.4480\n",
            ">>>> Epoch 18958:\t Train Loss: 2.5493\n",
            ">>>> Epoch 18959:\t Train Loss: 2.2596\n",
            ">>>> Epoch 18960:\t Train Loss: 2.1590\n",
            ">>>> Epoch 18961:\t Train Loss: 2.1256\n",
            ">>>> Epoch 18962:\t Train Loss: 2.4313\n",
            ">>>> Epoch 18963:\t Train Loss: 2.1222\n",
            ">>>> Epoch 18964:\t Train Loss: 2.3675\n",
            ">>>> Epoch 18965:\t Train Loss: 2.2418\n",
            ">>>> Epoch 18966:\t Train Loss: 2.0462\n",
            ">>>> Epoch 18967:\t Train Loss: 2.2971\n",
            ">>>> Epoch 18968:\t Train Loss: 2.3740\n",
            ">>>> Epoch 18969:\t Train Loss: 1.8207\n",
            ">>>> Epoch 18970:\t Train Loss: 2.1232\n",
            ">>>> Epoch 18971:\t Train Loss: 2.3537\n",
            ">>>> Epoch 18972:\t Train Loss: 2.3774\n",
            ">>>> Epoch 18973:\t Train Loss: 2.1043\n",
            ">>>> Epoch 18974:\t Train Loss: 2.1560\n",
            ">>>> Epoch 18975:\t Train Loss: 2.2211\n",
            ">>>> Epoch 18976:\t Train Loss: 2.2308\n",
            ">>>> Epoch 18977:\t Train Loss: 2.3637\n",
            ">>>> Epoch 18978:\t Train Loss: 2.3237\n",
            ">>>> Epoch 18979:\t Train Loss: 2.3128\n",
            ">>>> Epoch 18980:\t Train Loss: 2.4776\n",
            ">>>> Epoch 18981:\t Train Loss: 2.6254\n",
            ">>>> Epoch 18982:\t Train Loss: 1.9520\n",
            ">>>> Epoch 18983:\t Train Loss: 2.4952\n",
            ">>>> Epoch 18984:\t Train Loss: 2.4982\n",
            ">>>> Epoch 18985:\t Train Loss: 2.1177\n",
            ">>>> Epoch 18986:\t Train Loss: 2.0562\n",
            ">>>> Epoch 18987:\t Train Loss: 2.2430\n",
            ">>>> Epoch 18988:\t Train Loss: 2.3599\n",
            ">>>> Epoch 18989:\t Train Loss: 2.1008\n",
            ">>>> Epoch 18990:\t Train Loss: 2.1562\n",
            ">>>> Epoch 18991:\t Train Loss: 2.1459\n",
            ">>>> Epoch 18992:\t Train Loss: 2.0933\n",
            ">>>> Epoch 18993:\t Train Loss: 2.3943\n",
            ">>>> Epoch 18994:\t Train Loss: 1.8295\n",
            ">>>> Epoch 18995:\t Train Loss: 2.3929\n",
            ">>>> Epoch 18996:\t Train Loss: 2.0781\n",
            ">>>> Epoch 18997:\t Train Loss: 2.3684\n",
            ">>>> Epoch 18998:\t Train Loss: 2.3879\n",
            ">>>> Epoch 18999:\t Train Loss: 2.0101\n",
            ">>>> Epoch 19000:\t Train Loss: 2.3642\n",
            ">>>> Epoch 19001:\t Train Loss: 2.5247\n",
            ">>>> Epoch 19002:\t Train Loss: 2.5602\n",
            ">>>> Epoch 19003:\t Train Loss: 2.1381\n",
            ">>>> Epoch 19004:\t Train Loss: 2.5325\n",
            ">>>> Epoch 19005:\t Train Loss: 2.0659\n",
            ">>>> Epoch 19006:\t Train Loss: 2.3079\n",
            ">>>> Epoch 19007:\t Train Loss: 1.9638\n",
            ">>>> Epoch 19008:\t Train Loss: 2.2190\n",
            ">>>> Epoch 19009:\t Train Loss: 2.4122\n",
            ">>>> Epoch 19010:\t Train Loss: 2.1168\n",
            ">>>> Epoch 19011:\t Train Loss: 2.2388\n",
            ">>>> Epoch 19012:\t Train Loss: 2.1698\n",
            ">>>> Epoch 19013:\t Train Loss: 2.1830\n",
            ">>>> Epoch 19014:\t Train Loss: 2.4441\n",
            ">>>> Epoch 19015:\t Train Loss: 2.3318\n",
            ">>>> Epoch 19016:\t Train Loss: 2.3890\n",
            ">>>> Epoch 19017:\t Train Loss: 2.2730\n",
            ">>>> Epoch 19018:\t Train Loss: 2.3444\n",
            ">>>> Epoch 19019:\t Train Loss: 2.1287\n",
            ">>>> Epoch 19020:\t Train Loss: 2.1122\n",
            ">>>> Epoch 19021:\t Train Loss: 2.1992\n",
            ">>>> Epoch 19022:\t Train Loss: 2.4268\n",
            ">>>> Epoch 19023:\t Train Loss: 2.3669\n",
            ">>>> Epoch 19024:\t Train Loss: 2.0123\n",
            ">>>> Epoch 19025:\t Train Loss: 2.2680\n",
            ">>>> Epoch 19026:\t Train Loss: 2.6200\n",
            ">>>> Epoch 19027:\t Train Loss: 2.5565\n",
            ">>>> Epoch 19028:\t Train Loss: 2.1788\n",
            ">>>> Epoch 19029:\t Train Loss: 2.3185\n",
            ">>>> Epoch 19030:\t Train Loss: 2.3329\n",
            ">>>> Epoch 19031:\t Train Loss: 2.2281\n",
            ">>>> Epoch 19032:\t Train Loss: 2.5898\n",
            ">>>> Epoch 19033:\t Train Loss: 2.0957\n",
            ">>>> Epoch 19034:\t Train Loss: 2.2806\n",
            ">>>> Epoch 19035:\t Train Loss: 2.0614\n",
            ">>>> Epoch 19036:\t Train Loss: 2.5002\n",
            ">>>> Epoch 19037:\t Train Loss: 2.4833\n",
            ">>>> Epoch 19038:\t Train Loss: 2.1718\n",
            ">>>> Epoch 19039:\t Train Loss: 2.5394\n",
            ">>>> Epoch 19040:\t Train Loss: 2.4129\n",
            ">>>> Epoch 19041:\t Train Loss: 2.3391\n",
            ">>>> Epoch 19042:\t Train Loss: 2.4675\n",
            ">>>> Epoch 19043:\t Train Loss: 2.2118\n",
            ">>>> Epoch 19044:\t Train Loss: 2.3065\n",
            ">>>> Epoch 19045:\t Train Loss: 2.2969\n",
            ">>>> Epoch 19046:\t Train Loss: 2.2328\n",
            ">>>> Epoch 19047:\t Train Loss: 1.9031\n",
            ">>>> Epoch 19048:\t Train Loss: 2.6528\n",
            ">>>> Epoch 19049:\t Train Loss: 2.5142\n",
            ">>>> Epoch 19050:\t Train Loss: 2.2154\n",
            ">>>> Epoch 19051:\t Train Loss: 2.2763\n",
            ">>>> Epoch 19052:\t Train Loss: 2.4010\n",
            ">>>> Epoch 19053:\t Train Loss: 2.4979\n",
            ">>>> Epoch 19054:\t Train Loss: 2.0430\n",
            ">>>> Epoch 19055:\t Train Loss: 2.2584\n",
            ">>>> Epoch 19056:\t Train Loss: 2.3743\n",
            ">>>> Epoch 19057:\t Train Loss: 2.5068\n",
            ">>>> Epoch 19058:\t Train Loss: 2.3632\n",
            ">>>> Epoch 19059:\t Train Loss: 2.1032\n",
            ">>>> Epoch 19060:\t Train Loss: 2.4294\n",
            ">>>> Epoch 19061:\t Train Loss: 2.3128\n",
            ">>>> Epoch 19062:\t Train Loss: 2.3084\n",
            ">>>> Epoch 19063:\t Train Loss: 2.2284\n",
            ">>>> Epoch 19064:\t Train Loss: 2.0442\n",
            ">>>> Epoch 19065:\t Train Loss: 2.2307\n",
            ">>>> Epoch 19066:\t Train Loss: 2.3167\n",
            ">>>> Epoch 19067:\t Train Loss: 2.4141\n",
            ">>>> Epoch 19068:\t Train Loss: 2.0686\n",
            ">>>> Epoch 19069:\t Train Loss: 2.1664\n",
            ">>>> Epoch 19070:\t Train Loss: 2.1733\n",
            ">>>> Epoch 19071:\t Train Loss: 2.2494\n",
            ">>>> Epoch 19072:\t Train Loss: 2.0202\n",
            ">>>> Epoch 19073:\t Train Loss: 2.2254\n",
            ">>>> Epoch 19074:\t Train Loss: 2.1148\n",
            ">>>> Epoch 19075:\t Train Loss: 2.5612\n",
            ">>>> Epoch 19076:\t Train Loss: 1.9504\n",
            ">>>> Epoch 19077:\t Train Loss: 2.3607\n",
            ">>>> Epoch 19078:\t Train Loss: 1.9647\n",
            ">>>> Epoch 19079:\t Train Loss: 1.8067\n",
            ">>>> Epoch 19080:\t Train Loss: 2.4502\n",
            ">>>> Epoch 19081:\t Train Loss: 2.3039\n",
            ">>>> Epoch 19082:\t Train Loss: 2.0089\n",
            ">>>> Epoch 19083:\t Train Loss: 2.3092\n",
            ">>>> Epoch 19084:\t Train Loss: 2.2990\n",
            ">>>> Epoch 19085:\t Train Loss: 2.1902\n",
            ">>>> Epoch 19086:\t Train Loss: 2.2019\n",
            ">>>> Epoch 19087:\t Train Loss: 2.4561\n",
            ">>>> Epoch 19088:\t Train Loss: 1.8537\n",
            ">>>> Epoch 19089:\t Train Loss: 2.0692\n",
            ">>>> Epoch 19090:\t Train Loss: 2.2407\n",
            ">>>> Epoch 19091:\t Train Loss: 2.3600\n",
            ">>>> Epoch 19092:\t Train Loss: 2.0729\n",
            ">>>> Epoch 19093:\t Train Loss: 2.5843\n",
            ">>>> Epoch 19094:\t Train Loss: 2.0719\n",
            ">>>> Epoch 19095:\t Train Loss: 1.7716\n",
            ">>>> Epoch 19096:\t Train Loss: 2.1995\n",
            ">>>> Epoch 19097:\t Train Loss: 1.9527\n",
            ">>>> Epoch 19098:\t Train Loss: 1.8231\n",
            ">>>> Epoch 19099:\t Train Loss: 2.1514\n",
            ">>>> Epoch 19100:\t Train Loss: 2.1432\n",
            ">>>> Epoch 19101:\t Train Loss: 2.2705\n",
            ">>>> Epoch 19102:\t Train Loss: 2.4221\n",
            ">>>> Epoch 19103:\t Train Loss: 2.4006\n",
            ">>>> Epoch 19104:\t Train Loss: 2.3664\n",
            ">>>> Epoch 19105:\t Train Loss: 2.4658\n",
            ">>>> Epoch 19106:\t Train Loss: 2.3132\n",
            ">>>> Epoch 19107:\t Train Loss: 2.5663\n",
            ">>>> Epoch 19108:\t Train Loss: 2.4833\n",
            ">>>> Epoch 19109:\t Train Loss: 1.9023\n",
            ">>>> Epoch 19110:\t Train Loss: 2.8523\n",
            ">>>> Epoch 19111:\t Train Loss: 2.1510\n",
            ">>>> Epoch 19112:\t Train Loss: 2.1228\n",
            ">>>> Epoch 19113:\t Train Loss: 2.3914\n",
            ">>>> Epoch 19114:\t Train Loss: 1.9220\n",
            ">>>> Epoch 19115:\t Train Loss: 2.2542\n",
            ">>>> Epoch 19116:\t Train Loss: 2.1515\n",
            ">>>> Epoch 19117:\t Train Loss: 2.1256\n",
            ">>>> Epoch 19118:\t Train Loss: 2.3370\n",
            ">>>> Epoch 19119:\t Train Loss: 2.1356\n",
            ">>>> Epoch 19120:\t Train Loss: 1.9647\n",
            ">>>> Epoch 19121:\t Train Loss: 1.8995\n",
            ">>>> Epoch 19122:\t Train Loss: 3.0283\n",
            ">>>> Epoch 19123:\t Train Loss: 2.1485\n",
            ">>>> Epoch 19124:\t Train Loss: 2.1540\n",
            ">>>> Epoch 19125:\t Train Loss: 2.4978\n",
            ">>>> Epoch 19126:\t Train Loss: 2.6952\n",
            ">>>> Epoch 19127:\t Train Loss: 2.5800\n",
            ">>>> Epoch 19128:\t Train Loss: 1.9929\n",
            ">>>> Epoch 19129:\t Train Loss: 2.1135\n",
            ">>>> Epoch 19130:\t Train Loss: 2.2490\n",
            ">>>> Epoch 19131:\t Train Loss: 2.2088\n",
            ">>>> Epoch 19132:\t Train Loss: 1.9561\n",
            ">>>> Epoch 19133:\t Train Loss: 2.2277\n",
            ">>>> Epoch 19134:\t Train Loss: 2.3363\n",
            ">>>> Epoch 19135:\t Train Loss: 2.3248\n",
            ">>>> Epoch 19136:\t Train Loss: 2.4900\n",
            ">>>> Epoch 19137:\t Train Loss: 2.6296\n",
            ">>>> Epoch 19138:\t Train Loss: 2.2695\n",
            ">>>> Epoch 19139:\t Train Loss: 2.0876\n",
            ">>>> Epoch 19140:\t Train Loss: 2.6491\n",
            ">>>> Epoch 19141:\t Train Loss: 2.0745\n",
            ">>>> Epoch 19142:\t Train Loss: 2.2835\n",
            ">>>> Epoch 19143:\t Train Loss: 2.6245\n",
            ">>>> Epoch 19144:\t Train Loss: 2.2928\n",
            ">>>> Epoch 19145:\t Train Loss: 2.2070\n",
            ">>>> Epoch 19146:\t Train Loss: 2.2640\n",
            ">>>> Epoch 19147:\t Train Loss: 2.4013\n",
            ">>>> Epoch 19148:\t Train Loss: 2.4475\n",
            ">>>> Epoch 19149:\t Train Loss: 2.1178\n",
            ">>>> Epoch 19150:\t Train Loss: 2.1815\n",
            ">>>> Epoch 19151:\t Train Loss: 1.7415\n",
            ">>>> Epoch 19152:\t Train Loss: 2.4251\n",
            ">>>> Epoch 19153:\t Train Loss: 2.6068\n",
            ">>>> Epoch 19154:\t Train Loss: 2.3798\n",
            ">>>> Epoch 19155:\t Train Loss: 2.4028\n",
            ">>>> Epoch 19156:\t Train Loss: 1.6211\n",
            ">>>> Epoch 19157:\t Train Loss: 2.3305\n",
            ">>>> Epoch 19158:\t Train Loss: 2.3825\n",
            ">>>> Epoch 19159:\t Train Loss: 2.3927\n",
            ">>>> Epoch 19160:\t Train Loss: 2.0919\n",
            ">>>> Epoch 19161:\t Train Loss: 1.5581\n",
            ">>>> Epoch 19162:\t Train Loss: 2.4091\n",
            ">>>> Epoch 19163:\t Train Loss: 2.4105\n",
            ">>>> Epoch 19164:\t Train Loss: 2.1571\n",
            ">>>> Epoch 19165:\t Train Loss: 2.2773\n",
            ">>>> Epoch 19166:\t Train Loss: 2.5128\n",
            ">>>> Epoch 19167:\t Train Loss: 2.3242\n",
            ">>>> Epoch 19168:\t Train Loss: 2.2794\n",
            ">>>> Epoch 19169:\t Train Loss: 2.3463\n",
            ">>>> Epoch 19170:\t Train Loss: 2.0961\n",
            ">>>> Epoch 19171:\t Train Loss: 2.3792\n",
            ">>>> Epoch 19172:\t Train Loss: 2.3939\n",
            ">>>> Epoch 19173:\t Train Loss: 2.2689\n",
            ">>>> Epoch 19174:\t Train Loss: 2.1366\n",
            ">>>> Epoch 19175:\t Train Loss: 2.0160\n",
            ">>>> Epoch 19176:\t Train Loss: 2.4603\n",
            ">>>> Epoch 19177:\t Train Loss: 2.1325\n",
            ">>>> Epoch 19178:\t Train Loss: 2.2231\n",
            ">>>> Epoch 19179:\t Train Loss: 2.0388\n",
            ">>>> Epoch 19180:\t Train Loss: 2.3417\n",
            ">>>> Epoch 19181:\t Train Loss: 2.2466\n",
            ">>>> Epoch 19182:\t Train Loss: 2.5078\n",
            ">>>> Epoch 19183:\t Train Loss: 2.1530\n",
            ">>>> Epoch 19184:\t Train Loss: 2.5765\n",
            ">>>> Epoch 19185:\t Train Loss: 2.1902\n",
            ">>>> Epoch 19186:\t Train Loss: 2.4310\n",
            ">>>> Epoch 19187:\t Train Loss: 2.3228\n",
            ">>>> Epoch 19188:\t Train Loss: 2.5054\n",
            ">>>> Epoch 19189:\t Train Loss: 2.3763\n",
            ">>>> Epoch 19190:\t Train Loss: 2.1053\n",
            ">>>> Epoch 19191:\t Train Loss: 1.9815\n",
            ">>>> Epoch 19192:\t Train Loss: 2.3736\n",
            ">>>> Epoch 19193:\t Train Loss: 2.4962\n",
            ">>>> Epoch 19194:\t Train Loss: 2.3087\n",
            ">>>> Epoch 19195:\t Train Loss: 2.2955\n",
            ">>>> Epoch 19196:\t Train Loss: 2.3709\n",
            ">>>> Epoch 19197:\t Train Loss: 2.1760\n",
            ">>>> Epoch 19198:\t Train Loss: 1.7904\n",
            ">>>> Epoch 19199:\t Train Loss: 2.3588\n",
            ">>>> Epoch 19200:\t Train Loss: 2.1851\n",
            ">>>> Epoch 19201:\t Train Loss: 2.4525\n",
            ">>>> Epoch 19202:\t Train Loss: 2.2616\n",
            ">>>> Epoch 19203:\t Train Loss: 2.4080\n",
            ">>>> Epoch 19204:\t Train Loss: 2.5573\n",
            ">>>> Epoch 19205:\t Train Loss: 2.2945\n",
            ">>>> Epoch 19206:\t Train Loss: 1.9742\n",
            ">>>> Epoch 19207:\t Train Loss: 2.0119\n",
            ">>>> Epoch 19208:\t Train Loss: 1.8175\n",
            ">>>> Epoch 19209:\t Train Loss: 2.0285\n",
            ">>>> Epoch 19210:\t Train Loss: 2.0930\n",
            ">>>> Epoch 19211:\t Train Loss: 2.1263\n",
            ">>>> Epoch 19212:\t Train Loss: 2.2095\n",
            ">>>> Epoch 19213:\t Train Loss: 1.9846\n",
            ">>>> Epoch 19214:\t Train Loss: 2.2546\n",
            ">>>> Epoch 19215:\t Train Loss: 2.3954\n",
            ">>>> Epoch 19216:\t Train Loss: 2.1368\n",
            ">>>> Epoch 19217:\t Train Loss: 2.1925\n",
            ">>>> Epoch 19218:\t Train Loss: 1.9303\n",
            ">>>> Epoch 19219:\t Train Loss: 2.6639\n",
            ">>>> Epoch 19220:\t Train Loss: 2.2306\n",
            ">>>> Epoch 19221:\t Train Loss: 2.4389\n",
            ">>>> Epoch 19222:\t Train Loss: 2.1984\n",
            ">>>> Epoch 19223:\t Train Loss: 2.1149\n",
            ">>>> Epoch 19224:\t Train Loss: 2.0699\n",
            ">>>> Epoch 19225:\t Train Loss: 2.3275\n",
            ">>>> Epoch 19226:\t Train Loss: 2.2584\n",
            ">>>> Epoch 19227:\t Train Loss: 1.9045\n",
            ">>>> Epoch 19228:\t Train Loss: 1.9063\n",
            ">>>> Epoch 19229:\t Train Loss: 2.2214\n",
            ">>>> Epoch 19230:\t Train Loss: 2.4901\n",
            ">>>> Epoch 19231:\t Train Loss: 2.0241\n",
            ">>>> Epoch 19232:\t Train Loss: 2.5080\n",
            ">>>> Epoch 19233:\t Train Loss: 2.0486\n",
            ">>>> Epoch 19234:\t Train Loss: 2.6267\n",
            ">>>> Epoch 19235:\t Train Loss: 2.0656\n",
            ">>>> Epoch 19236:\t Train Loss: 2.4701\n",
            ">>>> Epoch 19237:\t Train Loss: 2.0962\n",
            ">>>> Epoch 19238:\t Train Loss: 2.4548\n",
            ">>>> Epoch 19239:\t Train Loss: 2.1871\n",
            ">>>> Epoch 19240:\t Train Loss: 2.0233\n",
            ">>>> Epoch 19241:\t Train Loss: 2.5844\n",
            ">>>> Epoch 19242:\t Train Loss: 2.1115\n",
            ">>>> Epoch 19243:\t Train Loss: 2.1056\n",
            ">>>> Epoch 19244:\t Train Loss: 1.9770\n",
            ">>>> Epoch 19245:\t Train Loss: 1.9757\n",
            ">>>> Epoch 19246:\t Train Loss: 2.2975\n",
            ">>>> Epoch 19247:\t Train Loss: 2.2371\n",
            ">>>> Epoch 19248:\t Train Loss: 2.2292\n",
            ">>>> Epoch 19249:\t Train Loss: 2.3464\n",
            ">>>> Epoch 19250:\t Train Loss: 2.2510\n",
            ">>>> Epoch 19251:\t Train Loss: 2.5027\n",
            ">>>> Epoch 19252:\t Train Loss: 1.7276\n",
            ">>>> Epoch 19253:\t Train Loss: 2.2212\n",
            ">>>> Epoch 19254:\t Train Loss: 2.3768\n",
            ">>>> Epoch 19255:\t Train Loss: 2.2907\n",
            ">>>> Epoch 19256:\t Train Loss: 2.4341\n",
            ">>>> Epoch 19257:\t Train Loss: 2.1398\n",
            ">>>> Epoch 19258:\t Train Loss: 2.4427\n",
            ">>>> Epoch 19259:\t Train Loss: 2.7322\n",
            ">>>> Epoch 19260:\t Train Loss: 2.4246\n",
            ">>>> Epoch 19261:\t Train Loss: 2.7884\n",
            ">>>> Epoch 19262:\t Train Loss: 2.5068\n",
            ">>>> Epoch 19263:\t Train Loss: 1.7610\n",
            ">>>> Epoch 19264:\t Train Loss: 2.2016\n",
            ">>>> Epoch 19265:\t Train Loss: 2.2903\n",
            ">>>> Epoch 19266:\t Train Loss: 2.3724\n",
            ">>>> Epoch 19267:\t Train Loss: 2.3645\n",
            ">>>> Epoch 19268:\t Train Loss: 1.9435\n",
            ">>>> Epoch 19269:\t Train Loss: 2.0885\n",
            ">>>> Epoch 19270:\t Train Loss: 2.1611\n",
            ">>>> Epoch 19271:\t Train Loss: 2.2282\n",
            ">>>> Epoch 19272:\t Train Loss: 2.3123\n",
            ">>>> Epoch 19273:\t Train Loss: 2.2312\n",
            ">>>> Epoch 19274:\t Train Loss: 2.3492\n",
            ">>>> Epoch 19275:\t Train Loss: 2.2052\n",
            ">>>> Epoch 19276:\t Train Loss: 2.2273\n",
            ">>>> Epoch 19277:\t Train Loss: 2.0323\n",
            ">>>> Epoch 19278:\t Train Loss: 2.1047\n",
            ">>>> Epoch 19279:\t Train Loss: 2.2632\n",
            ">>>> Epoch 19280:\t Train Loss: 1.9696\n",
            ">>>> Epoch 19281:\t Train Loss: 2.5217\n",
            ">>>> Epoch 19282:\t Train Loss: 2.0082\n",
            ">>>> Epoch 19283:\t Train Loss: 2.1624\n",
            ">>>> Epoch 19284:\t Train Loss: 2.0169\n",
            ">>>> Epoch 19285:\t Train Loss: 2.4958\n",
            ">>>> Epoch 19286:\t Train Loss: 2.2878\n",
            ">>>> Epoch 19287:\t Train Loss: 2.1860\n",
            ">>>> Epoch 19288:\t Train Loss: 1.9794\n",
            ">>>> Epoch 19289:\t Train Loss: 2.5989\n",
            ">>>> Epoch 19290:\t Train Loss: 2.5726\n",
            ">>>> Epoch 19291:\t Train Loss: 1.8581\n",
            ">>>> Epoch 19292:\t Train Loss: 2.2401\n",
            ">>>> Epoch 19293:\t Train Loss: 2.5917\n",
            ">>>> Epoch 19294:\t Train Loss: 2.6194\n",
            ">>>> Epoch 19295:\t Train Loss: 2.1364\n",
            ">>>> Epoch 19296:\t Train Loss: 2.2669\n",
            ">>>> Epoch 19297:\t Train Loss: 2.1614\n",
            ">>>> Epoch 19298:\t Train Loss: 2.1823\n",
            ">>>> Epoch 19299:\t Train Loss: 2.1789\n",
            ">>>> Epoch 19300:\t Train Loss: 2.3415\n",
            ">>>> Epoch 19301:\t Train Loss: 2.0885\n",
            ">>>> Epoch 19302:\t Train Loss: 2.3720\n",
            ">>>> Epoch 19303:\t Train Loss: 2.2232\n",
            ">>>> Epoch 19304:\t Train Loss: 2.1462\n",
            ">>>> Epoch 19305:\t Train Loss: 2.1362\n",
            ">>>> Epoch 19306:\t Train Loss: 2.1724\n",
            ">>>> Epoch 19307:\t Train Loss: 2.0978\n",
            ">>>> Epoch 19308:\t Train Loss: 2.2788\n",
            ">>>> Epoch 19309:\t Train Loss: 2.0252\n",
            ">>>> Epoch 19310:\t Train Loss: 1.8024\n",
            ">>>> Epoch 19311:\t Train Loss: 2.2913\n",
            ">>>> Epoch 19312:\t Train Loss: 2.5036\n",
            ">>>> Epoch 19313:\t Train Loss: 2.4695\n",
            ">>>> Epoch 19314:\t Train Loss: 2.2751\n",
            ">>>> Epoch 19315:\t Train Loss: 2.4966\n",
            ">>>> Epoch 19316:\t Train Loss: 2.2243\n",
            ">>>> Epoch 19317:\t Train Loss: 2.0068\n",
            ">>>> Epoch 19318:\t Train Loss: 1.9701\n",
            ">>>> Epoch 19319:\t Train Loss: 2.0646\n",
            ">>>> Epoch 19320:\t Train Loss: 2.3565\n",
            ">>>> Epoch 19321:\t Train Loss: 2.3328\n",
            ">>>> Epoch 19322:\t Train Loss: 2.0931\n",
            ">>>> Epoch 19323:\t Train Loss: 2.0289\n",
            ">>>> Epoch 19324:\t Train Loss: 2.0512\n",
            ">>>> Epoch 19325:\t Train Loss: 2.7951\n",
            ">>>> Epoch 19326:\t Train Loss: 2.2714\n",
            ">>>> Epoch 19327:\t Train Loss: 2.2876\n",
            ">>>> Epoch 19328:\t Train Loss: 2.3002\n",
            ">>>> Epoch 19329:\t Train Loss: 1.9711\n",
            ">>>> Epoch 19330:\t Train Loss: 2.1326\n",
            ">>>> Epoch 19331:\t Train Loss: 2.3007\n",
            ">>>> Epoch 19332:\t Train Loss: 2.3602\n",
            ">>>> Epoch 19333:\t Train Loss: 2.0258\n",
            ">>>> Epoch 19334:\t Train Loss: 2.4715\n",
            ">>>> Epoch 19335:\t Train Loss: 2.4240\n",
            ">>>> Epoch 19336:\t Train Loss: 2.0614\n",
            ">>>> Epoch 19337:\t Train Loss: 1.9508\n",
            ">>>> Epoch 19338:\t Train Loss: 2.8043\n",
            ">>>> Epoch 19339:\t Train Loss: 2.0982\n",
            ">>>> Epoch 19340:\t Train Loss: 2.0990\n",
            ">>>> Epoch 19341:\t Train Loss: 2.1041\n",
            ">>>> Epoch 19342:\t Train Loss: 2.1879\n",
            ">>>> Epoch 19343:\t Train Loss: 2.3260\n",
            ">>>> Epoch 19344:\t Train Loss: 2.0515\n",
            ">>>> Epoch 19345:\t Train Loss: 2.7280\n",
            ">>>> Epoch 19346:\t Train Loss: 2.5342\n",
            ">>>> Epoch 19347:\t Train Loss: 2.2267\n",
            ">>>> Epoch 19348:\t Train Loss: 2.5044\n",
            ">>>> Epoch 19349:\t Train Loss: 2.2668\n",
            ">>>> Epoch 19350:\t Train Loss: 2.3021\n",
            ">>>> Epoch 19351:\t Train Loss: 1.9085\n",
            ">>>> Epoch 19352:\t Train Loss: 2.4353\n",
            ">>>> Epoch 19353:\t Train Loss: 2.1484\n",
            ">>>> Epoch 19354:\t Train Loss: 2.0895\n",
            ">>>> Epoch 19355:\t Train Loss: 2.1253\n",
            ">>>> Epoch 19356:\t Train Loss: 2.1764\n",
            ">>>> Epoch 19357:\t Train Loss: 2.5814\n",
            ">>>> Epoch 19358:\t Train Loss: 2.0496\n",
            ">>>> Epoch 19359:\t Train Loss: 2.2570\n",
            ">>>> Epoch 19360:\t Train Loss: 1.9691\n",
            ">>>> Epoch 19361:\t Train Loss: 2.2041\n",
            ">>>> Epoch 19362:\t Train Loss: 2.4490\n",
            ">>>> Epoch 19363:\t Train Loss: 1.9908\n",
            ">>>> Epoch 19364:\t Train Loss: 2.4892\n",
            ">>>> Epoch 19365:\t Train Loss: 2.0233\n",
            ">>>> Epoch 19366:\t Train Loss: 2.5576\n",
            ">>>> Epoch 19367:\t Train Loss: 1.9690\n",
            ">>>> Epoch 19368:\t Train Loss: 2.3468\n",
            ">>>> Epoch 19369:\t Train Loss: 2.2449\n",
            ">>>> Epoch 19370:\t Train Loss: 2.2260\n",
            ">>>> Epoch 19371:\t Train Loss: 2.0721\n",
            ">>>> Epoch 19372:\t Train Loss: 1.9917\n",
            ">>>> Epoch 19373:\t Train Loss: 2.1471\n",
            ">>>> Epoch 19374:\t Train Loss: 2.8958\n",
            ">>>> Epoch 19375:\t Train Loss: 2.6418\n",
            ">>>> Epoch 19376:\t Train Loss: 1.9599\n",
            ">>>> Epoch 19377:\t Train Loss: 2.0811\n",
            ">>>> Epoch 19378:\t Train Loss: 1.9844\n",
            ">>>> Epoch 19379:\t Train Loss: 1.7988\n",
            ">>>> Epoch 19380:\t Train Loss: 2.5195\n",
            ">>>> Epoch 19381:\t Train Loss: 2.1953\n",
            ">>>> Epoch 19382:\t Train Loss: 2.1828\n",
            ">>>> Epoch 19383:\t Train Loss: 2.0477\n",
            ">>>> Epoch 19384:\t Train Loss: 2.4728\n",
            ">>>> Epoch 19385:\t Train Loss: 2.0117\n",
            ">>>> Epoch 19386:\t Train Loss: 2.2786\n",
            ">>>> Epoch 19387:\t Train Loss: 2.0276\n",
            ">>>> Epoch 19388:\t Train Loss: 2.3699\n",
            ">>>> Epoch 19389:\t Train Loss: 1.9740\n",
            ">>>> Epoch 19390:\t Train Loss: 2.6910\n",
            ">>>> Epoch 19391:\t Train Loss: 2.1717\n",
            ">>>> Epoch 19392:\t Train Loss: 2.3557\n",
            ">>>> Epoch 19393:\t Train Loss: 2.0594\n",
            ">>>> Epoch 19394:\t Train Loss: 2.0063\n",
            ">>>> Epoch 19395:\t Train Loss: 2.7501\n",
            ">>>> Epoch 19396:\t Train Loss: 2.2060\n",
            ">>>> Epoch 19397:\t Train Loss: 2.2641\n",
            ">>>> Epoch 19398:\t Train Loss: 2.7024\n",
            ">>>> Epoch 19399:\t Train Loss: 2.2537\n",
            ">>>> Epoch 19400:\t Train Loss: 2.2149\n",
            ">>>> Epoch 19401:\t Train Loss: 2.3465\n",
            ">>>> Epoch 19402:\t Train Loss: 2.1659\n",
            ">>>> Epoch 19403:\t Train Loss: 2.4488\n",
            ">>>> Epoch 19404:\t Train Loss: 2.3555\n",
            ">>>> Epoch 19405:\t Train Loss: 2.4424\n",
            ">>>> Epoch 19406:\t Train Loss: 2.6193\n",
            ">>>> Epoch 19407:\t Train Loss: 2.4464\n",
            ">>>> Epoch 19408:\t Train Loss: 2.3305\n",
            ">>>> Epoch 19409:\t Train Loss: 2.0964\n",
            ">>>> Epoch 19410:\t Train Loss: 2.2503\n",
            ">>>> Epoch 19411:\t Train Loss: 2.1946\n",
            ">>>> Epoch 19412:\t Train Loss: 2.7995\n",
            ">>>> Epoch 19413:\t Train Loss: 1.9513\n",
            ">>>> Epoch 19414:\t Train Loss: 2.3629\n",
            ">>>> Epoch 19415:\t Train Loss: 2.5715\n",
            ">>>> Epoch 19416:\t Train Loss: 2.0265\n",
            ">>>> Epoch 19417:\t Train Loss: 2.2543\n",
            ">>>> Epoch 19418:\t Train Loss: 2.1325\n",
            ">>>> Epoch 19419:\t Train Loss: 2.1086\n",
            ">>>> Epoch 19420:\t Train Loss: 2.1467\n",
            ">>>> Epoch 19421:\t Train Loss: 2.0958\n",
            ">>>> Epoch 19422:\t Train Loss: 2.0663\n",
            ">>>> Epoch 19423:\t Train Loss: 2.2902\n",
            ">>>> Epoch 19424:\t Train Loss: 2.8470\n",
            ">>>> Epoch 19425:\t Train Loss: 2.0461\n",
            ">>>> Epoch 19426:\t Train Loss: 2.2078\n",
            ">>>> Epoch 19427:\t Train Loss: 2.1872\n",
            ">>>> Epoch 19428:\t Train Loss: 2.2945\n",
            ">>>> Epoch 19429:\t Train Loss: 2.1445\n",
            ">>>> Epoch 19430:\t Train Loss: 2.2101\n",
            ">>>> Epoch 19431:\t Train Loss: 2.5430\n",
            ">>>> Epoch 19432:\t Train Loss: 2.2203\n",
            ">>>> Epoch 19433:\t Train Loss: 2.1255\n",
            ">>>> Epoch 19434:\t Train Loss: 2.5138\n",
            ">>>> Epoch 19435:\t Train Loss: 2.1475\n",
            ">>>> Epoch 19436:\t Train Loss: 2.1846\n",
            ">>>> Epoch 19437:\t Train Loss: 2.6520\n",
            ">>>> Epoch 19438:\t Train Loss: 2.2939\n",
            ">>>> Epoch 19439:\t Train Loss: 2.3474\n",
            ">>>> Epoch 19440:\t Train Loss: 2.4213\n",
            ">>>> Epoch 19441:\t Train Loss: 2.0599\n",
            ">>>> Epoch 19442:\t Train Loss: 2.2396\n",
            ">>>> Epoch 19443:\t Train Loss: 2.0231\n",
            ">>>> Epoch 19444:\t Train Loss: 2.4049\n",
            ">>>> Epoch 19445:\t Train Loss: 2.2103\n",
            ">>>> Epoch 19446:\t Train Loss: 2.5029\n",
            ">>>> Epoch 19447:\t Train Loss: 1.9188\n",
            ">>>> Epoch 19448:\t Train Loss: 2.0549\n",
            ">>>> Epoch 19449:\t Train Loss: 2.3939\n",
            ">>>> Epoch 19450:\t Train Loss: 2.1725\n",
            ">>>> Epoch 19451:\t Train Loss: 2.3943\n",
            ">>>> Epoch 19452:\t Train Loss: 1.8501\n",
            ">>>> Epoch 19453:\t Train Loss: 2.4717\n",
            ">>>> Epoch 19454:\t Train Loss: 2.3920\n",
            ">>>> Epoch 19455:\t Train Loss: 2.2191\n",
            ">>>> Epoch 19456:\t Train Loss: 2.1700\n",
            ">>>> Epoch 19457:\t Train Loss: 2.1746\n",
            ">>>> Epoch 19458:\t Train Loss: 2.2885\n",
            ">>>> Epoch 19459:\t Train Loss: 2.2155\n",
            ">>>> Epoch 19460:\t Train Loss: 2.4016\n",
            ">>>> Epoch 19461:\t Train Loss: 2.3784\n",
            ">>>> Epoch 19462:\t Train Loss: 2.3947\n",
            ">>>> Epoch 19463:\t Train Loss: 2.0177\n",
            ">>>> Epoch 19464:\t Train Loss: 2.3737\n",
            ">>>> Epoch 19465:\t Train Loss: 1.9141\n",
            ">>>> Epoch 19466:\t Train Loss: 2.2307\n",
            ">>>> Epoch 19467:\t Train Loss: 2.4160\n",
            ">>>> Epoch 19468:\t Train Loss: 2.1417\n",
            ">>>> Epoch 19469:\t Train Loss: 2.8669\n",
            ">>>> Epoch 19470:\t Train Loss: 1.9987\n",
            ">>>> Epoch 19471:\t Train Loss: 2.2453\n",
            ">>>> Epoch 19472:\t Train Loss: 2.0947\n",
            ">>>> Epoch 19473:\t Train Loss: 2.2760\n",
            ">>>> Epoch 19474:\t Train Loss: 2.2055\n",
            ">>>> Epoch 19475:\t Train Loss: 2.1366\n",
            ">>>> Epoch 19476:\t Train Loss: 2.2650\n",
            ">>>> Epoch 19477:\t Train Loss: 2.4463\n",
            ">>>> Epoch 19478:\t Train Loss: 2.2678\n",
            ">>>> Epoch 19479:\t Train Loss: 2.3758\n",
            ">>>> Epoch 19480:\t Train Loss: 2.1647\n",
            ">>>> Epoch 19481:\t Train Loss: 2.1719\n",
            ">>>> Epoch 19482:\t Train Loss: 2.1791\n",
            ">>>> Epoch 19483:\t Train Loss: 2.2221\n",
            ">>>> Epoch 19484:\t Train Loss: 2.4066\n",
            ">>>> Epoch 19485:\t Train Loss: 2.3008\n",
            ">>>> Epoch 19486:\t Train Loss: 2.2625\n",
            ">>>> Epoch 19487:\t Train Loss: 2.0666\n",
            ">>>> Epoch 19488:\t Train Loss: 2.3761\n",
            ">>>> Epoch 19489:\t Train Loss: 1.7001\n",
            ">>>> Epoch 19490:\t Train Loss: 2.2250\n",
            ">>>> Epoch 19491:\t Train Loss: 2.1362\n",
            ">>>> Epoch 19492:\t Train Loss: 2.5287\n",
            ">>>> Epoch 19493:\t Train Loss: 2.1840\n",
            ">>>> Epoch 19494:\t Train Loss: 2.2070\n",
            ">>>> Epoch 19495:\t Train Loss: 2.0367\n",
            ">>>> Epoch 19496:\t Train Loss: 2.2876\n",
            ">>>> Epoch 19497:\t Train Loss: 1.9653\n",
            ">>>> Epoch 19498:\t Train Loss: 2.2947\n",
            ">>>> Epoch 19499:\t Train Loss: 2.0656\n",
            ">>>> Epoch 19500:\t Train Loss: 1.9565\n",
            ">>>> Epoch 19501:\t Train Loss: 2.1471\n",
            ">>>> Epoch 19502:\t Train Loss: 2.2703\n",
            ">>>> Epoch 19503:\t Train Loss: 2.0909\n",
            ">>>> Epoch 19504:\t Train Loss: 2.0692\n",
            ">>>> Epoch 19505:\t Train Loss: 1.9321\n",
            ">>>> Epoch 19506:\t Train Loss: 2.1065\n",
            ">>>> Epoch 19507:\t Train Loss: 2.0173\n",
            ">>>> Epoch 19508:\t Train Loss: 2.3334\n",
            ">>>> Epoch 19509:\t Train Loss: 2.1425\n",
            ">>>> Epoch 19510:\t Train Loss: 2.1431\n",
            ">>>> Epoch 19511:\t Train Loss: 2.7540\n",
            ">>>> Epoch 19512:\t Train Loss: 2.2024\n",
            ">>>> Epoch 19513:\t Train Loss: 2.3993\n",
            ">>>> Epoch 19514:\t Train Loss: 2.1749\n",
            ">>>> Epoch 19515:\t Train Loss: 2.1766\n",
            ">>>> Epoch 19516:\t Train Loss: 2.5308\n",
            ">>>> Epoch 19517:\t Train Loss: 2.4165\n",
            ">>>> Epoch 19518:\t Train Loss: 2.1629\n",
            ">>>> Epoch 19519:\t Train Loss: 2.1652\n",
            ">>>> Epoch 19520:\t Train Loss: 2.5490\n",
            ">>>> Epoch 19521:\t Train Loss: 2.2669\n",
            ">>>> Epoch 19522:\t Train Loss: 2.4595\n",
            ">>>> Epoch 19523:\t Train Loss: 1.6958\n",
            ">>>> Epoch 19524:\t Train Loss: 2.1466\n",
            ">>>> Epoch 19525:\t Train Loss: 2.3791\n",
            ">>>> Epoch 19526:\t Train Loss: 2.0050\n",
            ">>>> Epoch 19527:\t Train Loss: 2.1085\n",
            ">>>> Epoch 19528:\t Train Loss: 1.9927\n",
            ">>>> Epoch 19529:\t Train Loss: 1.8954\n",
            ">>>> Epoch 19530:\t Train Loss: 1.9456\n",
            ">>>> Epoch 19531:\t Train Loss: 2.3758\n",
            ">>>> Epoch 19532:\t Train Loss: 2.0096\n",
            ">>>> Epoch 19533:\t Train Loss: 2.2886\n",
            ">>>> Epoch 19534:\t Train Loss: 2.2764\n",
            ">>>> Epoch 19535:\t Train Loss: 2.3678\n",
            ">>>> Epoch 19536:\t Train Loss: 2.2901\n",
            ">>>> Epoch 19537:\t Train Loss: 1.8218\n",
            ">>>> Epoch 19538:\t Train Loss: 2.4737\n",
            ">>>> Epoch 19539:\t Train Loss: 2.3686\n",
            ">>>> Epoch 19540:\t Train Loss: 2.3199\n",
            ">>>> Epoch 19541:\t Train Loss: 2.2007\n",
            ">>>> Epoch 19542:\t Train Loss: 2.3462\n",
            ">>>> Epoch 19543:\t Train Loss: 2.2272\n",
            ">>>> Epoch 19544:\t Train Loss: 1.9213\n",
            ">>>> Epoch 19545:\t Train Loss: 2.0351\n",
            ">>>> Epoch 19546:\t Train Loss: 2.4126\n",
            ">>>> Epoch 19547:\t Train Loss: 2.1199\n",
            ">>>> Epoch 19548:\t Train Loss: 2.2228\n",
            ">>>> Epoch 19549:\t Train Loss: 2.5430\n",
            ">>>> Epoch 19550:\t Train Loss: 2.1055\n",
            ">>>> Epoch 19551:\t Train Loss: 1.9178\n",
            ">>>> Epoch 19552:\t Train Loss: 2.2870\n",
            ">>>> Epoch 19553:\t Train Loss: 2.0812\n",
            ">>>> Epoch 19554:\t Train Loss: 2.1050\n",
            ">>>> Epoch 19555:\t Train Loss: 2.6722\n",
            ">>>> Epoch 19556:\t Train Loss: 2.2188\n",
            ">>>> Epoch 19557:\t Train Loss: 2.4338\n",
            ">>>> Epoch 19558:\t Train Loss: 2.6437\n",
            ">>>> Epoch 19559:\t Train Loss: 2.1524\n",
            ">>>> Epoch 19560:\t Train Loss: 2.3066\n",
            ">>>> Epoch 19561:\t Train Loss: 2.1522\n",
            ">>>> Epoch 19562:\t Train Loss: 2.5806\n",
            ">>>> Epoch 19563:\t Train Loss: 2.6783\n",
            ">>>> Epoch 19564:\t Train Loss: 2.1151\n",
            ">>>> Epoch 19565:\t Train Loss: 2.2675\n",
            ">>>> Epoch 19566:\t Train Loss: 2.3996\n",
            ">>>> Epoch 19567:\t Train Loss: 1.8205\n",
            ">>>> Epoch 19568:\t Train Loss: 2.4752\n",
            ">>>> Epoch 19569:\t Train Loss: 2.0635\n",
            ">>>> Epoch 19570:\t Train Loss: 2.3731\n",
            ">>>> Epoch 19571:\t Train Loss: 2.5680\n",
            ">>>> Epoch 19572:\t Train Loss: 2.2546\n",
            ">>>> Epoch 19573:\t Train Loss: 2.2796\n",
            ">>>> Epoch 19574:\t Train Loss: 2.4106\n",
            ">>>> Epoch 19575:\t Train Loss: 1.9040\n",
            ">>>> Epoch 19576:\t Train Loss: 1.9360\n",
            ">>>> Epoch 19577:\t Train Loss: 2.2015\n",
            ">>>> Epoch 19578:\t Train Loss: 2.9644\n",
            ">>>> Epoch 19579:\t Train Loss: 2.2317\n",
            ">>>> Epoch 19580:\t Train Loss: 2.4412\n",
            ">>>> Epoch 19581:\t Train Loss: 2.1222\n",
            ">>>> Epoch 19582:\t Train Loss: 2.3582\n",
            ">>>> Epoch 19583:\t Train Loss: 2.1351\n",
            ">>>> Epoch 19584:\t Train Loss: 2.1764\n",
            ">>>> Epoch 19585:\t Train Loss: 2.1475\n",
            ">>>> Epoch 19586:\t Train Loss: 2.0574\n",
            ">>>> Epoch 19587:\t Train Loss: 2.4794\n",
            ">>>> Epoch 19588:\t Train Loss: 1.9562\n",
            ">>>> Epoch 19589:\t Train Loss: 1.8999\n",
            ">>>> Epoch 19590:\t Train Loss: 2.3000\n",
            ">>>> Epoch 19591:\t Train Loss: 2.1560\n",
            ">>>> Epoch 19592:\t Train Loss: 2.6292\n",
            ">>>> Epoch 19593:\t Train Loss: 2.4051\n",
            ">>>> Epoch 19594:\t Train Loss: 2.0953\n",
            ">>>> Epoch 19595:\t Train Loss: 2.4966\n",
            ">>>> Epoch 19596:\t Train Loss: 1.7124\n",
            ">>>> Epoch 19597:\t Train Loss: 2.2228\n",
            ">>>> Epoch 19598:\t Train Loss: 2.0236\n",
            ">>>> Epoch 19599:\t Train Loss: 1.9902\n",
            ">>>> Epoch 19600:\t Train Loss: 2.1047\n",
            ">>>> Epoch 19601:\t Train Loss: 2.3200\n",
            ">>>> Epoch 19602:\t Train Loss: 2.6494\n",
            ">>>> Epoch 19603:\t Train Loss: 2.0996\n",
            ">>>> Epoch 19604:\t Train Loss: 2.3236\n",
            ">>>> Epoch 19605:\t Train Loss: 1.8969\n",
            ">>>> Epoch 19606:\t Train Loss: 2.2556\n",
            ">>>> Epoch 19607:\t Train Loss: 1.8392\n",
            ">>>> Epoch 19608:\t Train Loss: 2.5463\n",
            ">>>> Epoch 19609:\t Train Loss: 2.1868\n",
            ">>>> Epoch 19610:\t Train Loss: 2.0832\n",
            ">>>> Epoch 19611:\t Train Loss: 2.0460\n",
            ">>>> Epoch 19612:\t Train Loss: 2.1998\n",
            ">>>> Epoch 19613:\t Train Loss: 2.2684\n",
            ">>>> Epoch 19614:\t Train Loss: 2.2731\n",
            ">>>> Epoch 19615:\t Train Loss: 2.3431\n",
            ">>>> Epoch 19616:\t Train Loss: 2.1883\n",
            ">>>> Epoch 19617:\t Train Loss: 2.3156\n",
            ">>>> Epoch 19618:\t Train Loss: 2.3453\n",
            ">>>> Epoch 19619:\t Train Loss: 2.2057\n",
            ">>>> Epoch 19620:\t Train Loss: 1.9969\n",
            ">>>> Epoch 19621:\t Train Loss: 2.1692\n",
            ">>>> Epoch 19622:\t Train Loss: 2.2640\n",
            ">>>> Epoch 19623:\t Train Loss: 2.2125\n",
            ">>>> Epoch 19624:\t Train Loss: 1.7390\n",
            ">>>> Epoch 19625:\t Train Loss: 2.5198\n",
            ">>>> Epoch 19626:\t Train Loss: 2.1359\n",
            ">>>> Epoch 19627:\t Train Loss: 2.3277\n",
            ">>>> Epoch 19628:\t Train Loss: 2.1979\n",
            ">>>> Epoch 19629:\t Train Loss: 2.4333\n",
            ">>>> Epoch 19630:\t Train Loss: 2.3531\n",
            ">>>> Epoch 19631:\t Train Loss: 2.2720\n",
            ">>>> Epoch 19632:\t Train Loss: 2.2158\n",
            ">>>> Epoch 19633:\t Train Loss: 1.7852\n",
            ">>>> Epoch 19634:\t Train Loss: 2.2140\n",
            ">>>> Epoch 19635:\t Train Loss: 2.5171\n",
            ">>>> Epoch 19636:\t Train Loss: 2.1975\n",
            ">>>> Epoch 19637:\t Train Loss: 2.0668\n",
            ">>>> Epoch 19638:\t Train Loss: 2.3388\n",
            ">>>> Epoch 19639:\t Train Loss: 2.2218\n",
            ">>>> Epoch 19640:\t Train Loss: 2.7834\n",
            ">>>> Epoch 19641:\t Train Loss: 2.2824\n",
            ">>>> Epoch 19642:\t Train Loss: 2.1463\n",
            ">>>> Epoch 19643:\t Train Loss: 2.5003\n",
            ">>>> Epoch 19644:\t Train Loss: 2.4946\n",
            ">>>> Epoch 19645:\t Train Loss: 2.2909\n",
            ">>>> Epoch 19646:\t Train Loss: 1.8322\n",
            ">>>> Epoch 19647:\t Train Loss: 2.2014\n",
            ">>>> Epoch 19648:\t Train Loss: 2.6518\n",
            ">>>> Epoch 19649:\t Train Loss: 2.1274\n",
            ">>>> Epoch 19650:\t Train Loss: 2.1045\n",
            ">>>> Epoch 19651:\t Train Loss: 2.2084\n",
            ">>>> Epoch 19652:\t Train Loss: 2.1888\n",
            ">>>> Epoch 19653:\t Train Loss: 2.2494\n",
            ">>>> Epoch 19654:\t Train Loss: 1.8914\n",
            ">>>> Epoch 19655:\t Train Loss: 2.3763\n",
            ">>>> Epoch 19656:\t Train Loss: 2.3017\n",
            ">>>> Epoch 19657:\t Train Loss: 2.2128\n",
            ">>>> Epoch 19658:\t Train Loss: 2.4556\n",
            ">>>> Epoch 19659:\t Train Loss: 2.1988\n",
            ">>>> Epoch 19660:\t Train Loss: 2.5195\n",
            ">>>> Epoch 19661:\t Train Loss: 2.3144\n",
            ">>>> Epoch 19662:\t Train Loss: 2.2092\n",
            ">>>> Epoch 19663:\t Train Loss: 2.3857\n",
            ">>>> Epoch 19664:\t Train Loss: 2.2819\n",
            ">>>> Epoch 19665:\t Train Loss: 2.6781\n",
            ">>>> Epoch 19666:\t Train Loss: 2.7588\n",
            ">>>> Epoch 19667:\t Train Loss: 2.0776\n",
            ">>>> Epoch 19668:\t Train Loss: 2.2914\n",
            ">>>> Epoch 19669:\t Train Loss: 2.2724\n",
            ">>>> Epoch 19670:\t Train Loss: 2.1900\n",
            ">>>> Epoch 19671:\t Train Loss: 2.0968\n",
            ">>>> Epoch 19672:\t Train Loss: 2.0719\n",
            ">>>> Epoch 19673:\t Train Loss: 2.5053\n",
            ">>>> Epoch 19674:\t Train Loss: 1.9765\n",
            ">>>> Epoch 19675:\t Train Loss: 2.4433\n",
            ">>>> Epoch 19676:\t Train Loss: 1.9804\n",
            ">>>> Epoch 19677:\t Train Loss: 2.1664\n",
            ">>>> Epoch 19678:\t Train Loss: 2.1794\n",
            ">>>> Epoch 19679:\t Train Loss: 2.4897\n",
            ">>>> Epoch 19680:\t Train Loss: 2.2868\n",
            ">>>> Epoch 19681:\t Train Loss: 1.8614\n",
            ">>>> Epoch 19682:\t Train Loss: 1.9699\n",
            ">>>> Epoch 19683:\t Train Loss: 1.9409\n",
            ">>>> Epoch 19684:\t Train Loss: 2.2360\n",
            ">>>> Epoch 19685:\t Train Loss: 2.2404\n",
            ">>>> Epoch 19686:\t Train Loss: 2.1631\n",
            ">>>> Epoch 19687:\t Train Loss: 2.1231\n",
            ">>>> Epoch 19688:\t Train Loss: 2.5207\n",
            ">>>> Epoch 19689:\t Train Loss: 2.1021\n",
            ">>>> Epoch 19690:\t Train Loss: 2.3783\n",
            ">>>> Epoch 19691:\t Train Loss: 2.1622\n",
            ">>>> Epoch 19692:\t Train Loss: 2.3748\n",
            ">>>> Epoch 19693:\t Train Loss: 1.9930\n",
            ">>>> Epoch 19694:\t Train Loss: 2.1491\n",
            ">>>> Epoch 19695:\t Train Loss: 2.1876\n",
            ">>>> Epoch 19696:\t Train Loss: 2.4061\n",
            ">>>> Epoch 19697:\t Train Loss: 2.2032\n",
            ">>>> Epoch 19698:\t Train Loss: 2.4580\n",
            ">>>> Epoch 19699:\t Train Loss: 2.4861\n",
            ">>>> Epoch 19700:\t Train Loss: 2.0858\n",
            ">>>> Epoch 19701:\t Train Loss: 2.0076\n",
            ">>>> Epoch 19702:\t Train Loss: 2.4528\n",
            ">>>> Epoch 19703:\t Train Loss: 2.2443\n",
            ">>>> Epoch 19704:\t Train Loss: 2.2188\n",
            ">>>> Epoch 19705:\t Train Loss: 2.2434\n",
            ">>>> Epoch 19706:\t Train Loss: 2.2532\n",
            ">>>> Epoch 19707:\t Train Loss: 2.7192\n",
            ">>>> Epoch 19708:\t Train Loss: 2.2037\n",
            ">>>> Epoch 19709:\t Train Loss: 2.2151\n",
            ">>>> Epoch 19710:\t Train Loss: 2.5818\n",
            ">>>> Epoch 19711:\t Train Loss: 2.4265\n",
            ">>>> Epoch 19712:\t Train Loss: 2.3195\n",
            ">>>> Epoch 19713:\t Train Loss: 2.3126\n",
            ">>>> Epoch 19714:\t Train Loss: 2.3559\n",
            ">>>> Epoch 19715:\t Train Loss: 2.1200\n",
            ">>>> Epoch 19716:\t Train Loss: 2.3788\n",
            ">>>> Epoch 19717:\t Train Loss: 1.8468\n",
            ">>>> Epoch 19718:\t Train Loss: 2.2450\n",
            ">>>> Epoch 19719:\t Train Loss: 1.9820\n",
            ">>>> Epoch 19720:\t Train Loss: 2.2666\n",
            ">>>> Epoch 19721:\t Train Loss: 2.1518\n",
            ">>>> Epoch 19722:\t Train Loss: 2.1894\n",
            ">>>> Epoch 19723:\t Train Loss: 2.2095\n",
            ">>>> Epoch 19724:\t Train Loss: 2.0244\n",
            ">>>> Epoch 19725:\t Train Loss: 2.1042\n",
            ">>>> Epoch 19726:\t Train Loss: 2.2592\n",
            ">>>> Epoch 19727:\t Train Loss: 2.4876\n",
            ">>>> Epoch 19728:\t Train Loss: 2.0569\n",
            ">>>> Epoch 19729:\t Train Loss: 2.3698\n",
            ">>>> Epoch 19730:\t Train Loss: 2.0857\n",
            ">>>> Epoch 19731:\t Train Loss: 2.4551\n",
            ">>>> Epoch 19732:\t Train Loss: 2.3575\n",
            ">>>> Epoch 19733:\t Train Loss: 2.2205\n",
            ">>>> Epoch 19734:\t Train Loss: 2.2804\n",
            ">>>> Epoch 19735:\t Train Loss: 2.0116\n",
            ">>>> Epoch 19736:\t Train Loss: 2.0429\n",
            ">>>> Epoch 19737:\t Train Loss: 1.9591\n",
            ">>>> Epoch 19738:\t Train Loss: 2.3705\n",
            ">>>> Epoch 19739:\t Train Loss: 2.0720\n",
            ">>>> Epoch 19740:\t Train Loss: 2.1871\n",
            ">>>> Epoch 19741:\t Train Loss: 2.3502\n",
            ">>>> Epoch 19742:\t Train Loss: 2.1219\n",
            ">>>> Epoch 19743:\t Train Loss: 2.2373\n",
            ">>>> Epoch 19744:\t Train Loss: 2.2228\n",
            ">>>> Epoch 19745:\t Train Loss: 2.1620\n",
            ">>>> Epoch 19746:\t Train Loss: 2.2249\n",
            ">>>> Epoch 19747:\t Train Loss: 2.2493\n",
            ">>>> Epoch 19748:\t Train Loss: 2.3235\n",
            ">>>> Epoch 19749:\t Train Loss: 2.1228\n",
            ">>>> Epoch 19750:\t Train Loss: 2.5034\n",
            ">>>> Epoch 19751:\t Train Loss: 2.3340\n",
            ">>>> Epoch 19752:\t Train Loss: 2.6250\n",
            ">>>> Epoch 19753:\t Train Loss: 2.0268\n",
            ">>>> Epoch 19754:\t Train Loss: 2.0711\n",
            ">>>> Epoch 19755:\t Train Loss: 1.9366\n",
            ">>>> Epoch 19756:\t Train Loss: 2.2343\n",
            ">>>> Epoch 19757:\t Train Loss: 2.0598\n",
            ">>>> Epoch 19758:\t Train Loss: 2.1750\n",
            ">>>> Epoch 19759:\t Train Loss: 2.3694\n",
            ">>>> Epoch 19760:\t Train Loss: 2.1265\n",
            ">>>> Epoch 19761:\t Train Loss: 2.2452\n",
            ">>>> Epoch 19762:\t Train Loss: 2.1605\n",
            ">>>> Epoch 19763:\t Train Loss: 2.4585\n",
            ">>>> Epoch 19764:\t Train Loss: 2.1822\n",
            ">>>> Epoch 19765:\t Train Loss: 2.0679\n",
            ">>>> Epoch 19766:\t Train Loss: 2.5233\n",
            ">>>> Epoch 19767:\t Train Loss: 2.2399\n",
            ">>>> Epoch 19768:\t Train Loss: 2.1738\n",
            ">>>> Epoch 19769:\t Train Loss: 1.8540\n",
            ">>>> Epoch 19770:\t Train Loss: 2.3305\n",
            ">>>> Epoch 19771:\t Train Loss: 2.0268\n",
            ">>>> Epoch 19772:\t Train Loss: 2.3409\n",
            ">>>> Epoch 19773:\t Train Loss: 1.9358\n",
            ">>>> Epoch 19774:\t Train Loss: 2.4674\n",
            ">>>> Epoch 19775:\t Train Loss: 2.0897\n",
            ">>>> Epoch 19776:\t Train Loss: 2.2326\n",
            ">>>> Epoch 19777:\t Train Loss: 1.9231\n",
            ">>>> Epoch 19778:\t Train Loss: 2.3228\n",
            ">>>> Epoch 19779:\t Train Loss: 2.3872\n",
            ">>>> Epoch 19780:\t Train Loss: 2.4627\n",
            ">>>> Epoch 19781:\t Train Loss: 2.8536\n",
            ">>>> Epoch 19782:\t Train Loss: 2.1781\n",
            ">>>> Epoch 19783:\t Train Loss: 2.2017\n",
            ">>>> Epoch 19784:\t Train Loss: 2.2879\n",
            ">>>> Epoch 19785:\t Train Loss: 2.4270\n",
            ">>>> Epoch 19786:\t Train Loss: 2.2330\n",
            ">>>> Epoch 19787:\t Train Loss: 2.3437\n",
            ">>>> Epoch 19788:\t Train Loss: 2.0718\n",
            ">>>> Epoch 19789:\t Train Loss: 2.0445\n",
            ">>>> Epoch 19790:\t Train Loss: 2.0916\n",
            ">>>> Epoch 19791:\t Train Loss: 2.0115\n",
            ">>>> Epoch 19792:\t Train Loss: 2.1975\n",
            ">>>> Epoch 19793:\t Train Loss: 2.3576\n",
            ">>>> Epoch 19794:\t Train Loss: 2.5369\n",
            ">>>> Epoch 19795:\t Train Loss: 2.6526\n",
            ">>>> Epoch 19796:\t Train Loss: 2.0047\n",
            ">>>> Epoch 19797:\t Train Loss: 2.2663\n",
            ">>>> Epoch 19798:\t Train Loss: 2.2017\n",
            ">>>> Epoch 19799:\t Train Loss: 2.1791\n",
            ">>>> Epoch 19800:\t Train Loss: 2.1592\n",
            ">>>> Epoch 19801:\t Train Loss: 2.1592\n",
            ">>>> Epoch 19802:\t Train Loss: 2.2609\n",
            ">>>> Epoch 19803:\t Train Loss: 1.9940\n",
            ">>>> Epoch 19804:\t Train Loss: 1.9506\n",
            ">>>> Epoch 19805:\t Train Loss: 2.6519\n",
            ">>>> Epoch 19806:\t Train Loss: 2.6275\n",
            ">>>> Epoch 19807:\t Train Loss: 2.2733\n",
            ">>>> Epoch 19808:\t Train Loss: 2.1465\n",
            ">>>> Epoch 19809:\t Train Loss: 2.0113\n",
            ">>>> Epoch 19810:\t Train Loss: 1.8936\n",
            ">>>> Epoch 19811:\t Train Loss: 2.5905\n",
            ">>>> Epoch 19812:\t Train Loss: 2.1712\n",
            ">>>> Epoch 19813:\t Train Loss: 2.1836\n",
            ">>>> Epoch 19814:\t Train Loss: 2.2411\n",
            ">>>> Epoch 19815:\t Train Loss: 1.9340\n",
            ">>>> Epoch 19816:\t Train Loss: 2.1572\n",
            ">>>> Epoch 19817:\t Train Loss: 2.1900\n",
            ">>>> Epoch 19818:\t Train Loss: 2.2920\n",
            ">>>> Epoch 19819:\t Train Loss: 1.9917\n",
            ">>>> Epoch 19820:\t Train Loss: 2.4543\n",
            ">>>> Epoch 19821:\t Train Loss: 2.4702\n",
            ">>>> Epoch 19822:\t Train Loss: 2.1911\n",
            ">>>> Epoch 19823:\t Train Loss: 2.0766\n",
            ">>>> Epoch 19824:\t Train Loss: 2.3289\n",
            ">>>> Epoch 19825:\t Train Loss: 2.5413\n",
            ">>>> Epoch 19826:\t Train Loss: 2.4512\n",
            ">>>> Epoch 19827:\t Train Loss: 2.4156\n",
            ">>>> Epoch 19828:\t Train Loss: 2.1820\n",
            ">>>> Epoch 19829:\t Train Loss: 2.3244\n",
            ">>>> Epoch 19830:\t Train Loss: 2.2892\n",
            ">>>> Epoch 19831:\t Train Loss: 2.0773\n",
            ">>>> Epoch 19832:\t Train Loss: 2.3118\n",
            ">>>> Epoch 19833:\t Train Loss: 2.4328\n",
            ">>>> Epoch 19834:\t Train Loss: 2.0107\n",
            ">>>> Epoch 19835:\t Train Loss: 2.4783\n",
            ">>>> Epoch 19836:\t Train Loss: 2.3750\n",
            ">>>> Epoch 19837:\t Train Loss: 2.2210\n",
            ">>>> Epoch 19838:\t Train Loss: 2.1341\n",
            ">>>> Epoch 19839:\t Train Loss: 2.2897\n",
            ">>>> Epoch 19840:\t Train Loss: 2.1081\n",
            ">>>> Epoch 19841:\t Train Loss: 2.3497\n",
            ">>>> Epoch 19842:\t Train Loss: 2.2891\n",
            ">>>> Epoch 19843:\t Train Loss: 2.2538\n",
            ">>>> Epoch 19844:\t Train Loss: 2.3276\n",
            ">>>> Epoch 19845:\t Train Loss: 2.0559\n",
            ">>>> Epoch 19846:\t Train Loss: 2.2065\n",
            ">>>> Epoch 19847:\t Train Loss: 2.2047\n",
            ">>>> Epoch 19848:\t Train Loss: 1.9018\n",
            ">>>> Epoch 19849:\t Train Loss: 1.9037\n",
            ">>>> Epoch 19850:\t Train Loss: 2.2127\n",
            ">>>> Epoch 19851:\t Train Loss: 2.0293\n",
            ">>>> Epoch 19852:\t Train Loss: 2.4647\n",
            ">>>> Epoch 19853:\t Train Loss: 2.1022\n",
            ">>>> Epoch 19854:\t Train Loss: 2.2147\n",
            ">>>> Epoch 19855:\t Train Loss: 2.3608\n",
            ">>>> Epoch 19856:\t Train Loss: 2.3336\n",
            ">>>> Epoch 19857:\t Train Loss: 2.1104\n",
            ">>>> Epoch 19858:\t Train Loss: 2.2813\n",
            ">>>> Epoch 19859:\t Train Loss: 1.8055\n",
            ">>>> Epoch 19860:\t Train Loss: 2.2831\n",
            ">>>> Epoch 19861:\t Train Loss: 2.0450\n",
            ">>>> Epoch 19862:\t Train Loss: 2.3703\n",
            ">>>> Epoch 19863:\t Train Loss: 2.4539\n",
            ">>>> Epoch 19864:\t Train Loss: 2.1257\n",
            ">>>> Epoch 19865:\t Train Loss: 2.2460\n",
            ">>>> Epoch 19866:\t Train Loss: 1.7148\n",
            ">>>> Epoch 19867:\t Train Loss: 2.1559\n",
            ">>>> Epoch 19868:\t Train Loss: 2.2966\n",
            ">>>> Epoch 19869:\t Train Loss: 2.4505\n",
            ">>>> Epoch 19870:\t Train Loss: 2.2455\n",
            ">>>> Epoch 19871:\t Train Loss: 2.3888\n",
            ">>>> Epoch 19872:\t Train Loss: 2.3058\n",
            ">>>> Epoch 19873:\t Train Loss: 2.3497\n",
            ">>>> Epoch 19874:\t Train Loss: 2.1725\n",
            ">>>> Epoch 19875:\t Train Loss: 2.0481\n",
            ">>>> Epoch 19876:\t Train Loss: 2.2478\n",
            ">>>> Epoch 19877:\t Train Loss: 1.9102\n",
            ">>>> Epoch 19878:\t Train Loss: 1.8982\n",
            ">>>> Epoch 19879:\t Train Loss: 1.8221\n",
            ">>>> Epoch 19880:\t Train Loss: 2.3533\n",
            ">>>> Epoch 19881:\t Train Loss: 2.8263\n",
            ">>>> Epoch 19882:\t Train Loss: 2.4875\n",
            ">>>> Epoch 19883:\t Train Loss: 2.3850\n",
            ">>>> Epoch 19884:\t Train Loss: 2.3031\n",
            ">>>> Epoch 19885:\t Train Loss: 1.9762\n",
            ">>>> Epoch 19886:\t Train Loss: 1.7333\n",
            ">>>> Epoch 19887:\t Train Loss: 2.0882\n",
            ">>>> Epoch 19888:\t Train Loss: 2.1487\n",
            ">>>> Epoch 19889:\t Train Loss: 2.1421\n",
            ">>>> Epoch 19890:\t Train Loss: 2.4658\n",
            ">>>> Epoch 19891:\t Train Loss: 2.6562\n",
            ">>>> Epoch 19892:\t Train Loss: 1.8847\n",
            ">>>> Epoch 19893:\t Train Loss: 1.8551\n",
            ">>>> Epoch 19894:\t Train Loss: 2.1443\n",
            ">>>> Epoch 19895:\t Train Loss: 1.8757\n",
            ">>>> Epoch 19896:\t Train Loss: 2.2997\n",
            ">>>> Epoch 19897:\t Train Loss: 2.1936\n",
            ">>>> Epoch 19898:\t Train Loss: 2.3960\n",
            ">>>> Epoch 19899:\t Train Loss: 2.2923\n",
            ">>>> Epoch 19900:\t Train Loss: 2.5631\n",
            ">>>> Epoch 19901:\t Train Loss: 1.9923\n",
            ">>>> Epoch 19902:\t Train Loss: 2.5045\n",
            ">>>> Epoch 19903:\t Train Loss: 2.5071\n",
            ">>>> Epoch 19904:\t Train Loss: 2.5739\n",
            ">>>> Epoch 19905:\t Train Loss: 2.1018\n",
            ">>>> Epoch 19906:\t Train Loss: 2.6752\n",
            ">>>> Epoch 19907:\t Train Loss: 2.1704\n",
            ">>>> Epoch 19908:\t Train Loss: 2.1157\n",
            ">>>> Epoch 19909:\t Train Loss: 2.2674\n",
            ">>>> Epoch 19910:\t Train Loss: 2.3288\n",
            ">>>> Epoch 19911:\t Train Loss: 2.5640\n",
            ">>>> Epoch 19912:\t Train Loss: 2.1755\n",
            ">>>> Epoch 19913:\t Train Loss: 2.1208\n",
            ">>>> Epoch 19914:\t Train Loss: 1.8380\n",
            ">>>> Epoch 19915:\t Train Loss: 1.8270\n",
            ">>>> Epoch 19916:\t Train Loss: 2.2910\n",
            ">>>> Epoch 19917:\t Train Loss: 1.9034\n",
            ">>>> Epoch 19918:\t Train Loss: 2.2474\n",
            ">>>> Epoch 19919:\t Train Loss: 2.0336\n",
            ">>>> Epoch 19920:\t Train Loss: 2.2318\n",
            ">>>> Epoch 19921:\t Train Loss: 2.3404\n",
            ">>>> Epoch 19922:\t Train Loss: 2.2908\n",
            ">>>> Epoch 19923:\t Train Loss: 2.2070\n",
            ">>>> Epoch 19924:\t Train Loss: 2.6876\n",
            ">>>> Epoch 19925:\t Train Loss: 2.1729\n",
            ">>>> Epoch 19926:\t Train Loss: 2.5685\n",
            ">>>> Epoch 19927:\t Train Loss: 2.5453\n",
            ">>>> Epoch 19928:\t Train Loss: 2.2808\n",
            ">>>> Epoch 19929:\t Train Loss: 2.2199\n",
            ">>>> Epoch 19930:\t Train Loss: 2.5691\n",
            ">>>> Epoch 19931:\t Train Loss: 2.3075\n",
            ">>>> Epoch 19932:\t Train Loss: 2.0545\n",
            ">>>> Epoch 19933:\t Train Loss: 2.2843\n",
            ">>>> Epoch 19934:\t Train Loss: 2.5805\n",
            ">>>> Epoch 19935:\t Train Loss: 1.9747\n",
            ">>>> Epoch 19936:\t Train Loss: 2.4093\n",
            ">>>> Epoch 19937:\t Train Loss: 2.4652\n",
            ">>>> Epoch 19938:\t Train Loss: 2.4068\n",
            ">>>> Epoch 19939:\t Train Loss: 2.2967\n",
            ">>>> Epoch 19940:\t Train Loss: 2.1947\n",
            ">>>> Epoch 19941:\t Train Loss: 2.3108\n",
            ">>>> Epoch 19942:\t Train Loss: 2.3815\n",
            ">>>> Epoch 19943:\t Train Loss: 2.1564\n",
            ">>>> Epoch 19944:\t Train Loss: 2.3216\n",
            ">>>> Epoch 19945:\t Train Loss: 2.5911\n",
            ">>>> Epoch 19946:\t Train Loss: 2.0103\n",
            ">>>> Epoch 19947:\t Train Loss: 2.3567\n",
            ">>>> Epoch 19948:\t Train Loss: 2.2923\n",
            ">>>> Epoch 19949:\t Train Loss: 2.3766\n",
            ">>>> Epoch 19950:\t Train Loss: 1.9641\n",
            ">>>> Epoch 19951:\t Train Loss: 2.1681\n",
            ">>>> Epoch 19952:\t Train Loss: 2.2249\n",
            ">>>> Epoch 19953:\t Train Loss: 2.3333\n",
            ">>>> Epoch 19954:\t Train Loss: 2.7082\n",
            ">>>> Epoch 19955:\t Train Loss: 1.9740\n",
            ">>>> Epoch 19956:\t Train Loss: 1.9079\n",
            ">>>> Epoch 19957:\t Train Loss: 2.2148\n",
            ">>>> Epoch 19958:\t Train Loss: 2.1642\n",
            ">>>> Epoch 19959:\t Train Loss: 2.1323\n",
            ">>>> Epoch 19960:\t Train Loss: 2.4791\n",
            ">>>> Epoch 19961:\t Train Loss: 2.1524\n",
            ">>>> Epoch 19962:\t Train Loss: 2.1751\n",
            ">>>> Epoch 19963:\t Train Loss: 2.6646\n",
            ">>>> Epoch 19964:\t Train Loss: 2.0554\n",
            ">>>> Epoch 19965:\t Train Loss: 2.3795\n",
            ">>>> Epoch 19966:\t Train Loss: 2.0313\n",
            ">>>> Epoch 19967:\t Train Loss: 2.2787\n",
            ">>>> Epoch 19968:\t Train Loss: 2.1687\n",
            ">>>> Epoch 19969:\t Train Loss: 2.0323\n",
            ">>>> Epoch 19970:\t Train Loss: 2.6265\n",
            ">>>> Epoch 19971:\t Train Loss: 2.2932\n",
            ">>>> Epoch 19972:\t Train Loss: 2.2026\n",
            ">>>> Epoch 19973:\t Train Loss: 2.3311\n",
            ">>>> Epoch 19974:\t Train Loss: 2.4008\n",
            ">>>> Epoch 19975:\t Train Loss: 1.7695\n",
            ">>>> Epoch 19976:\t Train Loss: 2.0083\n",
            ">>>> Epoch 19977:\t Train Loss: 2.4003\n",
            ">>>> Epoch 19978:\t Train Loss: 2.8287\n",
            ">>>> Epoch 19979:\t Train Loss: 2.5457\n",
            ">>>> Epoch 19980:\t Train Loss: 2.1639\n",
            ">>>> Epoch 19981:\t Train Loss: 1.9550\n",
            ">>>> Epoch 19982:\t Train Loss: 2.3834\n",
            ">>>> Epoch 19983:\t Train Loss: 2.0839\n",
            ">>>> Epoch 19984:\t Train Loss: 2.1820\n",
            ">>>> Epoch 19985:\t Train Loss: 2.3406\n",
            ">>>> Epoch 19986:\t Train Loss: 2.0704\n",
            ">>>> Epoch 19987:\t Train Loss: 2.2094\n",
            ">>>> Epoch 19988:\t Train Loss: 2.1226\n",
            ">>>> Epoch 19989:\t Train Loss: 2.3913\n",
            ">>>> Epoch 19990:\t Train Loss: 2.5422\n",
            ">>>> Epoch 19991:\t Train Loss: 2.3938\n",
            ">>>> Epoch 19992:\t Train Loss: 2.3618\n",
            ">>>> Epoch 19993:\t Train Loss: 2.3004\n",
            ">>>> Epoch 19994:\t Train Loss: 1.7091\n",
            ">>>> Epoch 19995:\t Train Loss: 1.9296\n",
            ">>>> Epoch 19996:\t Train Loss: 2.4625\n",
            ">>>> Epoch 19997:\t Train Loss: 2.2390\n",
            ">>>> Epoch 19998:\t Train Loss: 2.2253\n",
            ">>>> Epoch 19999:\t Train Loss: 2.4950\n",
            ">>>> Average training loss: 2.3379\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc4092fb070>]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot([0, 1000], [-3,-3], \"k\")"
      ],
      "metadata": {
        "id": "cdhAWzPk71N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J1umM3KdITEF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
