{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recarp to Language Models -.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMJZMdJVLdnWziW4c1O7x7i",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinpius/MachineTranslation/blob/main/Recarp_to_Language_Models_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMYKIofneNOM",
        "outputId": "592f375c-8107-461f-9351-ecc2f4144beb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount = True)\n",
        "try:\n",
        "  COLAB = True\n",
        "  import tensorflow as tf\n",
        "  print(f\"You are on Google Colab with tensoflow version: {tf.__version__}\")\n",
        "except Exception as e:\n",
        "  COLAB = False\n",
        "  print(f\"{type(e)}: {e}\\n...Please Load Your Drive...\")\n",
        "def time_fmt(t):\n",
        "  h = int(t / (60 * 60))\n",
        "  m = int(t % (60 * 60) / 60)\n",
        "  s = int(t % 60)\n",
        "  return f\"{h}: {m:>03}: {s:>05.2f}\"\n",
        "print(f\"time elapse is: {time_fmt(123.46987)}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "You are on Google Colab with tensoflow version: 2.4.0\n",
            "time elapse is: 0: 002: 03.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fK10vZYFftQr"
      },
      "source": [
        "import re, unicodedata, time,io,os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib as mpl\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ao9S31jgo7p"
      },
      "source": [
        "#Import and preprocess the data from the url using tensorflow utils"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U34Nwd0bgx0P",
        "outputId": "2297afb0-bb57-43e8-a0e2-1df4fa39dbaa"
      },
      "source": [
        "folder_path = tf.keras.utils.get_file(fname = 'spa-eng.zip', origin = \"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\", extract = True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSXYfMhXhU4I"
      },
      "source": [
        "file_path = os.path.dirname(folder_path)+ '/spa-eng/spa.txt'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mkl6H8_hrH3"
      },
      "source": [
        "def unicode_ascii(text):\n",
        "  return \"\".join(k for k in unicodedata.normalize(\"NFD\", text) if unicodedata.category(k) != 'Mn')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M4zNL6BkAGC"
      },
      "source": [
        "def text_preprocess(text):\n",
        "  text = unicode_ascii(text.lower().strip())\n",
        "  text = re.sub(r\"([!,.多?])\", r\" \\1 \", text)\n",
        "  text = re.sub(r'[\" \"]+', r\" \", text)\n",
        "  text = re.sub(\"[^a-zA-Z.,多?!]+\", r\" \", text)\n",
        "  text = text.strip()\n",
        "  text = \"<start> \" + text + \" <end>\"\n",
        "  return text\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I282QTEAsn7W"
      },
      "source": [
        "#Testing the processing function\n",
        "en_verse = u\"May I borrow this @ book?\"\n",
        "sp_verse = u\"多Puedo tomar prestado este libro?\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pD2CLZN8tqkN",
        "outputId": "a30778b1-3a76-47e4-c764-b7e3843fe175"
      },
      "source": [
        "print(f\"english sentence: {text_preprocess(en_verse)}\\nspanish sentence: {text_preprocess(sp_verse)}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "english sentence: <start> may i borrow this book ? <end>\n",
            "spanish sentence: <start> 多 puedo tomar prestado este libro ? <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdJ5uhMbuAs1"
      },
      "source": [
        "#Return a pair of sentences matching each language:\n",
        "def matched_texts(path, sample_size):\n",
        "  S = io.open(path,encoding = 'UTF-8').read().strip().split('\\n')\n",
        "  paired_text = [[text_preprocess(k) for k in l.split(\"\\t\")] for l in S[:sample_size]]\n",
        "  return zip(*paired_text)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWlPq3t4yPN2"
      },
      "source": [
        "#Testing the function:\n",
        "english, spanish = matched_texts(file_path, None)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmatGHfXycjp",
        "outputId": "90d6b047-b9a8-47b2-e2c7-e3751f146dd3"
      },
      "source": [
        "print(f\"last english verse: {english[-1]}\\nlast spanish verse: {spanish[-1]}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "last english verse: <start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
            "last spanish verse: <start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUazBq5Oy_id"
      },
      "source": [
        "#Tokenization of the text"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NkBt1kN17cc"
      },
      "source": [
        "def tokenization(text):\n",
        "  tokenizer = tf.keras.preprocessing.text.Tokenizer(filters = \"\")\n",
        "  tokenizer.fit_on_texts(text)\n",
        "  tensor = tokenizer.texts_to_sequences(text)\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding = 'post')\n",
        "  return tensor, tokenizer"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04I-x_B83UU7"
      },
      "source": [
        "#Loading the clean data of size 100000 which is ready for training:"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5yx4Lfr3prB"
      },
      "source": [
        "output_lang, input_lang = matched_texts(file_path, sample_size = 100000)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWADIGVO4AVM"
      },
      "source": [
        "input_tensor, input_tokenizer = tokenization(input_lang)\n",
        "output_tensor, output_tokenizer = tokenization(output_lang)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRmDUVn54T5a",
        "outputId": "eac84ad1-d20e-4661-ce00-b9c5f7ae9bb2"
      },
      "source": [
        "print(f\"input_tensor_shape: {input_tensor.shape}\\noutput_tensor_shape: {output_tensor.shape}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_tensor_shape: (100000, 20)\n",
            "output_tensor_shape: (100000, 17)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jujC8QNHm8X",
        "outputId": "b5b80af3-b7e5-467c-b3ea-24d13af13a0f"
      },
      "source": [
        "#Split the data into train and test using cv\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train,y_test = train_test_split(input_tensor, output_tensor, test_size = 0.1)\n",
        "print(f\"train_shape: {x_train.shape}, {y_train.shape}\\ntest_shape: {x_test.shape}, {y_test.shape}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_shape: (90000, 20), (90000, 17)\n",
            "test_shape: (10000, 20), (10000, 17)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9YEKaem4rVc"
      },
      "source": [
        "#Create index-word matching for the languages:"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrF5EXWF46Zk"
      },
      "source": [
        "def create_index(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t != 0:\n",
        "      print(\"%d---->%s\" %(t, lang.index_word[t]))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUKSmQPJHTM3",
        "outputId": "9abdfb9d-012c-49e3-fc6f-da09e93b0736"
      },
      "source": [
        "print(\"Input language, from word to index mapping:\")\n",
        "print(create_index(input_tokenizer, x_train[10]))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input language, from word to index mapping:\n",
            "1----><start>\n",
            "13---->es\n",
            "21---->una\n",
            "197---->buena\n",
            "467---->camara\n",
            "3---->.\n",
            "2----><end>\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGLt42EqIXI4",
        "outputId": "2cb49ebe-0db1-4175-d084-9574b9b378be"
      },
      "source": [
        "print(\"Output language, from word to index mapping:\")\n",
        "print(create_index(output_tokenizer, y_train[10]))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output language, from word to index mapping:\n",
            "1----><start>\n",
            "14---->it\n",
            "15---->s\n",
            "10---->a\n",
            "76---->good\n",
            "496---->camera\n",
            "3---->.\n",
            "2----><end>\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIccFd_GJz99"
      },
      "source": [
        "#Convert to tensorflow data type and splits into batches\n",
        "batch_size = 64\n",
        "embedding_dim = 512\n",
        "units = 1024\n",
        "input_voc_size = len(input_tokenizer.word_index)+1\n",
        "output_voc_size = len(output_tokenizer.word_index)+ 1\n",
        "steps_per_epoch_train = len(x_train)//batch_size\n",
        "steps_per_epoch_val = len(x_test)//batch_size\n",
        "BUFFER = len(x_train)\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRDDHW5kXYRH"
      },
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_data = train_data.shuffle(BUFFER).batch(batch_size, drop_remainder = True)\n",
        "validation_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "validation_data = validation_data.shuffle(BUFFER).batch(batch_size, drop_remainder = True)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjDJBqJjZtti"
      },
      "source": [
        "#Get the sample batch for testing the classes later on during model building"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0pxSk1uZ4Vu",
        "outputId": "0574e664-3359-42e2-8372-ccfaa3024aba"
      },
      "source": [
        "train_sample_x, train_sample_y = next(iter(train_data))\n",
        "print(f\"train_sample_x_shape: {train_sample_x.shape}\\ntrain_sample_y_shape: {train_sample_y.shape}\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_sample_x_shape: (64, 20)\n",
            "train_sample_y_shape: (64, 17)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfuQBIKVaDju"
      },
      "source": [
        "#Model Building:"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEbZnm1_agbl"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, enc_units, embedding_dim, voc_size, batch_size,name = 'encoder', **kwargs):\n",
        "    super(Encoder, self).__init__(self, name = name, **kwargs)\n",
        "    self.enc_units = enc_units\n",
        "    self.batch_size = batch_size\n",
        "    self.embedding_layer = tf.keras.layers.Embedding(input_dim = voc_size, output_dim = embedding_dim, name = 'dec_embedding')\n",
        "    self.gru_layer = tf.keras.layers.GRU(units = self.enc_units, kernel_initializer = 'glorot_uniform',\n",
        "                                         return_state = True, return_sequences = True,\n",
        "                                         recurrent_dropout = 0.5, dropout = 0.5,\n",
        "                                         name = 'decoder_gru')\n",
        "  \n",
        "  def call(self, inputs, enc_hidden):\n",
        "    inputs = self.embedding_layer(inputs)\n",
        "    enc_out, enc_hidden = self.gru_layer(inputs, initial_state = enc_hidden)\n",
        "    return enc_out, enc_hidden\n",
        "  \n",
        "  def hidden_initializer(self):\n",
        "    return tf.zeros(shape = (self.batch_size, self.enc_units))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axDJDvPocoaq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf37bd02-8107-41c7-a6cd-5f519608169f"
      },
      "source": [
        "#Instantiating and testing the encoder using the sample batch:\n",
        "encoder = Encoder(units, embedding_dim,input_voc_size,batch_size,name = 'encoder')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer decoder_gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLGE7VModCjt",
        "outputId": "6e7a3419-8b51-4ea1-8194-979c415f8863"
      },
      "source": [
        "enc_hidden = encoder.hidden_initializer()\n",
        "sample_enc_out, sample_enc_hidden = encoder(train_sample_x, enc_hidden)\n",
        "print(f\"sample_enc_out_shape: {sample_enc_out.shape}\\nsample_enc_hidden_shape: {sample_enc_hidden.shape}\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_enc_out_shape: (64, 20, 1024)\n",
            "sample_enc_hidden_shape: (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SfrKMtwd8S-"
      },
      "source": [
        "#Decoder without an attention:\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, dec_units, embedding_dim, voc_size, batch_size, name = 'decoder',**kwargs):\n",
        "    super(Decoder, self).__init__(name = name, **kwargs)\n",
        "    self.dec_units = dec_units\n",
        "    self.batch_size = batch_size\n",
        "    self.embedding_layer = tf.keras.layers.Embedding(input_dim = voc_size, output_dim = embedding_dim, name = 'decoder_embedding')\n",
        "    self.gru_layer = tf.keras.layers.GRU(units = self.dec_units, kernel_initializer = 'glorot_uniform',\n",
        "                                         return_state = True, return_sequences = True,\n",
        "                                         recurrent_dropout = 0.5, dropout = 0.5,\n",
        "                                         name = 'decoder_gru')\n",
        "    self.fc = tf.keras.layers.Dense(units = voc_size, kernel_initializer = 'random_normal', activation = 'softmax',name = 'decoder_output')\n",
        "\n",
        "  def call(self, inputs, enc_hidden, enc_out):\n",
        "    inputs = self.embedding_layer(inputs)\n",
        "    dec_out, dec_hidden = self.gru_layer(inputs, initial_state = enc_hidden)\n",
        "    dec_out = tf.reshape(dec_out, shape = (-1, dec_out.shape[2]))\n",
        "    inputs = self.fc(dec_out)\n",
        "    return inputs, dec_hidden"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTFV3BH2g_kv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96ce81f0-d6a2-4b14-b9fb-1cb029776adf"
      },
      "source": [
        "#Instantiating and testing the decoder class using the sample batch:\n",
        "decoder = Decoder(units, embedding_dim, output_voc_size,batch_size,name = 'decoder')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer decoder_gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUCsyajZhQ_5"
      },
      "source": [
        "sample_dec_out, sample_dec_hidden = decoder(tf.random.uniform(shape = (batch_size, 1)), sample_enc_hidden, sample_enc_out)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aJlcBMQh2wd",
        "outputId": "61c9a106-8f33-4dbe-984d-a580d21a1856"
      },
      "source": [
        "print(f\"sample_dec_out_shape: {sample_dec_out.shape}\\nsample_dec_hidden_shape: {sample_dec_hidden.shape}\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_dec_out_shape: (64, 10785)\n",
            "sample_dec_hidden_shape: (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFwo5GI8iF3g"
      },
      "source": [
        "#The dot-product attention:\n",
        "class DotprtAttention(tf.keras.layers.Layer):\n",
        "  def call(self, query, values):\n",
        "    query_expanded = tf.expand_dims(query, 1) #Adding the time dimension to the encoder's hidden\n",
        "    score = query_expanded * values\n",
        "    score = tf.reduce_sum(score,axis = 2)\n",
        "    score = tf.expand_dims(score, axis = 2)\n",
        "    attention_wts = tf.nn.softmax(score, axis = 1)\n",
        "    context = attention_wts * values\n",
        "    context_vector = tf.reduce_sum(context, axis = 1)\n",
        "    return context_vector, attention_wts"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97hCMJWcjhxq",
        "outputId": "a7154be7-d438-45a8-f09f-54bc4e98bd22"
      },
      "source": [
        "#Instantiating and testing the attention class using the sample batch:\n",
        "dotproduct = DotprtAttention()\n",
        "sample_context_vector, sample_attention_wts = dotproduct(sample_enc_hidden, sample_enc_out)\n",
        "print(f\"sample_context_vector_shape: {sample_context_vector.shape}\\nsample_attention_wt_shape: {sample_attention_wts.shape}\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_context_vector_shape: (64, 1024)\n",
            "sample_attention_wt_shape: (64, 20, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPrIGIQqkPdO"
      },
      "source": [
        "#Bhanadau's-Additive attention: \n",
        "class Additive_Attention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, name = 'additive_attention', **kwargs):\n",
        "    super(Additive_Attention, self).__init__(name = name, **kwargs)\n",
        "    self.W1 = tf.keras.layers.Dense(units = units, kernel_initializer = 'random_normal')\n",
        "    self.W2 = tf.keras.layers.Dense(units = units, kernel_initializer = 'random_normal')\n",
        "    self.V = tf.keras.layers.Dense(units = 1, kernel_initializer = 'random_normal')\n",
        "  \n",
        "  def call(self, query, values):\n",
        "    expanded_query = tf.expand_dims(query, axis = 1)\n",
        "    score = self.V(tf.nn.tanh(self.W1(values) + self.W2(expanded_query)))\n",
        "    attention_wts = tf.nn.softmax(score, axis = 1)\n",
        "    context = attention_wts * values\n",
        "    contenxt_vector = tf.reduce_sum(context, axis = 1)\n",
        "    return contenxt_vector, attention_wts"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1zGKk6UnnPT",
        "outputId": "1bf20991-6692-4e29-d511-b81c3cc3d896"
      },
      "source": [
        "#Instantiate and testing the Bhanadau's attention using the sample batch:\n",
        "additive = Additive_Attention(units = 10)\n",
        "sample_context_vector_bn, sample_attention_wts_bn = additive(sample_enc_hidden, sample_enc_out)\n",
        "print(f\"sample_context_vector_bn_shape: {sample_context_vector_bn.shape}\\nsample_attention_wts_bn: {sample_attention_wts_bn.shape}\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_context_vector_bn_shape: (64, 1024)\n",
            "sample_attention_wts_bn: (64, 20, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuygj21NoeY2"
      },
      "source": [
        "#Decoder_with Attention:\n",
        "class DecoderWithAttention(tf.keras.Model):\n",
        "  def __init__(self, dec_units, voc_size, batch_size,embedding_dim,attention_layer = None, name = 'decoder_with_attention', **kwargs):\n",
        "    super(DecoderWithAttention, self). __init__(self, name = name, **kwargs)\n",
        "    self.dec_units = dec_units\n",
        "    self.batch_size = batch_size\n",
        "    self.embedding_layer = tf.keras.layers.Embedding(input_dim = voc_size, output_dim = embedding_dim, name = 'embedding_dec')\n",
        "    self.gru_layer = tf.keras.layers.GRU(units = self.dec_units, kernel_initializer = 'glorot_uniform',\n",
        "                                         return_state = True, return_sequences = True, \n",
        "                                         recurrent_dropout = 0.5, dropout = 0.5,\n",
        "                                         name = 'gru_dec')\n",
        "    self.fc = tf.keras.layers.Dense(units = voc_size, activation = 'softmax', name = 'dec_outputs')\n",
        "    self.attention = attention_layer\n",
        "  \n",
        "  def call(self, inputs, enc_hidden, enc_out):\n",
        "    inputs = self.embedding_layer(inputs)\n",
        "    attention_wts = None\n",
        "    if self.attention:\n",
        "      context_vector, attention_wts = self.attention(enc_hidden, enc_out)\n",
        "      inputs = tf.concat([tf.expand_dims(context_vector, axis = 1), inputs], axis = -1)\n",
        "    dec_out, dec_hidden = self.gru_layer(inputs, initial_state = enc_hidden)\n",
        "    dec_out = tf.reshape(dec_out, shape = (-1, dec_out.shape[2]))\n",
        "    inputs = self.fc(dec_out)\n",
        "    return inputs, dec_hidden, attention_wts\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx-uoE0Mv0TB"
      },
      "source": [
        "#Instantiating and testing the decoder with attention class using the sample batch"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubAdDk4vv-Qv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caa01e61-9258-4d10-e55b-b958d31961ac"
      },
      "source": [
        "decoder_with_attention = DecoderWithAttention(units, output_voc_size,batch_size, embedding_dim,attention_layer= additive)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru_dec will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeIIbKBIwUr-"
      },
      "source": [
        "sample_dec_out_with_attn, sample_dec_hidden_with_attn, attention_wts = decoder_with_attention(tf.random.uniform(shape = (batch_size, 1)), sample_enc_hidden, sample_enc_out)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq-VMrbaw4fJ",
        "outputId": "e84075db-089d-401a-d2d7-f6e1cb4bafc6"
      },
      "source": [
        "print(f\"sample_dec_out_shape: {sample_dec_out_with_attn.shape}\\nsample_dec_hidden_shape: {sample_dec_hidden_with_attn.shape}\\nsample_attn_wt_shape: {sample_attention_wts.shape}\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_dec_out_shape: (64, 10785)\n",
            "sample_dec_hidden_shape: (64, 1024)\n",
            "sample_attn_wt_shape: (64, 20, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEtLzHfNymVM"
      },
      "source": [
        "#The training Loop from the scratch"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rppbzU4uBPEQ",
        "outputId": "7fddc9cf-ebe8-4160-b34f-61f3d24dddba"
      },
      "source": [
        "#Get the loss object, customize the loss function and prepapare the optimizer:\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = 'none')\n",
        "\n",
        "def custom_loss(y_real, y_pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(y_real,0))\n",
        "  loss_ = loss_object(y_real, y_pred)\n",
        "  mask = tf.cast(mask, dtype = loss_.dtype)\n",
        "  loss_*=mask\n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "print(f\"Uncostomized loss: {loss_object([1.2,0.9],[[0.3,0.7,0.9,1.0],[0.5,0.8,1.0,0.3]])}\")\n",
        "print(f\"Customized_loss: {custom_loss([1.2,0.9],[[0.3,0.7,0.9,1.0],[0.5,0.8,1.0,0.3]])}\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uncostomized loss: [1.4449363 1.5722159]\n",
            "Customized_loss: 1.5085761547088623\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKGlygfY8viY"
      },
      "source": [
        "def get_training_step():\n",
        "  @tf.function\n",
        "  def train_step(inputs, outputs, enc_hidden,encoder, decoder):\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "      enc_out, enc_hidden = encoder(inputs, enc_hidden)\n",
        "      dec_hidden = enc_hidden\n",
        "      dec_inputs = tf.expand_dims([output_tokenizer.word_index['<start>']] * batch_size, axis = 1)\n",
        "      for t in range(1, outputs.shape[1]):\n",
        "        preds, dec_hidden, attention_wts = decoder(dec_inputs, dec_hidden, enc_out)\n",
        "        loss+=custom_loss(outputs[:,t], preds)\n",
        "        dec_inputs = tf.expand_dims(outputs[:, t], 1)\n",
        "    batch_loss = (loss / int(outputs.shape[1]))\n",
        "    trainable_vars = encoder.trainable_variables + decoder.trainable_variables\n",
        "    grads = tape.gradient(loss, trainable_vars)\n",
        "    optimizer.apply_gradients(zip(grads, trainable_vars))\n",
        "    return batch_loss\n",
        "  return train_step \n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzA6f6PsG_ke"
      },
      "source": [
        "#Define the function to compute validation loss:"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37YbT76fHFtV"
      },
      "source": [
        "def get_validation_loss(inputs, outputs,enc_hidden,encoder,decoder):\n",
        "  val_loss = 0\n",
        "  enc_out, enc_hidden = encoder(inputs,enc_hidden)\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_inputs = tf.expand_dims([output_tokenizer.word_index['<start>']] * batch_size, 1)\n",
        "  for t in range(1, outputs.shape[1]):\n",
        "    preds, dec_hidden, attention_wts = decoder(dec_inputs, dec_hidden, enc_out)\n",
        "    val_loss+=custom_loss(outputs[:,t], preds)\n",
        "    dec_inputs = tf.expand_dims(outputs[:,t],1)\n",
        "  val_loss = (val_loss / int(outputs.shape[1]))\n",
        "  return val_loss "
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlmnJ4YcJ7QA"
      },
      "source": [
        "#Training the model:\n",
        "def train_mtn_model(epochs, attention):\n",
        "  encoder = Encoder(units, embedding_dim,input_voc_size,batch_size, name = 'encoder')\n",
        "  decoder = DecoderWithAttention(units, output_voc_size, batch_size,embedding_dim, attention_layer = attention)\n",
        "  train_fn = get_training_step()\n",
        "  train_loss = []\n",
        "  validation_loss = []\n",
        "  for epoch in range(epochs):\n",
        "    tic = time.time()\n",
        "    enc_hidden = encoder.hidden_initializer()\n",
        "    total_loss = 0\n",
        "    for (steps, (input, output)) in enumerate(train_data.take(steps_per_epoch_train)):\n",
        "      batch_loss = train_fn(input, output,enc_hidden,encoder, decoder)\n",
        "      total_loss+=batch_loss\n",
        "      if steps % 100 == 0:\n",
        "        print(f\"Epoch: {epoch + 1}: Batch: {steps}: Loss: {batch_loss:.4f}\")\n",
        "    enc_hidden = encoder.hidden_initializer()\n",
        "    total_val_loss = 0\n",
        "    for (steps, (input, output)) in enumerate(validation_data.take(steps_per_epoch_val)):\n",
        "      val_loss = get_validation_loss(input, output,enc_hidden,encoder, decoder)\n",
        "      total_val_loss+=val_loss\n",
        "    train_loss.append(total_loss/steps_per_epoch_train)\n",
        "    validation_loss.append(total_val_loss/steps_per_epoch_val)\n",
        "    print(f\"At epoch: {epoch+ 1}: training loss is: {train_loss[-1]:.4f}: validation loss is: {validation_loss[-1]:.4f}\")\n",
        "    print(time_fmt(time.time() - tic))\n",
        "  return encoder, decoder, train_loss, validation_loss\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DamEEYCPJEk"
      },
      "source": [
        "#Train without attention:"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RHgJPA9PNBI",
        "outputId": "df9c17dd-8ba1-4aa5-ae68-6cdc31df717f"
      },
      "source": [
        "epochs = 15\n",
        "attention = None\n",
        "train_mtn_model(epochs, attention)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer decoder_gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru_dec will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Epoch: 1: Batch: 0: Loss: 4.4037\n",
            "Epoch: 1: Batch: 100: Loss: 2.3943\n",
            "Epoch: 1: Batch: 200: Loss: 1.8535\n",
            "Epoch: 1: Batch: 300: Loss: 1.6135\n",
            "Epoch: 1: Batch: 400: Loss: 1.6292\n",
            "Epoch: 1: Batch: 500: Loss: 1.3531\n",
            "Epoch: 1: Batch: 600: Loss: 1.3853\n",
            "Epoch: 1: Batch: 700: Loss: 1.2677\n",
            "Epoch: 1: Batch: 800: Loss: 1.0937\n",
            "Epoch: 1: Batch: 900: Loss: 0.9646\n",
            "Epoch: 1: Batch: 1000: Loss: 1.0725\n",
            "Epoch: 1: Batch: 1100: Loss: 1.1077\n",
            "Epoch: 1: Batch: 1200: Loss: 1.0820\n",
            "Epoch: 1: Batch: 1300: Loss: 0.9843\n",
            "Epoch: 1: Batch: 1400: Loss: 0.9715\n",
            "At epoch: 1: training loss is: 1.4310: validation loss is: 0.9277\n",
            "0: 005: 29.00\n",
            "Epoch: 2: Batch: 0: Loss: 0.7463\n",
            "Epoch: 2: Batch: 100: Loss: 0.8139\n",
            "Epoch: 2: Batch: 200: Loss: 0.6730\n",
            "Epoch: 2: Batch: 300: Loss: 0.8752\n",
            "Epoch: 2: Batch: 400: Loss: 0.7149\n",
            "Epoch: 2: Batch: 500: Loss: 0.7079\n",
            "Epoch: 2: Batch: 600: Loss: 0.7404\n",
            "Epoch: 2: Batch: 700: Loss: 0.6102\n",
            "Epoch: 2: Batch: 800: Loss: 0.6862\n",
            "Epoch: 2: Batch: 900: Loss: 0.5521\n",
            "Epoch: 2: Batch: 1000: Loss: 0.6308\n",
            "Epoch: 2: Batch: 1100: Loss: 0.6607\n",
            "Epoch: 2: Batch: 1200: Loss: 0.6011\n",
            "Epoch: 2: Batch: 1300: Loss: 0.5812\n",
            "Epoch: 2: Batch: 1400: Loss: 0.5896\n",
            "At epoch: 2: training loss is: 0.6767: validation loss is: 0.6538\n",
            "0: 005: 13.00\n",
            "Epoch: 3: Batch: 0: Loss: 0.3615\n",
            "Epoch: 3: Batch: 100: Loss: 0.3720\n",
            "Epoch: 3: Batch: 200: Loss: 0.3095\n",
            "Epoch: 3: Batch: 300: Loss: 0.4255\n",
            "Epoch: 3: Batch: 400: Loss: 0.3686\n",
            "Epoch: 3: Batch: 500: Loss: 0.3769\n",
            "Epoch: 3: Batch: 600: Loss: 0.3793\n",
            "Epoch: 3: Batch: 700: Loss: 0.4277\n",
            "Epoch: 3: Batch: 800: Loss: 0.3418\n",
            "Epoch: 3: Batch: 900: Loss: 0.3385\n",
            "Epoch: 3: Batch: 1000: Loss: 0.3325\n",
            "Epoch: 3: Batch: 1100: Loss: 0.3377\n",
            "Epoch: 3: Batch: 1200: Loss: 0.3855\n",
            "Epoch: 3: Batch: 1300: Loss: 0.4088\n",
            "Epoch: 3: Batch: 1400: Loss: 0.3470\n",
            "At epoch: 3: training loss is: 0.3821: validation loss is: 0.5825\n",
            "0: 005: 10.00\n",
            "Epoch: 4: Batch: 0: Loss: 0.2388\n",
            "Epoch: 4: Batch: 100: Loss: 0.1993\n",
            "Epoch: 4: Batch: 200: Loss: 0.1780\n",
            "Epoch: 4: Batch: 300: Loss: 0.2053\n",
            "Epoch: 4: Batch: 400: Loss: 0.1975\n",
            "Epoch: 4: Batch: 500: Loss: 0.2668\n",
            "Epoch: 4: Batch: 600: Loss: 0.2452\n",
            "Epoch: 4: Batch: 700: Loss: 0.2634\n",
            "Epoch: 4: Batch: 800: Loss: 0.2589\n",
            "Epoch: 4: Batch: 900: Loss: 0.2261\n",
            "Epoch: 4: Batch: 1000: Loss: 0.2293\n",
            "Epoch: 4: Batch: 1100: Loss: 0.2333\n",
            "Epoch: 4: Batch: 1200: Loss: 0.2697\n",
            "Epoch: 4: Batch: 1300: Loss: 0.2750\n",
            "Epoch: 4: Batch: 1400: Loss: 0.2741\n",
            "At epoch: 4: training loss is: 0.2301: validation loss is: 0.5739\n",
            "0: 005: 08.00\n",
            "Epoch: 5: Batch: 0: Loss: 0.1245\n",
            "Epoch: 5: Batch: 100: Loss: 0.1622\n",
            "Epoch: 5: Batch: 200: Loss: 0.1320\n",
            "Epoch: 5: Batch: 300: Loss: 0.1618\n",
            "Epoch: 5: Batch: 400: Loss: 0.1339\n",
            "Epoch: 5: Batch: 500: Loss: 0.1174\n",
            "Epoch: 5: Batch: 600: Loss: 0.1717\n",
            "Epoch: 5: Batch: 700: Loss: 0.1517\n",
            "Epoch: 5: Batch: 800: Loss: 0.1746\n",
            "Epoch: 5: Batch: 900: Loss: 0.1393\n",
            "Epoch: 5: Batch: 1000: Loss: 0.1606\n",
            "Epoch: 5: Batch: 1100: Loss: 0.1614\n",
            "Epoch: 5: Batch: 1200: Loss: 0.1735\n",
            "Epoch: 5: Batch: 1300: Loss: 0.1980\n",
            "Epoch: 5: Batch: 1400: Loss: 0.1532\n",
            "At epoch: 5: training loss is: 0.1532: validation loss is: 0.5867\n",
            "0: 005: 09.00\n",
            "Epoch: 6: Batch: 0: Loss: 0.1014\n",
            "Epoch: 6: Batch: 100: Loss: 0.0915\n",
            "Epoch: 6: Batch: 200: Loss: 0.0772\n",
            "Epoch: 6: Batch: 300: Loss: 0.0697\n",
            "Epoch: 6: Batch: 400: Loss: 0.0917\n",
            "Epoch: 6: Batch: 500: Loss: 0.1031\n",
            "Epoch: 6: Batch: 600: Loss: 0.0952\n",
            "Epoch: 6: Batch: 700: Loss: 0.1065\n",
            "Epoch: 6: Batch: 800: Loss: 0.1251\n",
            "Epoch: 6: Batch: 900: Loss: 0.1101\n",
            "Epoch: 6: Batch: 1000: Loss: 0.1159\n",
            "Epoch: 6: Batch: 1100: Loss: 0.1305\n",
            "Epoch: 6: Batch: 1200: Loss: 0.1379\n",
            "Epoch: 6: Batch: 1300: Loss: 0.1231\n",
            "Epoch: 6: Batch: 1400: Loss: 0.1501\n",
            "At epoch: 6: training loss is: 0.1124: validation loss is: 0.6065\n",
            "0: 005: 09.00\n",
            "Epoch: 7: Batch: 0: Loss: 0.0981\n",
            "Epoch: 7: Batch: 100: Loss: 0.0650\n",
            "Epoch: 7: Batch: 200: Loss: 0.0680\n",
            "Epoch: 7: Batch: 300: Loss: 0.0790\n",
            "Epoch: 7: Batch: 400: Loss: 0.0864\n",
            "Epoch: 7: Batch: 500: Loss: 0.0742\n",
            "Epoch: 7: Batch: 600: Loss: 0.1150\n",
            "Epoch: 7: Batch: 700: Loss: 0.0923\n",
            "Epoch: 7: Batch: 800: Loss: 0.0812\n",
            "Epoch: 7: Batch: 900: Loss: 0.1029\n",
            "Epoch: 7: Batch: 1000: Loss: 0.0740\n",
            "Epoch: 7: Batch: 1100: Loss: 0.0766\n",
            "Epoch: 7: Batch: 1200: Loss: 0.0975\n",
            "Epoch: 7: Batch: 1300: Loss: 0.1213\n",
            "Epoch: 7: Batch: 1400: Loss: 0.1091\n",
            "At epoch: 7: training loss is: 0.0901: validation loss is: 0.6214\n",
            "0: 005: 08.00\n",
            "Epoch: 8: Batch: 0: Loss: 0.0523\n",
            "Epoch: 8: Batch: 100: Loss: 0.0643\n",
            "Epoch: 8: Batch: 200: Loss: 0.0614\n",
            "Epoch: 8: Batch: 300: Loss: 0.0841\n",
            "Epoch: 8: Batch: 400: Loss: 0.0451\n",
            "Epoch: 8: Batch: 500: Loss: 0.0924\n",
            "Epoch: 8: Batch: 600: Loss: 0.0913\n",
            "Epoch: 8: Batch: 700: Loss: 0.0982\n",
            "Epoch: 8: Batch: 800: Loss: 0.0784\n",
            "Epoch: 8: Batch: 900: Loss: 0.0602\n",
            "Epoch: 8: Batch: 1000: Loss: 0.0855\n",
            "Epoch: 8: Batch: 1100: Loss: 0.0781\n",
            "Epoch: 8: Batch: 1200: Loss: 0.1201\n",
            "Epoch: 8: Batch: 1300: Loss: 0.0828\n",
            "Epoch: 8: Batch: 1400: Loss: 0.0967\n",
            "At epoch: 8: training loss is: 0.0764: validation loss is: 0.6490\n",
            "0: 005: 09.00\n",
            "Epoch: 9: Batch: 0: Loss: 0.1010\n",
            "Epoch: 9: Batch: 100: Loss: 0.0521\n",
            "Epoch: 9: Batch: 200: Loss: 0.0486\n",
            "Epoch: 9: Batch: 300: Loss: 0.0583\n",
            "Epoch: 9: Batch: 400: Loss: 0.0509\n",
            "Epoch: 9: Batch: 500: Loss: 0.0770\n",
            "Epoch: 9: Batch: 600: Loss: 0.0499\n",
            "Epoch: 9: Batch: 700: Loss: 0.0696\n",
            "Epoch: 9: Batch: 800: Loss: 0.0602\n",
            "Epoch: 9: Batch: 900: Loss: 0.0488\n",
            "Epoch: 9: Batch: 1000: Loss: 0.0736\n",
            "Epoch: 9: Batch: 1100: Loss: 0.0514\n",
            "Epoch: 9: Batch: 1200: Loss: 0.0627\n",
            "Epoch: 9: Batch: 1300: Loss: 0.1100\n",
            "Epoch: 9: Batch: 1400: Loss: 0.1008\n",
            "At epoch: 9: training loss is: 0.0678: validation loss is: 0.6749\n",
            "0: 005: 08.00\n",
            "Epoch: 10: Batch: 0: Loss: 0.0423\n",
            "Epoch: 10: Batch: 100: Loss: 0.0542\n",
            "Epoch: 10: Batch: 200: Loss: 0.0496\n",
            "Epoch: 10: Batch: 300: Loss: 0.0658\n",
            "Epoch: 10: Batch: 400: Loss: 0.0408\n",
            "Epoch: 10: Batch: 500: Loss: 0.0438\n",
            "Epoch: 10: Batch: 600: Loss: 0.0525\n",
            "Epoch: 10: Batch: 700: Loss: 0.0470\n",
            "Epoch: 10: Batch: 800: Loss: 0.0384\n",
            "Epoch: 10: Batch: 900: Loss: 0.0879\n",
            "Epoch: 10: Batch: 1000: Loss: 0.0458\n",
            "Epoch: 10: Batch: 1100: Loss: 0.0892\n",
            "Epoch: 10: Batch: 1200: Loss: 0.0827\n",
            "Epoch: 10: Batch: 1300: Loss: 0.0751\n",
            "Epoch: 10: Batch: 1400: Loss: 0.0821\n",
            "At epoch: 10: training loss is: 0.0629: validation loss is: 0.6909\n",
            "0: 005: 09.00\n",
            "Epoch: 11: Batch: 0: Loss: 0.0450\n",
            "Epoch: 11: Batch: 100: Loss: 0.0473\n",
            "Epoch: 11: Batch: 200: Loss: 0.0451\n",
            "Epoch: 11: Batch: 300: Loss: 0.0522\n",
            "Epoch: 11: Batch: 400: Loss: 0.0814\n",
            "Epoch: 11: Batch: 500: Loss: 0.0353\n",
            "Epoch: 11: Batch: 600: Loss: 0.0652\n",
            "Epoch: 11: Batch: 700: Loss: 0.0565\n",
            "Epoch: 11: Batch: 800: Loss: 0.0651\n",
            "Epoch: 11: Batch: 900: Loss: 0.0462\n",
            "Epoch: 11: Batch: 1000: Loss: 0.0658\n",
            "Epoch: 11: Batch: 1100: Loss: 0.0590\n",
            "Epoch: 11: Batch: 1200: Loss: 0.0434\n",
            "Epoch: 11: Batch: 1300: Loss: 0.0841\n",
            "Epoch: 11: Batch: 1400: Loss: 0.0738\n",
            "At epoch: 11: training loss is: 0.0591: validation loss is: 0.7061\n",
            "0: 005: 10.00\n",
            "Epoch: 12: Batch: 0: Loss: 0.0426\n",
            "Epoch: 12: Batch: 100: Loss: 0.0390\n",
            "Epoch: 12: Batch: 200: Loss: 0.0480\n",
            "Epoch: 12: Batch: 300: Loss: 0.0342\n",
            "Epoch: 12: Batch: 400: Loss: 0.0585\n",
            "Epoch: 12: Batch: 500: Loss: 0.0452\n",
            "Epoch: 12: Batch: 600: Loss: 0.0613\n",
            "Epoch: 12: Batch: 700: Loss: 0.0564\n",
            "Epoch: 12: Batch: 800: Loss: 0.0691\n",
            "Epoch: 12: Batch: 900: Loss: 0.0670\n",
            "Epoch: 12: Batch: 1000: Loss: 0.0499\n",
            "Epoch: 12: Batch: 1100: Loss: 0.0728\n",
            "Epoch: 12: Batch: 1200: Loss: 0.0758\n",
            "Epoch: 12: Batch: 1300: Loss: 0.0581\n",
            "Epoch: 12: Batch: 1400: Loss: 0.0850\n",
            "At epoch: 12: training loss is: 0.0556: validation loss is: 0.7280\n",
            "0: 005: 10.00\n",
            "Epoch: 13: Batch: 0: Loss: 0.0427\n",
            "Epoch: 13: Batch: 100: Loss: 0.0554\n",
            "Epoch: 13: Batch: 200: Loss: 0.0323\n",
            "Epoch: 13: Batch: 300: Loss: 0.0720\n",
            "Epoch: 13: Batch: 400: Loss: 0.0648\n",
            "Epoch: 13: Batch: 500: Loss: 0.0456\n",
            "Epoch: 13: Batch: 600: Loss: 0.0504\n",
            "Epoch: 13: Batch: 700: Loss: 0.0662\n",
            "Epoch: 13: Batch: 800: Loss: 0.0593\n",
            "Epoch: 13: Batch: 900: Loss: 0.0471\n",
            "Epoch: 13: Batch: 1000: Loss: 0.0631\n",
            "Epoch: 13: Batch: 1100: Loss: 0.0590\n",
            "Epoch: 13: Batch: 1200: Loss: 0.0496\n",
            "Epoch: 13: Batch: 1300: Loss: 0.0654\n",
            "Epoch: 13: Batch: 1400: Loss: 0.0916\n",
            "At epoch: 13: training loss is: 0.0539: validation loss is: 0.7399\n",
            "0: 005: 10.00\n",
            "Epoch: 14: Batch: 0: Loss: 0.0484\n",
            "Epoch: 14: Batch: 100: Loss: 0.0315\n",
            "Epoch: 14: Batch: 200: Loss: 0.0346\n",
            "Epoch: 14: Batch: 300: Loss: 0.0336\n",
            "Epoch: 14: Batch: 400: Loss: 0.0275\n",
            "Epoch: 14: Batch: 500: Loss: 0.0449\n",
            "Epoch: 14: Batch: 600: Loss: 0.0721\n",
            "Epoch: 14: Batch: 700: Loss: 0.0415\n",
            "Epoch: 14: Batch: 800: Loss: 0.0796\n",
            "Epoch: 14: Batch: 900: Loss: 0.0568\n",
            "Epoch: 14: Batch: 1000: Loss: 0.0587\n",
            "Epoch: 14: Batch: 1100: Loss: 0.0590\n",
            "Epoch: 14: Batch: 1200: Loss: 0.0734\n",
            "Epoch: 14: Batch: 1300: Loss: 0.0714\n",
            "Epoch: 14: Batch: 1400: Loss: 0.0696\n",
            "At epoch: 14: training loss is: 0.0526: validation loss is: 0.7604\n",
            "0: 005: 12.00\n",
            "Epoch: 15: Batch: 0: Loss: 0.0659\n",
            "Epoch: 15: Batch: 100: Loss: 0.0182\n",
            "Epoch: 15: Batch: 200: Loss: 0.0437\n",
            "Epoch: 15: Batch: 300: Loss: 0.0336\n",
            "Epoch: 15: Batch: 400: Loss: 0.0468\n",
            "Epoch: 15: Batch: 500: Loss: 0.1313\n",
            "Epoch: 15: Batch: 600: Loss: 0.0491\n",
            "Epoch: 15: Batch: 700: Loss: 0.0379\n",
            "Epoch: 15: Batch: 800: Loss: 0.0291\n",
            "Epoch: 15: Batch: 900: Loss: 0.0424\n",
            "Epoch: 15: Batch: 1000: Loss: 0.0587\n",
            "Epoch: 15: Batch: 1100: Loss: 0.0678\n",
            "Epoch: 15: Batch: 1200: Loss: 0.0513\n",
            "Epoch: 15: Batch: 1300: Loss: 0.0607\n",
            "Epoch: 15: Batch: 1400: Loss: 0.0825\n",
            "At epoch: 15: training loss is: 0.0514: validation loss is: 0.7736\n",
            "0: 005: 12.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<__main__.Encoder at 0x7f532e348f28>,\n",
              " <__main__.DecoderWithAttention at 0x7f532e2b9cc0>,\n",
              " [<tf.Tensor: shape=(), dtype=float32, numpy=1.431033>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.6767448>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.3821498>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.23012912>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.15318148>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.1124177>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.09006567>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.07641119>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.06780918>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.06294307>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.059121843>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.055616636>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.053861495>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.05259737>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.051350623>],\n",
              " [<tf.Tensor: shape=(), dtype=float32, numpy=0.9277065>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.653818>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.5825367>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.5739366>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.58671373>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.6064918>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.62135905>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.64901996>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.674872>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.69088465>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.7060905>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.72797525>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.73994744>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.76039684>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.7736032>])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I0HJGtEPrrG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68b8ab08-a092-4a1f-c1e1-a5f927e7951c"
      },
      "source": [
        "epochs = 15\n",
        "attention = dotproduct\n",
        "train_mtn_model(epochs, attention)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer decoder_gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru_dec will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Epoch: 1: Batch: 0: Loss: 4.2670\n",
            "Epoch: 1: Batch: 100: Loss: 2.2518\n",
            "Epoch: 1: Batch: 200: Loss: 1.7869\n",
            "Epoch: 1: Batch: 300: Loss: 1.4391\n",
            "Epoch: 1: Batch: 400: Loss: 1.3762\n",
            "Epoch: 1: Batch: 500: Loss: 1.3870\n",
            "Epoch: 1: Batch: 600: Loss: 1.3819\n",
            "Epoch: 1: Batch: 700: Loss: 1.3010\n",
            "Epoch: 1: Batch: 800: Loss: 1.3333\n",
            "Epoch: 1: Batch: 900: Loss: 1.0212\n",
            "Epoch: 1: Batch: 1000: Loss: 1.1368\n",
            "Epoch: 1: Batch: 1100: Loss: 0.9467\n",
            "Epoch: 1: Batch: 1200: Loss: 0.9509\n",
            "Epoch: 1: Batch: 1300: Loss: 0.8669\n",
            "Epoch: 1: Batch: 1400: Loss: 0.9145\n",
            "At epoch: 1: training loss is: 1.3911: validation loss is: 0.9129\n",
            "0: 006: 12.00\n",
            "Epoch: 2: Batch: 0: Loss: 0.7150\n",
            "Epoch: 2: Batch: 100: Loss: 0.6572\n",
            "Epoch: 2: Batch: 200: Loss: 0.7748\n",
            "Epoch: 2: Batch: 300: Loss: 0.7619\n",
            "Epoch: 2: Batch: 400: Loss: 0.7566\n",
            "Epoch: 2: Batch: 500: Loss: 0.6824\n",
            "Epoch: 2: Batch: 600: Loss: 0.6610\n",
            "Epoch: 2: Batch: 700: Loss: 0.6902\n",
            "Epoch: 2: Batch: 800: Loss: 0.6286\n",
            "Epoch: 2: Batch: 900: Loss: 0.7525\n",
            "Epoch: 2: Batch: 1000: Loss: 0.5774\n",
            "Epoch: 2: Batch: 1100: Loss: 0.7489\n",
            "Epoch: 2: Batch: 1200: Loss: 0.6847\n",
            "Epoch: 2: Batch: 1300: Loss: 0.6170\n",
            "Epoch: 2: Batch: 1400: Loss: 0.5793\n",
            "At epoch: 2: training loss is: 0.6800: validation loss is: 0.6637\n",
            "0: 005: 50.00\n",
            "Epoch: 3: Batch: 0: Loss: 0.3991\n",
            "Epoch: 3: Batch: 100: Loss: 0.3972\n",
            "Epoch: 3: Batch: 200: Loss: 0.3565\n",
            "Epoch: 3: Batch: 300: Loss: 0.3807\n",
            "Epoch: 3: Batch: 400: Loss: 0.4640\n",
            "Epoch: 3: Batch: 500: Loss: 0.4389\n",
            "Epoch: 3: Batch: 600: Loss: 0.5221\n",
            "Epoch: 3: Batch: 700: Loss: 0.3750\n",
            "Epoch: 3: Batch: 800: Loss: 0.4161\n",
            "Epoch: 3: Batch: 900: Loss: 0.4332\n",
            "Epoch: 3: Batch: 1000: Loss: 0.4025\n",
            "Epoch: 3: Batch: 1100: Loss: 0.4475\n",
            "Epoch: 3: Batch: 1200: Loss: 0.4400\n",
            "Epoch: 3: Batch: 1300: Loss: 0.4093\n",
            "Epoch: 3: Batch: 1400: Loss: 0.3327\n",
            "At epoch: 3: training loss is: 0.4137: validation loss is: 0.5855\n",
            "0: 005: 50.00\n",
            "Epoch: 4: Batch: 0: Loss: 0.3547\n",
            "Epoch: 4: Batch: 100: Loss: 0.2411\n",
            "Epoch: 4: Batch: 200: Loss: 0.2533\n",
            "Epoch: 4: Batch: 300: Loss: 0.2359\n",
            "Epoch: 4: Batch: 400: Loss: 0.3361\n",
            "Epoch: 4: Batch: 500: Loss: 0.2860\n",
            "Epoch: 4: Batch: 600: Loss: 0.2411\n",
            "Epoch: 4: Batch: 700: Loss: 0.3087\n",
            "Epoch: 4: Batch: 800: Loss: 0.2515\n",
            "Epoch: 4: Batch: 900: Loss: 0.2998\n",
            "Epoch: 4: Batch: 1000: Loss: 0.2922\n",
            "Epoch: 4: Batch: 1100: Loss: 0.2974\n",
            "Epoch: 4: Batch: 1200: Loss: 0.2541\n",
            "Epoch: 4: Batch: 1300: Loss: 0.3061\n",
            "Epoch: 4: Batch: 1400: Loss: 0.3123\n",
            "At epoch: 4: training loss is: 0.2721: validation loss is: 0.5675\n",
            "0: 005: 49.00\n",
            "Epoch: 5: Batch: 0: Loss: 0.1534\n",
            "Epoch: 5: Batch: 100: Loss: 0.2186\n",
            "Epoch: 5: Batch: 200: Loss: 0.1588\n",
            "Epoch: 5: Batch: 300: Loss: 0.1661\n",
            "Epoch: 5: Batch: 400: Loss: 0.1866\n",
            "Epoch: 5: Batch: 500: Loss: 0.1785\n",
            "Epoch: 5: Batch: 600: Loss: 0.1854\n",
            "Epoch: 5: Batch: 700: Loss: 0.1581\n",
            "Epoch: 5: Batch: 800: Loss: 0.2168\n",
            "Epoch: 5: Batch: 900: Loss: 0.2120\n",
            "Epoch: 5: Batch: 1000: Loss: 0.1849\n",
            "Epoch: 5: Batch: 1100: Loss: 0.1680\n",
            "Epoch: 5: Batch: 1200: Loss: 0.2267\n",
            "Epoch: 5: Batch: 1300: Loss: 0.2121\n",
            "Epoch: 5: Batch: 1400: Loss: 0.2060\n",
            "At epoch: 5: training loss is: 0.1933: validation loss is: 0.5699\n",
            "0: 005: 49.00\n",
            "Epoch: 6: Batch: 0: Loss: 0.1224\n",
            "Epoch: 6: Batch: 100: Loss: 0.1414\n",
            "Epoch: 6: Batch: 200: Loss: 0.1293\n",
            "Epoch: 6: Batch: 300: Loss: 0.1498\n",
            "Epoch: 6: Batch: 400: Loss: 0.1223\n",
            "Epoch: 6: Batch: 500: Loss: 0.1255\n",
            "Epoch: 6: Batch: 600: Loss: 0.1719\n",
            "Epoch: 6: Batch: 700: Loss: 0.1475\n",
            "Epoch: 6: Batch: 800: Loss: 0.1361\n",
            "Epoch: 6: Batch: 900: Loss: 0.1368\n",
            "Epoch: 6: Batch: 1000: Loss: 0.1491\n",
            "Epoch: 6: Batch: 1100: Loss: 0.1475\n",
            "Epoch: 6: Batch: 1200: Loss: 0.1657\n",
            "Epoch: 6: Batch: 1300: Loss: 0.1174\n",
            "Epoch: 6: Batch: 1400: Loss: 0.1477\n",
            "At epoch: 6: training loss is: 0.1452: validation loss is: 0.5828\n",
            "0: 005: 49.00\n",
            "Epoch: 7: Batch: 0: Loss: 0.1056\n",
            "Epoch: 7: Batch: 100: Loss: 0.0956\n",
            "Epoch: 7: Batch: 200: Loss: 0.0931\n",
            "Epoch: 7: Batch: 300: Loss: 0.1334\n",
            "Epoch: 7: Batch: 400: Loss: 0.1063\n",
            "Epoch: 7: Batch: 500: Loss: 0.0845\n",
            "Epoch: 7: Batch: 600: Loss: 0.1031\n",
            "Epoch: 7: Batch: 700: Loss: 0.1140\n",
            "Epoch: 7: Batch: 800: Loss: 0.1152\n",
            "Epoch: 7: Batch: 900: Loss: 0.0941\n",
            "Epoch: 7: Batch: 1000: Loss: 0.1226\n",
            "Epoch: 7: Batch: 1100: Loss: 0.0910\n",
            "Epoch: 7: Batch: 1200: Loss: 0.1045\n",
            "Epoch: 7: Batch: 1300: Loss: 0.1794\n",
            "Epoch: 7: Batch: 1400: Loss: 0.1562\n",
            "At epoch: 7: training loss is: 0.1139: validation loss is: 0.6007\n",
            "0: 005: 48.00\n",
            "Epoch: 8: Batch: 0: Loss: 0.0862\n",
            "Epoch: 8: Batch: 100: Loss: 0.0798\n",
            "Epoch: 8: Batch: 200: Loss: 0.0859\n",
            "Epoch: 8: Batch: 300: Loss: 0.1067\n",
            "Epoch: 8: Batch: 400: Loss: 0.0582\n",
            "Epoch: 8: Batch: 500: Loss: 0.0751\n",
            "Epoch: 8: Batch: 600: Loss: 0.0997\n",
            "Epoch: 8: Batch: 700: Loss: 0.1352\n",
            "Epoch: 8: Batch: 800: Loss: 0.0735\n",
            "Epoch: 8: Batch: 900: Loss: 0.1246\n",
            "Epoch: 8: Batch: 1000: Loss: 0.0922\n",
            "Epoch: 8: Batch: 1100: Loss: 0.1070\n",
            "Epoch: 8: Batch: 1200: Loss: 0.1021\n",
            "Epoch: 8: Batch: 1300: Loss: 0.1039\n",
            "Epoch: 8: Batch: 1400: Loss: 0.1004\n",
            "At epoch: 8: training loss is: 0.0926: validation loss is: 0.6186\n",
            "0: 005: 49.00\n",
            "Epoch: 9: Batch: 0: Loss: 0.0865\n",
            "Epoch: 9: Batch: 100: Loss: 0.0501\n",
            "Epoch: 9: Batch: 200: Loss: 0.0624\n",
            "Epoch: 9: Batch: 300: Loss: 0.0434\n",
            "Epoch: 9: Batch: 400: Loss: 0.0719\n",
            "Epoch: 9: Batch: 500: Loss: 0.0630\n",
            "Epoch: 9: Batch: 600: Loss: 0.0624\n",
            "Epoch: 9: Batch: 700: Loss: 0.0667\n",
            "Epoch: 9: Batch: 800: Loss: 0.0812\n",
            "Epoch: 9: Batch: 900: Loss: 0.0934\n",
            "Epoch: 9: Batch: 1000: Loss: 0.0835\n",
            "Epoch: 9: Batch: 1100: Loss: 0.0721\n",
            "Epoch: 9: Batch: 1200: Loss: 0.1167\n",
            "Epoch: 9: Batch: 1300: Loss: 0.1028\n",
            "Epoch: 9: Batch: 1400: Loss: 0.0932\n",
            "At epoch: 9: training loss is: 0.0781: validation loss is: 0.6344\n",
            "0: 005: 50.00\n",
            "Epoch: 10: Batch: 0: Loss: 0.0651\n",
            "Epoch: 10: Batch: 100: Loss: 0.0542\n",
            "Epoch: 10: Batch: 200: Loss: 0.0678\n",
            "Epoch: 10: Batch: 300: Loss: 0.0836\n",
            "Epoch: 10: Batch: 400: Loss: 0.0467\n",
            "Epoch: 10: Batch: 500: Loss: 0.0731\n",
            "Epoch: 10: Batch: 600: Loss: 0.0530\n",
            "Epoch: 10: Batch: 700: Loss: 0.0642\n",
            "Epoch: 10: Batch: 800: Loss: 0.0801\n",
            "Epoch: 10: Batch: 900: Loss: 0.0902\n",
            "Epoch: 10: Batch: 1000: Loss: 0.0995\n",
            "Epoch: 10: Batch: 1100: Loss: 0.0974\n",
            "Epoch: 10: Batch: 1200: Loss: 0.0714\n",
            "Epoch: 10: Batch: 1300: Loss: 0.0901\n",
            "Epoch: 10: Batch: 1400: Loss: 0.0776\n",
            "At epoch: 10: training loss is: 0.0697: validation loss is: 0.6555\n",
            "0: 005: 49.00\n",
            "Epoch: 11: Batch: 0: Loss: 0.0387\n",
            "Epoch: 11: Batch: 100: Loss: 0.0452\n",
            "Epoch: 11: Batch: 200: Loss: 0.0531\n",
            "Epoch: 11: Batch: 300: Loss: 0.0569\n",
            "Epoch: 11: Batch: 400: Loss: 0.0497\n",
            "Epoch: 11: Batch: 500: Loss: 0.0413\n",
            "Epoch: 11: Batch: 600: Loss: 0.0713\n",
            "Epoch: 11: Batch: 700: Loss: 0.0533\n",
            "Epoch: 11: Batch: 800: Loss: 0.0738\n",
            "Epoch: 11: Batch: 900: Loss: 0.0703\n",
            "Epoch: 11: Batch: 1000: Loss: 0.0929\n",
            "Epoch: 11: Batch: 1100: Loss: 0.0689\n",
            "Epoch: 11: Batch: 1200: Loss: 0.0694\n",
            "Epoch: 11: Batch: 1300: Loss: 0.0617\n",
            "Epoch: 11: Batch: 1400: Loss: 0.0862\n",
            "At epoch: 11: training loss is: 0.0632: validation loss is: 0.6619\n",
            "0: 005: 48.00\n",
            "Epoch: 12: Batch: 0: Loss: 0.0412\n",
            "Epoch: 12: Batch: 100: Loss: 0.0716\n",
            "Epoch: 12: Batch: 200: Loss: 0.0496\n",
            "Epoch: 12: Batch: 300: Loss: 0.0369\n",
            "Epoch: 12: Batch: 400: Loss: 0.0603\n",
            "Epoch: 12: Batch: 500: Loss: 0.0503\n",
            "Epoch: 12: Batch: 600: Loss: 0.0771\n",
            "Epoch: 12: Batch: 700: Loss: 0.0494\n",
            "Epoch: 12: Batch: 800: Loss: 0.0574\n",
            "Epoch: 12: Batch: 900: Loss: 0.0477\n",
            "Epoch: 12: Batch: 1000: Loss: 0.0613\n",
            "Epoch: 12: Batch: 1100: Loss: 0.0554\n",
            "Epoch: 12: Batch: 1200: Loss: 0.0664\n",
            "Epoch: 12: Batch: 1300: Loss: 0.0893\n",
            "Epoch: 12: Batch: 1400: Loss: 0.0771\n",
            "At epoch: 12: training loss is: 0.0584: validation loss is: 0.6884\n",
            "0: 005: 48.00\n",
            "Epoch: 13: Batch: 0: Loss: 0.0409\n",
            "Epoch: 13: Batch: 100: Loss: 0.0383\n",
            "Epoch: 13: Batch: 200: Loss: 0.0416\n",
            "Epoch: 13: Batch: 300: Loss: 0.0371\n",
            "Epoch: 13: Batch: 400: Loss: 0.0510\n",
            "Epoch: 13: Batch: 500: Loss: 0.0573\n",
            "Epoch: 13: Batch: 600: Loss: 0.0575\n",
            "Epoch: 13: Batch: 700: Loss: 0.0274\n",
            "Epoch: 13: Batch: 800: Loss: 0.0721\n",
            "Epoch: 13: Batch: 900: Loss: 0.0581\n",
            "Epoch: 13: Batch: 1000: Loss: 0.0574\n",
            "Epoch: 13: Batch: 1100: Loss: 0.0499\n",
            "Epoch: 13: Batch: 1200: Loss: 0.0877\n",
            "Epoch: 13: Batch: 1300: Loss: 0.0916\n",
            "Epoch: 13: Batch: 1400: Loss: 0.0895\n",
            "At epoch: 13: training loss is: 0.0548: validation loss is: 0.7086\n",
            "0: 005: 49.00\n",
            "Epoch: 14: Batch: 0: Loss: 0.0463\n",
            "Epoch: 14: Batch: 100: Loss: 0.0446\n",
            "Epoch: 14: Batch: 200: Loss: 0.0207\n",
            "Epoch: 14: Batch: 300: Loss: 0.0498\n",
            "Epoch: 14: Batch: 400: Loss: 0.0456\n",
            "Epoch: 14: Batch: 500: Loss: 0.0500\n",
            "Epoch: 14: Batch: 600: Loss: 0.0512\n",
            "Epoch: 14: Batch: 700: Loss: 0.0553\n",
            "Epoch: 14: Batch: 800: Loss: 0.0690\n",
            "Epoch: 14: Batch: 900: Loss: 0.0446\n",
            "Epoch: 14: Batch: 1000: Loss: 0.0430\n",
            "Epoch: 14: Batch: 1100: Loss: 0.0525\n",
            "Epoch: 14: Batch: 1200: Loss: 0.0809\n",
            "Epoch: 14: Batch: 1300: Loss: 0.0523\n",
            "Epoch: 14: Batch: 1400: Loss: 0.0853\n",
            "At epoch: 14: training loss is: 0.0523: validation loss is: 0.7122\n",
            "0: 005: 49.00\n",
            "Epoch: 15: Batch: 0: Loss: 0.0488\n",
            "Epoch: 15: Batch: 100: Loss: 0.0312\n",
            "Epoch: 15: Batch: 200: Loss: 0.0254\n",
            "Epoch: 15: Batch: 300: Loss: 0.0328\n",
            "Epoch: 15: Batch: 400: Loss: 0.0384\n",
            "Epoch: 15: Batch: 500: Loss: 0.0377\n",
            "Epoch: 15: Batch: 600: Loss: 0.0452\n",
            "Epoch: 15: Batch: 700: Loss: 0.0498\n",
            "Epoch: 15: Batch: 800: Loss: 0.0354\n",
            "Epoch: 15: Batch: 900: Loss: 0.0472\n",
            "Epoch: 15: Batch: 1000: Loss: 0.0543\n",
            "Epoch: 15: Batch: 1100: Loss: 0.0618\n",
            "Epoch: 15: Batch: 1200: Loss: 0.0675\n",
            "Epoch: 15: Batch: 1300: Loss: 0.0359\n",
            "Epoch: 15: Batch: 1400: Loss: 0.0591\n",
            "At epoch: 15: training loss is: 0.0508: validation loss is: 0.7222\n",
            "0: 005: 49.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<__main__.Encoder at 0x7f532d37aeb8>,\n",
              " <__main__.DecoderWithAttention at 0x7f532e1f2940>,\n",
              " [<tf.Tensor: shape=(), dtype=float32, numpy=1.3910816>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.68004495>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.413652>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.27207646>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.19327904>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.14523251>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.113918364>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.09256906>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.07807478>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.069708124>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.06319567>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.058385316>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.05483585>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.05231696>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.050822828>],\n",
              " [<tf.Tensor: shape=(), dtype=float32, numpy=0.9128787>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.6637054>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.58545023>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.56752175>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.5699231>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.58278>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.6006675>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.6186198>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.63443637>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.6554825>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.6619203>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.68840826>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.7086151>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.7122211>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.7222379>])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKro3PCQ9vXE",
        "outputId": "6cafec59-aeed-4636-a37d-c3b278c49f1f"
      },
      "source": [
        "epochs = 15\n",
        "attention = additive\n",
        "train_mtn_model(epochs, attention)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer decoder_gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru_dec will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Epoch: 1: Batch: 0: Loss: 4.3783\n",
            "Epoch: 1: Batch: 100: Loss: 2.3845\n",
            "Epoch: 1: Batch: 200: Loss: 1.8689\n",
            "Epoch: 1: Batch: 300: Loss: 1.7603\n",
            "Epoch: 1: Batch: 400: Loss: 1.5609\n",
            "Epoch: 1: Batch: 500: Loss: 1.6098\n",
            "Epoch: 1: Batch: 600: Loss: 1.5337\n",
            "Epoch: 1: Batch: 700: Loss: 1.4159\n",
            "Epoch: 1: Batch: 800: Loss: 1.5343\n",
            "Epoch: 1: Batch: 900: Loss: 1.2867\n",
            "Epoch: 1: Batch: 1000: Loss: 1.4148\n",
            "Epoch: 1: Batch: 1100: Loss: 1.3500\n",
            "Epoch: 1: Batch: 1200: Loss: 1.3086\n",
            "Epoch: 1: Batch: 1300: Loss: 1.3006\n",
            "Epoch: 1: Batch: 1400: Loss: 1.2071\n",
            "At epoch: 1: training loss is: 1.5824: validation loss is: 1.2398\n",
            "0: 006: 03.00\n",
            "Epoch: 2: Batch: 0: Loss: 1.1595\n",
            "Epoch: 2: Batch: 100: Loss: 1.1248\n",
            "Epoch: 2: Batch: 200: Loss: 0.9861\n",
            "Epoch: 2: Batch: 300: Loss: 1.0528\n",
            "Epoch: 2: Batch: 400: Loss: 0.9710\n",
            "Epoch: 2: Batch: 500: Loss: 1.0445\n",
            "Epoch: 2: Batch: 600: Loss: 1.0102\n",
            "Epoch: 2: Batch: 700: Loss: 0.9774\n",
            "Epoch: 2: Batch: 800: Loss: 0.9488\n",
            "Epoch: 2: Batch: 900: Loss: 0.8640\n",
            "Epoch: 2: Batch: 1000: Loss: 0.9851\n",
            "Epoch: 2: Batch: 1100: Loss: 0.9972\n",
            "Epoch: 2: Batch: 1200: Loss: 0.9365\n",
            "Epoch: 2: Batch: 1300: Loss: 1.1366\n",
            "Epoch: 2: Batch: 1400: Loss: 0.8698\n",
            "At epoch: 2: training loss is: 1.0183: validation loss is: 0.9538\n",
            "0: 005: 42.00\n",
            "Epoch: 3: Batch: 0: Loss: 0.7064\n",
            "Epoch: 3: Batch: 100: Loss: 0.7349\n",
            "Epoch: 3: Batch: 200: Loss: 0.6951\n",
            "Epoch: 3: Batch: 300: Loss: 0.7792\n",
            "Epoch: 3: Batch: 400: Loss: 0.7558\n",
            "Epoch: 3: Batch: 500: Loss: 0.7170\n",
            "Epoch: 3: Batch: 600: Loss: 0.7365\n",
            "Epoch: 3: Batch: 700: Loss: 0.7438\n",
            "Epoch: 3: Batch: 800: Loss: 0.7658\n",
            "Epoch: 3: Batch: 900: Loss: 0.6516\n",
            "Epoch: 3: Batch: 1000: Loss: 0.7502\n",
            "Epoch: 3: Batch: 1100: Loss: 0.7298\n",
            "Epoch: 3: Batch: 1200: Loss: 0.6280\n",
            "Epoch: 3: Batch: 1300: Loss: 0.6251\n",
            "Epoch: 3: Batch: 1400: Loss: 0.7201\n",
            "At epoch: 3: training loss is: 0.7000: validation loss is: 0.7902\n",
            "0: 005: 42.00\n",
            "Epoch: 4: Batch: 0: Loss: 0.5275\n",
            "Epoch: 4: Batch: 100: Loss: 0.5061\n",
            "Epoch: 4: Batch: 200: Loss: 0.4605\n",
            "Epoch: 4: Batch: 300: Loss: 0.5152\n",
            "Epoch: 4: Batch: 400: Loss: 0.4225\n",
            "Epoch: 4: Batch: 500: Loss: 0.4497\n",
            "Epoch: 4: Batch: 600: Loss: 0.4572\n",
            "Epoch: 4: Batch: 700: Loss: 0.5123\n",
            "Epoch: 4: Batch: 800: Loss: 0.5379\n",
            "Epoch: 4: Batch: 900: Loss: 0.5499\n",
            "Epoch: 4: Batch: 1000: Loss: 0.4887\n",
            "Epoch: 4: Batch: 1100: Loss: 0.4616\n",
            "Epoch: 4: Batch: 1200: Loss: 0.5269\n",
            "Epoch: 4: Batch: 1300: Loss: 0.4489\n",
            "Epoch: 4: Batch: 1400: Loss: 0.4711\n",
            "At epoch: 4: training loss is: 0.4816: validation loss is: 0.7110\n",
            "0: 005: 43.00\n",
            "Epoch: 5: Batch: 0: Loss: 0.3512\n",
            "Epoch: 5: Batch: 100: Loss: 0.3278\n",
            "Epoch: 5: Batch: 200: Loss: 0.3769\n",
            "Epoch: 5: Batch: 300: Loss: 0.3068\n",
            "Epoch: 5: Batch: 400: Loss: 0.3760\n",
            "Epoch: 5: Batch: 500: Loss: 0.4012\n",
            "Epoch: 5: Batch: 600: Loss: 0.3004\n",
            "Epoch: 5: Batch: 700: Loss: 0.3400\n",
            "Epoch: 5: Batch: 800: Loss: 0.3472\n",
            "Epoch: 5: Batch: 900: Loss: 0.3382\n",
            "Epoch: 5: Batch: 1000: Loss: 0.2953\n",
            "Epoch: 5: Batch: 1100: Loss: 0.4204\n",
            "Epoch: 5: Batch: 1200: Loss: 0.3381\n",
            "Epoch: 5: Batch: 1300: Loss: 0.4313\n",
            "Epoch: 5: Batch: 1400: Loss: 0.3474\n",
            "At epoch: 5: training loss is: 0.3373: validation loss is: 0.6792\n",
            "0: 005: 42.00\n",
            "Epoch: 6: Batch: 0: Loss: 0.2285\n",
            "Epoch: 6: Batch: 100: Loss: 0.2020\n",
            "Epoch: 6: Batch: 200: Loss: 0.2349\n",
            "Epoch: 6: Batch: 300: Loss: 0.2408\n",
            "Epoch: 6: Batch: 400: Loss: 0.2385\n",
            "Epoch: 6: Batch: 500: Loss: 0.2367\n",
            "Epoch: 6: Batch: 600: Loss: 0.2616\n",
            "Epoch: 6: Batch: 700: Loss: 0.2701\n",
            "Epoch: 6: Batch: 800: Loss: 0.2607\n",
            "Epoch: 6: Batch: 900: Loss: 0.2600\n",
            "Epoch: 6: Batch: 1000: Loss: 0.2238\n",
            "Epoch: 6: Batch: 1100: Loss: 0.2462\n",
            "Epoch: 6: Batch: 1200: Loss: 0.2612\n",
            "Epoch: 6: Batch: 1300: Loss: 0.2656\n",
            "Epoch: 6: Batch: 1400: Loss: 0.2380\n",
            "At epoch: 6: training loss is: 0.2438: validation loss is: 0.6749\n",
            "0: 005: 42.00\n",
            "Epoch: 7: Batch: 0: Loss: 0.1445\n",
            "Epoch: 7: Batch: 100: Loss: 0.1357\n",
            "Epoch: 7: Batch: 200: Loss: 0.1405\n",
            "Epoch: 7: Batch: 300: Loss: 0.1936\n",
            "Epoch: 7: Batch: 400: Loss: 0.1498\n",
            "Epoch: 7: Batch: 500: Loss: 0.1835\n",
            "Epoch: 7: Batch: 600: Loss: 0.1562\n",
            "Epoch: 7: Batch: 700: Loss: 0.1703\n",
            "Epoch: 7: Batch: 800: Loss: 0.2173\n",
            "Epoch: 7: Batch: 900: Loss: 0.1888\n",
            "Epoch: 7: Batch: 1000: Loss: 0.1774\n",
            "Epoch: 7: Batch: 1100: Loss: 0.2339\n",
            "Epoch: 7: Batch: 1200: Loss: 0.2266\n",
            "Epoch: 7: Batch: 1300: Loss: 0.2158\n",
            "Epoch: 7: Batch: 1400: Loss: 0.2446\n",
            "At epoch: 7: training loss is: 0.1826: validation loss is: 0.6828\n",
            "0: 005: 43.00\n",
            "Epoch: 8: Batch: 0: Loss: 0.1433\n",
            "Epoch: 8: Batch: 100: Loss: 0.1070\n",
            "Epoch: 8: Batch: 200: Loss: 0.1007\n",
            "Epoch: 8: Batch: 300: Loss: 0.1328\n",
            "Epoch: 8: Batch: 400: Loss: 0.1306\n",
            "Epoch: 8: Batch: 500: Loss: 0.1475\n",
            "Epoch: 8: Batch: 600: Loss: 0.1551\n",
            "Epoch: 8: Batch: 700: Loss: 0.1300\n",
            "Epoch: 8: Batch: 800: Loss: 0.1205\n",
            "Epoch: 8: Batch: 900: Loss: 0.1764\n",
            "Epoch: 8: Batch: 1000: Loss: 0.1857\n",
            "Epoch: 8: Batch: 1100: Loss: 0.1459\n",
            "Epoch: 8: Batch: 1200: Loss: 0.1547\n",
            "Epoch: 8: Batch: 1300: Loss: 0.1644\n",
            "Epoch: 8: Batch: 1400: Loss: 0.1683\n",
            "At epoch: 8: training loss is: 0.1412: validation loss is: 0.6953\n",
            "0: 005: 45.00\n",
            "Epoch: 9: Batch: 0: Loss: 0.0769\n",
            "Epoch: 9: Batch: 100: Loss: 0.0888\n",
            "Epoch: 9: Batch: 200: Loss: 0.0862\n",
            "Epoch: 9: Batch: 300: Loss: 0.1087\n",
            "Epoch: 9: Batch: 400: Loss: 0.0795\n",
            "Epoch: 9: Batch: 500: Loss: 0.0986\n",
            "Epoch: 9: Batch: 600: Loss: 0.1103\n",
            "Epoch: 9: Batch: 700: Loss: 0.1000\n",
            "Epoch: 9: Batch: 800: Loss: 0.1129\n",
            "Epoch: 9: Batch: 900: Loss: 0.1383\n",
            "Epoch: 9: Batch: 1000: Loss: 0.1616\n",
            "Epoch: 9: Batch: 1100: Loss: 0.1303\n",
            "Epoch: 9: Batch: 1200: Loss: 0.1222\n",
            "Epoch: 9: Batch: 1300: Loss: 0.1422\n",
            "Epoch: 9: Batch: 1400: Loss: 0.1551\n",
            "At epoch: 9: training loss is: 0.1140: validation loss is: 0.7160\n",
            "0: 005: 48.00\n",
            "Epoch: 10: Batch: 0: Loss: 0.0998\n",
            "Epoch: 10: Batch: 100: Loss: 0.0779\n",
            "Epoch: 10: Batch: 200: Loss: 0.0812\n",
            "Epoch: 10: Batch: 300: Loss: 0.0769\n",
            "Epoch: 10: Batch: 400: Loss: 0.0904\n",
            "Epoch: 10: Batch: 500: Loss: 0.0812\n",
            "Epoch: 10: Batch: 600: Loss: 0.1092\n",
            "Epoch: 10: Batch: 700: Loss: 0.1181\n",
            "Epoch: 10: Batch: 800: Loss: 0.1208\n",
            "Epoch: 10: Batch: 900: Loss: 0.1017\n",
            "Epoch: 10: Batch: 1000: Loss: 0.1071\n",
            "Epoch: 10: Batch: 1100: Loss: 0.1253\n",
            "Epoch: 10: Batch: 1200: Loss: 0.1318\n",
            "Epoch: 10: Batch: 1300: Loss: 0.1500\n",
            "Epoch: 10: Batch: 1400: Loss: 0.0863\n",
            "At epoch: 10: training loss is: 0.0968: validation loss is: 0.7268\n",
            "0: 005: 49.00\n",
            "Epoch: 11: Batch: 0: Loss: 0.0555\n",
            "Epoch: 11: Batch: 100: Loss: 0.0661\n",
            "Epoch: 11: Batch: 200: Loss: 0.0590\n",
            "Epoch: 11: Batch: 300: Loss: 0.0787\n",
            "Epoch: 11: Batch: 400: Loss: 0.0843\n",
            "Epoch: 11: Batch: 500: Loss: 0.0899\n",
            "Epoch: 11: Batch: 600: Loss: 0.0724\n",
            "Epoch: 11: Batch: 700: Loss: 0.0930\n",
            "Epoch: 11: Batch: 800: Loss: 0.0750\n",
            "Epoch: 11: Batch: 900: Loss: 0.0965\n",
            "Epoch: 11: Batch: 1000: Loss: 0.0841\n",
            "Epoch: 11: Batch: 1100: Loss: 0.0824\n",
            "Epoch: 11: Batch: 1200: Loss: 0.0828\n",
            "Epoch: 11: Batch: 1300: Loss: 0.1031\n",
            "Epoch: 11: Batch: 1400: Loss: 0.0891\n",
            "At epoch: 11: training loss is: 0.0836: validation loss is: 0.7536\n",
            "0: 005: 48.00\n",
            "Epoch: 12: Batch: 0: Loss: 0.0569\n",
            "Epoch: 12: Batch: 100: Loss: 0.0761\n",
            "Epoch: 12: Batch: 200: Loss: 0.0716\n",
            "Epoch: 12: Batch: 300: Loss: 0.0789\n",
            "Epoch: 12: Batch: 400: Loss: 0.0685\n",
            "Epoch: 12: Batch: 500: Loss: 0.0543\n",
            "Epoch: 12: Batch: 600: Loss: 0.0576\n",
            "Epoch: 12: Batch: 700: Loss: 0.0940\n",
            "Epoch: 12: Batch: 800: Loss: 0.0712\n",
            "Epoch: 12: Batch: 900: Loss: 0.0606\n",
            "Epoch: 12: Batch: 1000: Loss: 0.0855\n",
            "Epoch: 12: Batch: 1100: Loss: 0.0619\n",
            "Epoch: 12: Batch: 1200: Loss: 0.0891\n",
            "Epoch: 12: Batch: 1300: Loss: 0.0720\n",
            "Epoch: 12: Batch: 1400: Loss: 0.1085\n",
            "At epoch: 12: training loss is: 0.0758: validation loss is: 0.7654\n",
            "0: 005: 48.00\n",
            "Epoch: 13: Batch: 0: Loss: 0.0739\n",
            "Epoch: 13: Batch: 100: Loss: 0.0399\n",
            "Epoch: 13: Batch: 200: Loss: 0.0496\n",
            "Epoch: 13: Batch: 300: Loss: 0.0707\n",
            "Epoch: 13: Batch: 400: Loss: 0.0603\n",
            "Epoch: 13: Batch: 500: Loss: 0.0719\n",
            "Epoch: 13: Batch: 600: Loss: 0.0720\n",
            "Epoch: 13: Batch: 700: Loss: 0.0611\n",
            "Epoch: 13: Batch: 800: Loss: 0.0590\n",
            "Epoch: 13: Batch: 900: Loss: 0.0947\n",
            "Epoch: 13: Batch: 1000: Loss: 0.0746\n",
            "Epoch: 13: Batch: 1100: Loss: 0.1019\n",
            "Epoch: 13: Batch: 1200: Loss: 0.1321\n",
            "Epoch: 13: Batch: 1300: Loss: 0.0708\n",
            "Epoch: 13: Batch: 1400: Loss: 0.1109\n",
            "At epoch: 13: training loss is: 0.0692: validation loss is: 0.7851\n",
            "0: 005: 47.00\n",
            "Epoch: 14: Batch: 0: Loss: 0.0449\n",
            "Epoch: 14: Batch: 100: Loss: 0.0482\n",
            "Epoch: 14: Batch: 200: Loss: 0.0489\n",
            "Epoch: 14: Batch: 300: Loss: 0.0671\n",
            "Epoch: 14: Batch: 400: Loss: 0.0414\n",
            "Epoch: 14: Batch: 500: Loss: 0.0671\n",
            "Epoch: 14: Batch: 600: Loss: 0.0507\n",
            "Epoch: 14: Batch: 700: Loss: 0.0615\n",
            "Epoch: 14: Batch: 800: Loss: 0.0730\n",
            "Epoch: 14: Batch: 900: Loss: 0.0562\n",
            "Epoch: 14: Batch: 1000: Loss: 0.0396\n",
            "Epoch: 14: Batch: 1100: Loss: 0.1035\n",
            "Epoch: 14: Batch: 1200: Loss: 0.0624\n",
            "Epoch: 14: Batch: 1300: Loss: 0.1011\n",
            "Epoch: 14: Batch: 1400: Loss: 0.0798\n",
            "At epoch: 14: training loss is: 0.0655: validation loss is: 0.7986\n",
            "0: 005: 48.00\n",
            "Epoch: 15: Batch: 0: Loss: 0.0518\n",
            "Epoch: 15: Batch: 100: Loss: 0.0674\n",
            "Epoch: 15: Batch: 200: Loss: 0.0587\n",
            "Epoch: 15: Batch: 300: Loss: 0.0517\n",
            "Epoch: 15: Batch: 400: Loss: 0.0529\n",
            "Epoch: 15: Batch: 500: Loss: 0.0507\n",
            "Epoch: 15: Batch: 600: Loss: 0.0508\n",
            "Epoch: 15: Batch: 700: Loss: 0.0522\n",
            "Epoch: 15: Batch: 800: Loss: 0.0808\n",
            "Epoch: 15: Batch: 900: Loss: 0.0618\n",
            "Epoch: 15: Batch: 1000: Loss: 0.0470\n",
            "Epoch: 15: Batch: 1100: Loss: 0.0870\n",
            "Epoch: 15: Batch: 1200: Loss: 0.0759\n",
            "Epoch: 15: Batch: 1300: Loss: 0.0530\n",
            "Epoch: 15: Batch: 1400: Loss: 0.0875\n",
            "At epoch: 15: training loss is: 0.0630: validation loss is: 0.8050\n",
            "0: 005: 49.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<__main__.Encoder at 0x7f532caba400>,\n",
              " <__main__.DecoderWithAttention at 0x7f532c7b96a0>,\n",
              " [<tf.Tensor: shape=(), dtype=float32, numpy=1.5823554>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.0183258>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.70003134>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.48164475>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.33727655>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.24375208>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.18258658>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.14117578>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.113993354>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.09677085>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.08355326>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.07577282>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.06923313>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.065496504>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.06303487>],\n",
              " [<tf.Tensor: shape=(), dtype=float32, numpy=1.2397861>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9538124>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.7901567>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.71102893>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.67924833>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.6748742>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.68284804>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.69531035>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.71604806>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.726838>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.7536254>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.7654302>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.7851378>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.7985598>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=0.80495274>])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_jF1oVzEJ-r"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    }
  ]
}